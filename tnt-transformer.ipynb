{"cells":[{"cell_type":"code","execution_count":2,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-03-08T02:45:01.466494Z","iopub.status.busy":"2024-03-08T02:45:01.465256Z","iopub.status.idle":"2024-03-08T02:45:01.474961Z","shell.execute_reply":"2024-03-08T02:45:01.473869Z","shell.execute_reply.started":"2024-03-08T02:45:01.466456Z"},"trusted":true},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","import pandas as pd\n","import zipfile\n","import os\n","import cv2\n","import seaborn as sns\n","from collections import Counter\n","# from google.colab.patches import cv2_imshow\n","from pathlib import Path\n","import random\n","\n","from sklearn.preprocessing import MinMaxScaler\n","from sklearn.decomposition import PCA\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn.model_selection import train_test_split\n","from sklearn.neighbors import NearestNeighbors\n","from imblearn.over_sampling import RandomOverSampler\n","from imblearn.over_sampling import ADASYN\n","import PIL\n","import torch\n","from torch import nn\n","from torch.utils.data import DataLoader\n","from torchvision import datasets, transforms\n","from torchvision.transforms import ToTensor\n","import torchvision\n","import torch.optim as optim\n","from functools import partial\n"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-03-08T02:45:05.433146Z","iopub.status.busy":"2024-03-08T02:45:05.432801Z","iopub.status.idle":"2024-03-08T02:45:05.439256Z","shell.execute_reply":"2024-03-08T02:45:05.438292Z","shell.execute_reply.started":"2024-03-08T02:45:05.433121Z"},"trusted":true},"outputs":[],"source":["def get_classes(data_dir):\n","    all_data = datasets.ImageFolder(data_dir)\n","    return all_data.classes"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-03-08T02:45:09.030564Z","iopub.status.busy":"2024-03-08T02:45:09.030183Z","iopub.status.idle":"2024-03-08T02:45:25.848532Z","shell.execute_reply":"2024-03-08T02:45:25.847445Z","shell.execute_reply.started":"2024-03-08T02:45:09.030533Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["['both', 'infection', 'ischaemia', 'none']\n"]}],"source":["test_transform = transforms.Compose([\n","                                transforms.Resize((224, 224)),\n","                                transforms.ToTensor()\n","                                    ])\n","\n","import torchvision.transforms as transforms\n","\n","transform = transforms.Compose([\n","    transforms.Resize((224, 224)),\n","    transforms.ToTensor(),\n","  \n","])\n","\n","\n","\n","\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","kwargs = {'num_workers': 1, 'pin_memory': True} if device=='cuda' else {}\n","\n","train_dataset_path = '/kaggle/input/dfuc-2021-split/A NEW DATASET SPLIT/train'\n","valid_dataset_path = '/kaggle/input/dfuc-2021-split/A NEW DATASET SPLIT/valid'\n","#test_dataset_path = '/kaggle/input/for-trial/new_tts_aug/test'\n","\n","train_dataset = datasets.ImageFolder(train_dataset_path, transform=transform)\n","valid_dataset = datasets.ImageFolder(valid_dataset_path, transform=transform)\n","#test_dataset = datasets.ImageFolder(test_dataset_path,transform=test_transform)\n","\n","train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=32,shuffle=True,**kwargs)\n","valid_dataloader = torch.utils.data.DataLoader(valid_dataset, batch_size=32,shuffle=True,**kwargs)\n","#test_dataloader =  torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=True, **kwargs)\n","\n","\n","CLASSES = train_dataset.classes\n","train_len = len(train_dataset)\n","valid_len = len(valid_dataset)\n","#test_len = len(test_dataset)\n","print(CLASSES)"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-03-08T02:45:30.207717Z","iopub.status.busy":"2024-03-08T02:45:30.207351Z","iopub.status.idle":"2024-03-08T02:45:33.801132Z","shell.execute_reply":"2024-03-08T02:45:33.800068Z","shell.execute_reply.started":"2024-03-08T02:45:30.207687Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/tmp/ipykernel_34/455749117.py:328: UserWarning: Overwriting tnt_s_patch16_224 in registry with __main__.tnt_s_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n","  def tnt_s_patch16_224(pretrained=False, **kwargs):\n","/tmp/ipykernel_34/455749117.py:348: UserWarning: Overwriting tnt_b_patch16_224 in registry with __main__.tnt_b_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n","  def tnt_b_patch16_224(pretrained=False, **kwargs):\n"]},{"data":{"text/plain":["==========================================================================================\n","Layer (type:depth-idx)                   Output Shape              Param #\n","==========================================================================================\n","TNT                                      [32, 4]                   151,680\n","├─PatchEmbed: 1-1                        [6272, 16, 24]            --\n","│    └─Unfold: 2-1                       [32, 768, 196]            --\n","│    └─Conv2d: 2-2                       [6272, 24, 4, 4]          3,552\n","├─LayerNorm: 1-2                         [32, 196, 384]            768\n","├─Linear: 1-3                            [32, 196, 384]            147,840\n","├─LayerNorm: 1-4                         [32, 196, 384]            768\n","├─Dropout: 1-5                           [32, 197, 384]            --\n","├─ModuleList: 1-6                        --                        --\n","│    └─Block: 2-3                        [6272, 16, 24]            --\n","│    │    └─LayerNorm: 3-1               [6272, 16, 24]            48\n","│    │    └─Attention: 3-2               [6272, 16, 24]            2,328\n","│    │    └─Identity: 3-3                [6272, 16, 24]            --\n","│    │    └─LayerNorm: 3-4               [6272, 16, 24]            48\n","│    │    └─Mlp: 3-5                     [6272, 16, 24]            4,728\n","│    │    └─Identity: 3-6                [6272, 16, 24]            --\n","│    │    └─LayerNorm: 3-7               [32, 196, 384]            768\n","│    │    └─Linear: 3-8                  [32, 196, 384]            147,456\n","│    │    └─LayerNorm: 3-9               [32, 196, 384]            768\n","│    │    └─LayerNorm: 3-10              [32, 197, 384]            768\n","│    │    └─Attention: 3-11              [32, 197, 384]            590,208\n","│    │    └─Identity: 3-12               [32, 197, 384]            --\n","│    │    └─LayerNorm: 3-13              [32, 197, 384]            768\n","│    │    └─Mlp: 3-14                    [32, 197, 384]            1,181,568\n","│    │    └─Identity: 3-15               [32, 197, 384]            --\n","│    └─Block: 2-4                        [6272, 16, 24]            --\n","│    │    └─LayerNorm: 3-16              [6272, 16, 24]            48\n","│    │    └─Attention: 3-17              [6272, 16, 24]            2,328\n","│    │    └─Identity: 3-18               [6272, 16, 24]            --\n","│    │    └─LayerNorm: 3-19              [6272, 16, 24]            48\n","│    │    └─Mlp: 3-20                    [6272, 16, 24]            4,728\n","│    │    └─Identity: 3-21               [6272, 16, 24]            --\n","│    │    └─LayerNorm: 3-22              [32, 196, 384]            768\n","│    │    └─Linear: 3-23                 [32, 196, 384]            147,456\n","│    │    └─LayerNorm: 3-24              [32, 196, 384]            768\n","│    │    └─LayerNorm: 3-25              [32, 197, 384]            768\n","│    │    └─Attention: 3-26              [32, 197, 384]            590,208\n","│    │    └─Identity: 3-27               [32, 197, 384]            --\n","│    │    └─LayerNorm: 3-28              [32, 197, 384]            768\n","│    │    └─Mlp: 3-29                    [32, 197, 384]            1,181,568\n","│    │    └─Identity: 3-30               [32, 197, 384]            --\n","│    └─Block: 2-5                        [6272, 16, 24]            --\n","│    │    └─LayerNorm: 3-31              [6272, 16, 24]            48\n","│    │    └─Attention: 3-32              [6272, 16, 24]            2,328\n","│    │    └─Identity: 3-33               [6272, 16, 24]            --\n","│    │    └─LayerNorm: 3-34              [6272, 16, 24]            48\n","│    │    └─Mlp: 3-35                    [6272, 16, 24]            4,728\n","│    │    └─Identity: 3-36               [6272, 16, 24]            --\n","│    │    └─LayerNorm: 3-37              [32, 196, 384]            768\n","│    │    └─Linear: 3-38                 [32, 196, 384]            147,456\n","│    │    └─LayerNorm: 3-39              [32, 196, 384]            768\n","│    │    └─LayerNorm: 3-40              [32, 197, 384]            768\n","│    │    └─Attention: 3-41              [32, 197, 384]            590,208\n","│    │    └─Identity: 3-42               [32, 197, 384]            --\n","│    │    └─LayerNorm: 3-43              [32, 197, 384]            768\n","│    │    └─Mlp: 3-44                    [32, 197, 384]            1,181,568\n","│    │    └─Identity: 3-45               [32, 197, 384]            --\n","│    └─Block: 2-6                        [6272, 16, 24]            --\n","│    │    └─LayerNorm: 3-46              [6272, 16, 24]            48\n","│    │    └─Attention: 3-47              [6272, 16, 24]            2,328\n","│    │    └─Identity: 3-48               [6272, 16, 24]            --\n","│    │    └─LayerNorm: 3-49              [6272, 16, 24]            48\n","│    │    └─Mlp: 3-50                    [6272, 16, 24]            4,728\n","│    │    └─Identity: 3-51               [6272, 16, 24]            --\n","│    │    └─LayerNorm: 3-52              [32, 196, 384]            768\n","│    │    └─Linear: 3-53                 [32, 196, 384]            147,456\n","│    │    └─LayerNorm: 3-54              [32, 196, 384]            768\n","│    │    └─LayerNorm: 3-55              [32, 197, 384]            768\n","│    │    └─Attention: 3-56              [32, 197, 384]            590,208\n","│    │    └─Identity: 3-57               [32, 197, 384]            --\n","│    │    └─LayerNorm: 3-58              [32, 197, 384]            768\n","│    │    └─Mlp: 3-59                    [32, 197, 384]            1,181,568\n","│    │    └─Identity: 3-60               [32, 197, 384]            --\n","│    └─Block: 2-7                        [6272, 16, 24]            --\n","│    │    └─LayerNorm: 3-61              [6272, 16, 24]            48\n","│    │    └─Attention: 3-62              [6272, 16, 24]            2,328\n","│    │    └─Identity: 3-63               [6272, 16, 24]            --\n","│    │    └─LayerNorm: 3-64              [6272, 16, 24]            48\n","│    │    └─Mlp: 3-65                    [6272, 16, 24]            4,728\n","│    │    └─Identity: 3-66               [6272, 16, 24]            --\n","│    │    └─LayerNorm: 3-67              [32, 196, 384]            768\n","│    │    └─Linear: 3-68                 [32, 196, 384]            147,456\n","│    │    └─LayerNorm: 3-69              [32, 196, 384]            768\n","│    │    └─LayerNorm: 3-70              [32, 197, 384]            768\n","│    │    └─Attention: 3-71              [32, 197, 384]            590,208\n","│    │    └─Identity: 3-72               [32, 197, 384]            --\n","│    │    └─LayerNorm: 3-73              [32, 197, 384]            768\n","│    │    └─Mlp: 3-74                    [32, 197, 384]            1,181,568\n","│    │    └─Identity: 3-75               [32, 197, 384]            --\n","│    └─Block: 2-8                        [6272, 16, 24]            --\n","│    │    └─LayerNorm: 3-76              [6272, 16, 24]            48\n","│    │    └─Attention: 3-77              [6272, 16, 24]            2,328\n","│    │    └─Identity: 3-78               [6272, 16, 24]            --\n","│    │    └─LayerNorm: 3-79              [6272, 16, 24]            48\n","│    │    └─Mlp: 3-80                    [6272, 16, 24]            4,728\n","│    │    └─Identity: 3-81               [6272, 16, 24]            --\n","│    │    └─LayerNorm: 3-82              [32, 196, 384]            768\n","│    │    └─Linear: 3-83                 [32, 196, 384]            147,456\n","│    │    └─LayerNorm: 3-84              [32, 196, 384]            768\n","│    │    └─LayerNorm: 3-85              [32, 197, 384]            768\n","│    │    └─Attention: 3-86              [32, 197, 384]            590,208\n","│    │    └─Identity: 3-87               [32, 197, 384]            --\n","│    │    └─LayerNorm: 3-88              [32, 197, 384]            768\n","│    │    └─Mlp: 3-89                    [32, 197, 384]            1,181,568\n","│    │    └─Identity: 3-90               [32, 197, 384]            --\n","│    └─Block: 2-9                        [6272, 16, 24]            --\n","│    │    └─LayerNorm: 3-91              [6272, 16, 24]            48\n","│    │    └─Attention: 3-92              [6272, 16, 24]            2,328\n","│    │    └─Identity: 3-93               [6272, 16, 24]            --\n","│    │    └─LayerNorm: 3-94              [6272, 16, 24]            48\n","│    │    └─Mlp: 3-95                    [6272, 16, 24]            4,728\n","│    │    └─Identity: 3-96               [6272, 16, 24]            --\n","│    │    └─LayerNorm: 3-97              [32, 196, 384]            768\n","│    │    └─Linear: 3-98                 [32, 196, 384]            147,456\n","│    │    └─LayerNorm: 3-99              [32, 196, 384]            768\n","│    │    └─LayerNorm: 3-100             [32, 197, 384]            768\n","│    │    └─Attention: 3-101             [32, 197, 384]            590,208\n","│    │    └─Identity: 3-102              [32, 197, 384]            --\n","│    │    └─LayerNorm: 3-103             [32, 197, 384]            768\n","│    │    └─Mlp: 3-104                   [32, 197, 384]            1,181,568\n","│    │    └─Identity: 3-105              [32, 197, 384]            --\n","│    └─Block: 2-10                       [6272, 16, 24]            --\n","│    │    └─LayerNorm: 3-106             [6272, 16, 24]            48\n","│    │    └─Attention: 3-107             [6272, 16, 24]            2,328\n","│    │    └─Identity: 3-108              [6272, 16, 24]            --\n","│    │    └─LayerNorm: 3-109             [6272, 16, 24]            48\n","│    │    └─Mlp: 3-110                   [6272, 16, 24]            4,728\n","│    │    └─Identity: 3-111              [6272, 16, 24]            --\n","│    │    └─LayerNorm: 3-112             [32, 196, 384]            768\n","│    │    └─Linear: 3-113                [32, 196, 384]            147,456\n","│    │    └─LayerNorm: 3-114             [32, 196, 384]            768\n","│    │    └─LayerNorm: 3-115             [32, 197, 384]            768\n","│    │    └─Attention: 3-116             [32, 197, 384]            590,208\n","│    │    └─Identity: 3-117              [32, 197, 384]            --\n","│    │    └─LayerNorm: 3-118             [32, 197, 384]            768\n","│    │    └─Mlp: 3-119                   [32, 197, 384]            1,181,568\n","│    │    └─Identity: 3-120              [32, 197, 384]            --\n","│    └─Block: 2-11                       [6272, 16, 24]            --\n","│    │    └─LayerNorm: 3-121             [6272, 16, 24]            48\n","│    │    └─Attention: 3-122             [6272, 16, 24]            2,328\n","│    │    └─Identity: 3-123              [6272, 16, 24]            --\n","│    │    └─LayerNorm: 3-124             [6272, 16, 24]            48\n","│    │    └─Mlp: 3-125                   [6272, 16, 24]            4,728\n","│    │    └─Identity: 3-126              [6272, 16, 24]            --\n","│    │    └─LayerNorm: 3-127             [32, 196, 384]            768\n","│    │    └─Linear: 3-128                [32, 196, 384]            147,456\n","│    │    └─LayerNorm: 3-129             [32, 196, 384]            768\n","│    │    └─LayerNorm: 3-130             [32, 197, 384]            768\n","│    │    └─Attention: 3-131             [32, 197, 384]            590,208\n","│    │    └─Identity: 3-132              [32, 197, 384]            --\n","│    │    └─LayerNorm: 3-133             [32, 197, 384]            768\n","│    │    └─Mlp: 3-134                   [32, 197, 384]            1,181,568\n","│    │    └─Identity: 3-135              [32, 197, 384]            --\n","│    └─Block: 2-12                       [6272, 16, 24]            --\n","│    │    └─LayerNorm: 3-136             [6272, 16, 24]            48\n","│    │    └─Attention: 3-137             [6272, 16, 24]            2,328\n","│    │    └─Identity: 3-138              [6272, 16, 24]            --\n","│    │    └─LayerNorm: 3-139             [6272, 16, 24]            48\n","│    │    └─Mlp: 3-140                   [6272, 16, 24]            4,728\n","│    │    └─Identity: 3-141              [6272, 16, 24]            --\n","│    │    └─LayerNorm: 3-142             [32, 196, 384]            768\n","│    │    └─Linear: 3-143                [32, 196, 384]            147,456\n","│    │    └─LayerNorm: 3-144             [32, 196, 384]            768\n","│    │    └─LayerNorm: 3-145             [32, 197, 384]            768\n","│    │    └─Attention: 3-146             [32, 197, 384]            590,208\n","│    │    └─Identity: 3-147              [32, 197, 384]            --\n","│    │    └─LayerNorm: 3-148             [32, 197, 384]            768\n","│    │    └─Mlp: 3-149                   [32, 197, 384]            1,181,568\n","│    │    └─Identity: 3-150              [32, 197, 384]            --\n","│    └─Block: 2-13                       [6272, 16, 24]            --\n","│    │    └─LayerNorm: 3-151             [6272, 16, 24]            48\n","│    │    └─Attention: 3-152             [6272, 16, 24]            2,328\n","│    │    └─Identity: 3-153              [6272, 16, 24]            --\n","│    │    └─LayerNorm: 3-154             [6272, 16, 24]            48\n","│    │    └─Mlp: 3-155                   [6272, 16, 24]            4,728\n","│    │    └─Identity: 3-156              [6272, 16, 24]            --\n","│    │    └─LayerNorm: 3-157             [32, 196, 384]            768\n","│    │    └─Linear: 3-158                [32, 196, 384]            147,456\n","│    │    └─LayerNorm: 3-159             [32, 196, 384]            768\n","│    │    └─LayerNorm: 3-160             [32, 197, 384]            768\n","│    │    └─Attention: 3-161             [32, 197, 384]            590,208\n","│    │    └─Identity: 3-162              [32, 197, 384]            --\n","│    │    └─LayerNorm: 3-163             [32, 197, 384]            768\n","│    │    └─Mlp: 3-164                   [32, 197, 384]            1,181,568\n","│    │    └─Identity: 3-165              [32, 197, 384]            --\n","│    └─Block: 2-14                       [6272, 16, 24]            --\n","│    │    └─LayerNorm: 3-166             [6272, 16, 24]            48\n","│    │    └─Attention: 3-167             [6272, 16, 24]            2,328\n","│    │    └─Identity: 3-168              [6272, 16, 24]            --\n","│    │    └─LayerNorm: 3-169             [6272, 16, 24]            48\n","│    │    └─Mlp: 3-170                   [6272, 16, 24]            4,728\n","│    │    └─Identity: 3-171              [6272, 16, 24]            --\n","│    │    └─LayerNorm: 3-172             [32, 196, 384]            768\n","│    │    └─Linear: 3-173                [32, 196, 384]            147,456\n","│    │    └─LayerNorm: 3-174             [32, 196, 384]            768\n","│    │    └─LayerNorm: 3-175             [32, 197, 384]            768\n","│    │    └─Attention: 3-176             [32, 197, 384]            590,208\n","│    │    └─Identity: 3-177              [32, 197, 384]            --\n","│    │    └─LayerNorm: 3-178             [32, 197, 384]            768\n","│    │    └─Mlp: 3-179                   [32, 197, 384]            1,181,568\n","│    │    └─Identity: 3-180              [32, 197, 384]            --\n","├─LayerNorm: 1-7                         [32, 197, 384]            768\n","├─Linear: 1-8                            [32, 4]                   1,540\n","==========================================================================================\n","Total params: 23,460,388\n","Trainable params: 23,385,124\n","Non-trainable params: 75,264\n","Total mult-adds (G): 1.64\n","==========================================================================================\n","Input size (MB): 19.27\n","Forward/backward pass size (MB): 5889.69\n","Params size (MB): 93.23\n","Estimated Total Size (MB): 6002.19\n","=========================================================================================="]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["# 2021.06.15-Changed for implementation of TNT model\n","#            Huawei Technologies Co., Ltd. <foss@huawei.com>\n","\"\"\" Vision Transformer (ViT) in PyTorch\n","\n","A PyTorch implement of Vision Transformers as described in\n","'An Image Is Worth 16 x 16 Words: Transformers for Image Recognition at Scale' - https://arxiv.org/abs/2010.11929\n","\n","The official jax code is released and available at https://github.com/google-research/vision_transformer\n","\n","Status/TODO:\n","* Models updated to be compatible with official impl. Args added to support backward compat for old PyTorch weights.\n","* Weights ported from official jax impl for 384x384 base and small models, 16x16 and 32x32 patches.\n","* Trained (supervised on ImageNet-1k) my custom 'small' patch model to 77.9, 'base' to 79.4 top-1 with this code.\n","* Hopefully find time and GPUs for SSL or unsupervised pretraining on OpenImages w/ ImageNet fine-tune in future.\n","\n","Acknowledgments:\n","* The paper authors for releasing code and weights, thanks!\n","* I fixed my class token impl based on Phil Wang's https://github.com/lucidrains/vit-pytorch ... check it out\n","for some einops/einsum fun\n","* Simple transformer style inspired by Andrej Karpathy's https://github.com/karpathy/minGPT\n","* Bert reference code checks against Huggingface Transformers and Tensorflow Bert\n","\n","Hacked together by / Copyright 2020 Ross Wightman\n","\"\"\"\n","import torch\n","import torch.nn as nn\n","from functools import partial\n","import math\n","\n","from timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD\n","from timm.models.helpers import load_pretrained\n","from timm.models.layers import DropPath, to_2tuple, trunc_normal_\n","from timm.models.resnet import resnet26d, resnet50d\n","from timm.models.registry import register_model\n","\n","\n","def _cfg(url='', **kwargs):\n","    return {\n","        'url': url,\n","        'num_classes': 1000, 'input_size': (3, 224, 224), 'pool_size': None,\n","        'crop_pct': .9, 'interpolation': 'bicubic',\n","        'mean': IMAGENET_DEFAULT_MEAN, 'std': IMAGENET_DEFAULT_STD,\n","        'first_conv': 'patch_embed.proj', 'classifier': 'head',\n","        **kwargs\n","    }\n","\n","\n","default_cfgs = {\n","    'tnt_s_patch16_224': _cfg(\n","        mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5),\n","    ),\n","    'tnt_b_patch16_224': _cfg(\n","        mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5),\n","    ),\n","}\n","\n","\n","def make_divisible(v, divisor=8, min_value=None):\n","    min_value = min_value or divisor\n","    new_v = max(min_value, int(v + divisor / 2) // divisor * divisor)\n","    # Make sure that round down does not go down by more than 10%.\n","    if new_v < 0.9 * v:\n","        new_v += divisor\n","    return new_v\n","\n","\n","class Mlp(nn.Module):\n","    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n","        super().__init__()\n","        out_features = out_features or in_features\n","        hidden_features = hidden_features or in_features\n","        self.fc1 = nn.Linear(in_features, hidden_features)\n","        self.act = act_layer()\n","        self.fc2 = nn.Linear(hidden_features, out_features)\n","        self.drop = nn.Dropout(drop)\n","\n","    def forward(self, x):\n","        x = self.fc1(x)\n","        x = self.act(x)\n","        x = self.drop(x)\n","        x = self.fc2(x)\n","        x = self.drop(x)\n","        return x\n","\n","\n","class SE(nn.Module):\n","    def __init__(self, dim, hidden_ratio=None):\n","        super().__init__()\n","        hidden_ratio = hidden_ratio or 1\n","        self.dim = dim\n","        hidden_dim = int(dim * hidden_ratio)\n","        self.fc = nn.Sequential(\n","            nn.LayerNorm(dim),\n","            nn.Linear(dim, hidden_dim),\n","            nn.ReLU(inplace=True),\n","            nn.Linear(hidden_dim, dim),\n","            nn.Tanh()\n","        )\n","\n","    def forward(self, x):\n","        a = x.mean(dim=1, keepdim=True) # B, 1, C\n","        a = self.fc(a)\n","        x = a * x\n","        return x\n","\n","\n","class Attention(nn.Module):\n","    def __init__(self, dim, hidden_dim, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0., proj_drop=0.):\n","        super().__init__()\n","        self.hidden_dim = hidden_dim\n","        self.num_heads = num_heads\n","        head_dim = hidden_dim // num_heads\n","        self.head_dim = head_dim\n","        # NOTE scale factor was wrong in my original version, can set manually to be compat with prev weights\n","        self.scale = qk_scale or head_dim ** -0.5\n","\n","        self.qk = nn.Linear(dim, hidden_dim * 2, bias=qkv_bias)\n","        self.v = nn.Linear(dim, dim, bias=qkv_bias)\n","        self.attn_drop = nn.Dropout(attn_drop, inplace=True)\n","        self.proj = nn.Linear(dim, dim)\n","        self.proj_drop = nn.Dropout(proj_drop, inplace=True)\n","\n","    def forward(self, x):\n","        B, N, C = x.shape\n","        qk = self.qk(x).reshape(B, N, 2, self.num_heads, self.head_dim).permute(2, 0, 3, 1, 4)\n","        q, k = qk[0], qk[1]   # make torchscript happy (cannot use tensor as tuple)\n","        v = self.v(x).reshape(B, N, self.num_heads, -1).permute(0, 2, 1, 3)\n","\n","        attn = (q @ k.transpose(-2, -1)) * self.scale\n","        attn = attn.softmax(dim=-1)\n","        attn = self.attn_drop(attn)\n","\n","        x = (attn @ v).transpose(1, 2).reshape(B, N, -1)\n","        x = self.proj(x)\n","        x = self.proj_drop(x)\n","        return x\n","\n","\n","class Block(nn.Module):\n","    \"\"\" TNT Block\n","    \"\"\"\n","    def __init__(self, outer_dim, inner_dim, outer_num_heads, inner_num_heads, num_words, mlp_ratio=4.,\n","                 qkv_bias=False, qk_scale=None, drop=0., attn_drop=0., drop_path=0., act_layer=nn.GELU,\n","                 norm_layer=nn.LayerNorm, se=0):\n","        super().__init__()\n","        self.has_inner = inner_dim > 0\n","        if self.has_inner:\n","            # Inner\n","            self.inner_norm1 = norm_layer(inner_dim)\n","            self.inner_attn = Attention(\n","                inner_dim, inner_dim, num_heads=inner_num_heads, qkv_bias=qkv_bias,\n","                qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop)\n","            self.inner_norm2 = norm_layer(inner_dim)\n","            self.inner_mlp = Mlp(in_features=inner_dim, hidden_features=int(inner_dim * mlp_ratio),\n","                                 out_features=inner_dim, act_layer=act_layer, drop=drop)\n","\n","            self.proj_norm1 = norm_layer(num_words * inner_dim)\n","            self.proj = nn.Linear(num_words * inner_dim, outer_dim, bias=False)\n","            self.proj_norm2 = norm_layer(outer_dim)\n","        # Outer\n","        self.outer_norm1 = norm_layer(outer_dim)\n","        self.outer_attn = Attention(\n","            outer_dim, outer_dim, num_heads=outer_num_heads, qkv_bias=qkv_bias,\n","            qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop)\n","        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n","        self.outer_norm2 = norm_layer(outer_dim)\n","        self.outer_mlp = Mlp(in_features=outer_dim, hidden_features=int(outer_dim * mlp_ratio),\n","                             out_features=outer_dim, act_layer=act_layer, drop=drop)\n","        # SE\n","        self.se = se\n","        self.se_layer = None\n","        if self.se > 0:\n","            self.se_layer = SE(outer_dim, 0.25)\n","\n","    def forward(self, inner_tokens, outer_tokens):\n","        if self.has_inner:\n","            inner_tokens = inner_tokens + self.drop_path(self.inner_attn(self.inner_norm1(inner_tokens))) # B*N, k*k, c\n","            inner_tokens = inner_tokens + self.drop_path(self.inner_mlp(self.inner_norm2(inner_tokens))) # B*N, k*k, c\n","            B, N, C = outer_tokens.size()\n","            outer_tokens[:,1:] = outer_tokens[:,1:] + self.proj_norm2(self.proj(self.proj_norm1(inner_tokens.reshape(B, N-1, -1)))) # B, N, C\n","        if self.se > 0:\n","            outer_tokens = outer_tokens + self.drop_path(self.outer_attn(self.outer_norm1(outer_tokens)))\n","            tmp_ = self.outer_mlp(self.outer_norm2(outer_tokens))\n","            outer_tokens = outer_tokens + self.drop_path(tmp_ + self.se_layer(tmp_))\n","        else:\n","            outer_tokens = outer_tokens + self.drop_path(self.outer_attn(self.outer_norm1(outer_tokens)))\n","            outer_tokens = outer_tokens + self.drop_path(self.outer_mlp(self.outer_norm2(outer_tokens)))\n","        return inner_tokens, outer_tokens\n","\n","\n","class PatchEmbed(nn.Module):\n","    \"\"\" Image to Visual Word Embedding\n","    \"\"\"\n","    def __init__(self, img_size=224, patch_size=16, in_chans=3, outer_dim=768, inner_dim=24, inner_stride=4):\n","        super().__init__()\n","        img_size = to_2tuple(img_size)\n","        patch_size = to_2tuple(patch_size)\n","        num_patches = (img_size[1] // patch_size[1]) * (img_size[0] // patch_size[0])\n","        self.img_size = img_size\n","        self.patch_size = patch_size\n","        self.num_patches = num_patches\n","        self.inner_dim = inner_dim\n","        self.num_words = math.ceil(patch_size[0] / inner_stride) * math.ceil(patch_size[1] / inner_stride)\n","        \n","        self.unfold = nn.Unfold(kernel_size=patch_size, stride=patch_size)\n","        self.proj = nn.Conv2d(in_chans, inner_dim, kernel_size=7, padding=3, stride=inner_stride)\n","\n","    def forward(self, x):\n","        B, C, H, W = x.shape\n","        # FIXME look at relaxing size constraints\n","        assert H == self.img_size[0] and W == self.img_size[1], \\\n","            f\"Input image size ({H}*{W}) doesn't match model ({self.img_size[0]}*{self.img_size[1]}).\"\n","        x = self.unfold(x) # B, Ck2, N\n","        x = x.transpose(1, 2).reshape(B * self.num_patches, C, *self.patch_size) # B*N, C, 16, 16\n","        x = self.proj(x) # B*N, C, 8, 8\n","        x = x.reshape(B * self.num_patches, self.inner_dim, -1).transpose(1, 2) # B*N, 8*8, C\n","        return x\n","\n","\n","class TNT(nn.Module):\n","    \"\"\" TNT (Transformer in Transformer) for computer vision\n","    \"\"\"\n","    def __init__(self, img_size=224, patch_size=16, in_chans=3, num_classes=1000, outer_dim=768, inner_dim=48,\n","                 depth=12, outer_num_heads=12, inner_num_heads=4, mlp_ratio=4., qkv_bias=False, qk_scale=None,\n","                 drop_rate=0., attn_drop_rate=0., drop_path_rate=0., norm_layer=nn.LayerNorm, inner_stride=4, se=0):\n","        super().__init__()\n","        self.num_classes = num_classes\n","        self.num_features = self.outer_dim = outer_dim  # num_features for consistency with other models\n","\n","        self.patch_embed = PatchEmbed(\n","            img_size=img_size, patch_size=patch_size, in_chans=in_chans, outer_dim=outer_dim,\n","            inner_dim=inner_dim, inner_stride=inner_stride)\n","        self.num_patches = num_patches = self.patch_embed.num_patches\n","        num_words = self.patch_embed.num_words\n","        \n","        self.proj_norm1 = norm_layer(num_words * inner_dim)\n","        self.proj = nn.Linear(num_words * inner_dim, outer_dim)\n","        self.proj_norm2 = norm_layer(outer_dim)\n","\n","        self.cls_token = nn.Parameter(torch.zeros(1, 1, outer_dim))\n","        self.outer_tokens = nn.Parameter(torch.zeros(1, num_patches, outer_dim), requires_grad=False)\n","        self.outer_pos = nn.Parameter(torch.zeros(1, num_patches + 1, outer_dim))\n","        self.inner_pos = nn.Parameter(torch.zeros(1, num_words, inner_dim))\n","        self.pos_drop = nn.Dropout(p=drop_rate)\n","\n","        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]  # stochastic depth decay rule\n","        vanilla_idxs = []\n","        blocks = []\n","        for i in range(depth):\n","            if i in vanilla_idxs:\n","                blocks.append(Block(\n","                    outer_dim=outer_dim, inner_dim=-1, outer_num_heads=outer_num_heads, inner_num_heads=inner_num_heads,\n","                    num_words=num_words, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale, drop=drop_rate,\n","                    attn_drop=attn_drop_rate, drop_path=dpr[i], norm_layer=norm_layer, se=se))\n","            else:\n","                blocks.append(Block(\n","                    outer_dim=outer_dim, inner_dim=inner_dim, outer_num_heads=outer_num_heads, inner_num_heads=inner_num_heads,\n","                    num_words=num_words, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale, drop=drop_rate,\n","                    attn_drop=attn_drop_rate, drop_path=dpr[i], norm_layer=norm_layer, se=se))\n","        self.blocks = nn.ModuleList(blocks)\n","        self.norm = norm_layer(outer_dim)\n","\n","        # NOTE as per official impl, we could have a pre-logits representation dense layer + tanh here\n","        #self.repr = nn.Linear(outer_dim, representation_size)\n","        #self.repr_act = nn.Tanh()\n","\n","        # Classifier head\n","        self.head = nn.Linear(outer_dim, num_classes) if num_classes > 0 else nn.Identity()\n","\n","        trunc_normal_(self.cls_token, std=.02)\n","        trunc_normal_(self.outer_pos, std=.02)\n","        trunc_normal_(self.inner_pos, std=.02)\n","        self.apply(self._init_weights)\n","\n","    def _init_weights(self, m):\n","        if isinstance(m, nn.Linear):\n","            trunc_normal_(m.weight, std=.02)\n","            if isinstance(m, nn.Linear) and m.bias is not None:\n","                nn.init.constant_(m.bias, 0)\n","        elif isinstance(m, nn.LayerNorm):\n","            nn.init.constant_(m.bias, 0)\n","            nn.init.constant_(m.weight, 1.0)\n","\n","    @torch.jit.ignore\n","    def no_weight_decay(self):\n","        return {'outer_pos', 'inner_pos', 'cls_token'}\n","\n","    def get_classifier(self):\n","        return self.head\n","\n","    def reset_classifier(self, num_classes, global_pool=''):\n","        self.num_classes = num_classes\n","        self.head = nn.Linear(self.outer_dim, num_classes) if num_classes > 0 else nn.Identity()\n","\n","    def forward_features(self, x):\n","        B = x.shape[0]\n","        inner_tokens = self.patch_embed(x) + self.inner_pos # B*N, 8*8, C\n","        \n","        outer_tokens = self.proj_norm2(self.proj(self.proj_norm1(inner_tokens.reshape(B, self.num_patches, -1))))        \n","        outer_tokens = torch.cat((self.cls_token.expand(B, -1, -1), outer_tokens), dim=1)\n","        \n","        outer_tokens = outer_tokens + self.outer_pos\n","        outer_tokens = self.pos_drop(outer_tokens)\n","\n","        for blk in self.blocks:\n","            inner_tokens, outer_tokens = blk(inner_tokens, outer_tokens)\n","\n","        outer_tokens = self.norm(outer_tokens)\n","        return outer_tokens[:, 0]\n","\n","    def forward(self, x):\n","        x = self.forward_features(x)\n","        x = self.head(x)\n","        return x\n","\n","\n","def _conv_filter(state_dict, patch_size=16):\n","    \"\"\" convert patch embedding weight from manual patchify + linear proj to conv\"\"\"\n","    out_dict = {}\n","    for k, v in state_dict.items():\n","        if 'patch_embed.proj.weight' in k:\n","            v = v.reshape((v.shape[0], 3, patch_size, patch_size))\n","        out_dict[k] = v\n","    return out_dict\n","\n","\n","@register_model\n","def tnt_s_patch16_224(pretrained=False, **kwargs):\n","    patch_size = 16\n","    inner_stride = 4\n","    outer_dim = 384\n","    inner_dim = 24\n","    outer_num_heads = 6\n","    inner_num_heads = 4\n","    outer_dim = make_divisible(outer_dim, outer_num_heads)\n","    inner_dim = make_divisible(inner_dim, inner_num_heads)\n","    model = TNT(img_size=224, patch_size=patch_size, outer_dim=outer_dim, inner_dim=inner_dim, depth=12,\n","                outer_num_heads=outer_num_heads, inner_num_heads=inner_num_heads, qkv_bias=False,\n","                inner_stride=inner_stride, **kwargs)\n","    model.default_cfg = default_cfgs['tnt_s_patch16_224']\n","    if pretrained:\n","        load_pretrained(\n","            model, num_classes=model.num_classes, in_chans=kwargs.get('in_chans', 3), filter_fn=_conv_filter)\n","    return model\n","\n","\n","@register_model\n","def tnt_b_patch16_224(pretrained=False, **kwargs):\n","    patch_size = 16\n","    inner_stride = 4\n","    outer_dim = 640\n","    inner_dim = 40\n","    outer_num_heads = 10\n","    inner_num_heads = 4\n","    outer_dim = make_divisible(outer_dim, outer_num_heads)\n","    inner_dim = make_divisible(inner_dim, inner_num_heads)\n","    model = TNT(img_size=224, patch_size=patch_size, outer_dim=outer_dim, inner_dim=inner_dim, depth=12,\n","                outer_num_heads=outer_num_heads, inner_num_heads=inner_num_heads, qkv_bias=False,\n","                inner_stride=inner_stride, **kwargs)\n","    model.default_cfg = default_cfgs['tnt_b_patch16_224']\n","    if pretrained:\n","        load_pretrained(\n","            model, num_classes=model.num_classes, in_chans=kwargs.get('in_chans', 3), filter_fn=_conv_filter)\n","    return model\n","\n","model=tnt_s_patch16_224(num_classes=4)\n","import torchinfo\n","torchinfo.summary(model,(32,3,224,224))"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-03-08T02:45:42.346698Z","iopub.status.busy":"2024-03-08T02:45:42.346316Z","iopub.status.idle":"2024-03-08T02:45:42.353707Z","shell.execute_reply":"2024-03-08T02:45:42.352694Z","shell.execute_reply.started":"2024-03-08T02:45:42.346668Z"},"trusted":true},"outputs":[{"data":{"text/plain":["4"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["len(CLASSES)"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-03-08T02:45:44.696596Z","iopub.status.busy":"2024-03-08T02:45:44.695699Z","iopub.status.idle":"2024-03-08T02:45:44.701106Z","shell.execute_reply":"2024-03-08T02:45:44.700151Z","shell.execute_reply.started":"2024-03-08T02:45:44.696562Z"},"trusted":true},"outputs":[],"source":["\n","dataloaders = {\n","    \"train\": train_dataloader,\n","    \"val\": valid_dataloader,\n","    #\"test\": test_dataloader\n","}\n","\n","dataset_sizes = {\n","    \"train\": train_len,\n","    \"val\": valid_len,\n","    #\"test\": test_len\n","}\n"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-03-08T02:45:47.417140Z","iopub.status.busy":"2024-03-08T02:45:47.416744Z","iopub.status.idle":"2024-03-08T02:45:47.433440Z","shell.execute_reply":"2024-03-08T02:45:47.432461Z","shell.execute_reply.started":"2024-03-08T02:45:47.417107Z"},"trusted":true},"outputs":[],"source":["model.to(device)\n","\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(model.parameters(),lr=0.001,weight_decay=0.00001)\n","criterion = criterion.to(device)\n","exp_lr_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=15, gamma=0.97)"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2024-03-08T02:46:10.013867Z","iopub.status.busy":"2024-03-08T02:46:10.013517Z","iopub.status.idle":"2024-03-08T02:46:10.036689Z","shell.execute_reply":"2024-03-08T02:46:10.035363Z","shell.execute_reply.started":"2024-03-08T02:46:10.013840Z"},"trusted":true},"outputs":[],"source":["import time\n","import copy\n","import torch\n","import pandas as pd\n","import os\n","from tqdm import tqdm\n","\n","def train_model(model, criterion, optimizer, exp_lr_scheduler, num_epochs=75, load_checkpoint=False, checkpoint_path=None):\n","    since = time.time()\n","    best_model_wts = copy.deepcopy(model.state_dict())\n","    best_acc = 0.0\n","    df_train = pd.DataFrame(columns=['epoch', 'train_loss', 'train_acc'])\n","    df_val = pd.DataFrame(columns=['epoch', 'val_loss', 'val_acc'])\n","    if load_checkpoint:\n","        if checkpoint_path is None:\n","            raise ValueError(\"Checkpoint path is not specified.\")\n","\n","        checkpoint = torch.load(checkpoint_path)\n","        model.load_state_dict(checkpoint['model_state_dict'])\n","        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n","        original_learning_rate = optimizer.param_groups[0]['lr']\n","        print(f\"Original Learning Rate: {original_learning_rate}\")\n","        loss = checkpoint['loss']\n","\n","        start_epoch = 46\n","    else:\n","        start_epoch = 0\n","\n","    for epoch in range(start_epoch, num_epochs):\n","        print(f'Epoch {epoch}/{num_epochs - 1}')\n","        print(\"-\" * 10)\n","\n","        if epoch == 0:\n","            if not os.path.isdir(\"/kaggle/working/\"):\n","                os.mkdir(\"/kaggle/working/\")\n","\n","        for phase in ['train', 'val']:  # We do training and validation phase per epoch\n","            if phase == 'train':\n","                model.train()  # model to training mode\n","            else:\n","                model.eval()  # model to evaluate\n","\n","            running_loss = 0.0\n","            running_corrects = 0.0\n","            total_samples = 0\n","\n","            progress_bar = tqdm(dataloaders[phase], desc=f'{phase.capitalize()} Epoch {epoch}/{num_epochs - 1}', leave=False)\n","\n","            for inputs, labels in progress_bar:\n","                inputs = inputs.to(device)\n","                labels = labels.to(device)\n","\n","                optimizer.zero_grad()\n","\n","                with torch.set_grad_enabled(phase == 'train'):  # no autograd makes validation go faster\n","                    outputs = model(inputs)\n","                    _, preds = torch.max(outputs, 1)  # used for accuracy\n","                    loss = criterion(outputs, labels)\n","\n","                    if phase == 'train':\n","                        loss.backward()\n","                        optimizer.step()\n","\n","                running_loss += loss.item() * inputs.size(0)\n","                running_corrects += torch.sum(preds == labels.data)\n","                total_samples += labels.size(0)\n","\n","                # Update progress bar description with live accuracy\n","                accuracy = running_corrects.double() / total_samples\n","                progress_bar.set_postfix(loss=running_loss / total_samples, accuracy=accuracy)\n","\n","            if phase == 'train':\n","                exp_lr_scheduler.step()  # step at the end of the epoch\n","\n","            epoch_loss = running_loss / dataset_sizes[phase]\n","            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n","\n","            if phase == 'train':\n","                df_new_row = pd.DataFrame({'epoch': [epoch], 'train_loss': [epoch_loss], 'train_acc': [epoch_acc.cpu()]})\n","                df_train = pd.concat([df_train, df_new_row])\n","                df_train.to_csv('train_metrics.csv')\n","            elif phase == 'val':\n","                df_new_row = pd.DataFrame({'epoch': [epoch], 'val_loss': [epoch_loss], 'val_acc': [epoch_acc.cpu()]})\n","                df_val = pd.concat([df_val, df_new_row])\n","                df_val.to_csv('val_metrics.csv')\n","\n","            print(\"{} Loss: {:.4f} Acc: {:.4f}\".format(phase, epoch_loss, epoch_acc))\n","\n","            # Save torch model for checkpoints\n","            if epoch % 9 == 0:\n","                torch.save({\n","                    'epoch': epoch,\n","                    'model_state_dict': model.state_dict(),\n","                    'optimizer_state_dict': optimizer.state_dict(),\n","                    'loss': epoch_loss,\n","                }, f\"/kaggle/working/sav_model{epoch}.pt\")\n","\n","            if phase == 'val' and epoch_acc > best_acc:\n","                best_acc = epoch_acc\n","                best_model_wts = copy.deepcopy(model.state_dict())  # keep the best validation accuracy model\n","        print()\n","\n","    time_elapsed = time.time() - since\n","    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n","    print(\"Best Val Acc: {:.4f}\".format(best_acc))\n","\n","    model.load_state_dict(best_model_wts)\n","    return model, df_train, df_val\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-08T02:46:49.520210Z","iopub.status.busy":"2024-03-08T02:46:49.519814Z"},"trusted":true},"outputs":[],"source":["model, df_train, df_val = train_model(model, criterion, optimizer, exp_lr_scheduler, num_epochs=200, load_checkpoint=False, checkpoint_path=None)\n","# Save the best model weights at the end of training\n","torch.save(model.state_dict(), '/kaggle/working/best_model_weights.pth')\n","\n"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":4548795,"sourceId":7774637,"sourceType":"datasetVersion"}],"dockerImageVersionId":30665,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
