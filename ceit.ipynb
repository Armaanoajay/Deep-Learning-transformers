{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-03-08T02:52:29.611154Z","iopub.status.busy":"2024-03-08T02:52:29.610814Z","iopub.status.idle":"2024-03-08T02:52:38.989297Z","shell.execute_reply":"2024-03-08T02:52:38.988517Z","shell.execute_reply.started":"2024-03-08T02:52:29.611125Z"},"trusted":true},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","import pandas as pd\n","import zipfile\n","import os\n","import cv2\n","import seaborn as sns\n","from collections import Counter\n","# from google.colab.patches import cv2_imshow\n","from pathlib import Path\n","import random\n","\n","from sklearn.preprocessing import MinMaxScaler\n","from sklearn.decomposition import PCA\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn.model_selection import train_test_split\n","from sklearn.neighbors import NearestNeighbors\n","from imblearn.over_sampling import RandomOverSampler\n","from imblearn.over_sampling import ADASYN\n","import PIL\n","import torch\n","from torch import nn\n","from torch.utils.data import DataLoader\n","from torchvision import datasets, transforms\n","from torchvision.transforms import ToTensor\n","import torchvision\n","import torch.optim as optim\n","from functools import partial\n"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-03-08T02:52:48.285388Z","iopub.status.busy":"2024-03-08T02:52:48.284560Z","iopub.status.idle":"2024-03-08T02:52:48.289616Z","shell.execute_reply":"2024-03-08T02:52:48.288703Z","shell.execute_reply.started":"2024-03-08T02:52:48.285335Z"},"trusted":true},"outputs":[],"source":["def get_classes(data_dir):\n","    all_data = datasets.ImageFolder(data_dir)\n","    return all_data.classes"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-03-08T02:52:50.574336Z","iopub.status.busy":"2024-03-08T02:52:50.573664Z","iopub.status.idle":"2024-03-08T02:53:02.871744Z","shell.execute_reply":"2024-03-08T02:53:02.870759Z","shell.execute_reply.started":"2024-03-08T02:52:50.574303Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["['both', 'infection', 'ischaemia', 'none']\n"]}],"source":["test_transform = transforms.Compose([\n","                                transforms.Resize((224, 224)),\n","                                transforms.ToTensor()\n","                                    ])\n","\n","import torchvision.transforms as transforms\n","\n","transform = transforms.Compose([\n","    transforms.Resize((224, 224)),\n","    transforms.ToTensor(),\n","  \n","])\n","\n","\n","\n","\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","kwargs = {'num_workers': 1, 'pin_memory': True} if device=='cuda' else {}\n","\n","train_dataset_path = '/kaggle/input/dfuc-2021-split/A NEW DATASET SPLIT/train'\n","valid_dataset_path = '/kaggle/input/dfuc-2021-split/A NEW DATASET SPLIT/valid'\n","#test_dataset_path = '/kaggle/input/for-trial/new_tts_aug/test'\n","\n","train_dataset = datasets.ImageFolder(train_dataset_path, transform=transform)\n","valid_dataset = datasets.ImageFolder(valid_dataset_path, transform=transform)\n","#test_dataset = datasets.ImageFolder(test_dataset_path,transform=test_transform)\n","\n","train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=32,shuffle=True,**kwargs)\n","valid_dataloader = torch.utils.data.DataLoader(valid_dataset, batch_size=32,shuffle=True,**kwargs)\n","#test_dataloader =  torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=True, **kwargs)\n","\n","\n","CLASSES = train_dataset.classes\n","train_len = len(train_dataset)\n","valid_len = len(valid_dataset)\n","#test_len = len(test_dataset)\n","print(CLASSES)"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-03-08T02:54:16.203813Z","iopub.status.busy":"2024-03-08T02:54:16.203390Z","iopub.status.idle":"2024-03-08T02:54:29.620795Z","shell.execute_reply":"2024-03-08T02:54:29.619547Z","shell.execute_reply.started":"2024-03-08T02:54:16.203784Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting einops\n","  Downloading einops-0.7.0-py3-none-any.whl.metadata (13 kB)\n","Downloading einops-0.7.0-py3-none-any.whl (44 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.6/44.6 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: einops\n","Successfully installed einops-0.7.0\n"]}],"source":["!pip install einops"]},{"cell_type":"code","execution_count":21,"metadata":{"execution":{"iopub.execute_input":"2024-03-08T04:09:39.529004Z","iopub.status.busy":"2024-03-08T04:09:39.528662Z","iopub.status.idle":"2024-03-08T04:09:39.869689Z","shell.execute_reply":"2024-03-08T04:09:39.868723Z","shell.execute_reply.started":"2024-03-08T04:09:39.528979Z"},"trusted":true},"outputs":[],"source":["import math\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from functools import partial\n","\n","from timm.models.layers import DropPath, to_2tuple, trunc_normal_\n","from timm.models.registry import register_model\n","from timm.models.vision_transformer import default_cfgs, _cfg\n","\n","\n","__all__ = [\n","    'ceit_tiny_patch16_224', 'ceit_small_patch16_224', 'ceit_base_patch16_224',\n","    'ceit_tiny_patch16_384', 'ceit_small_patch16_384',\n","]\n","\n","\n","class Image2Tokens(nn.Module):\n","    def __init__(self, in_chans=3, out_chans=64, kernel_size=7, stride=2):\n","        super(Image2Tokens, self).__init__()\n","        self.conv = nn.Conv2d(in_chans, out_chans, kernel_size=kernel_size, stride=stride,\n","                              padding=kernel_size // 2, bias=False)\n","        self.bn = nn.BatchNorm2d(out_chans)\n","        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n","\n","    def forward(self, x):\n","        x = self.conv(x)\n","        x = self.bn(x)\n","        x = self.maxpool(x)\n","        return x\n","\n","\n","class Mlp(nn.Module):\n","    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n","        super().__init__()\n","        out_features = out_features or in_features\n","        hidden_features = hidden_features or in_features\n","        self.fc1 = nn.Linear(in_features, hidden_features)\n","        self.act = act_layer()\n","        self.fc2 = nn.Linear(hidden_features, out_features)\n","        self.drop = nn.Dropout(drop)\n","\n","    def forward(self, x):\n","        x = self.fc1(x)\n","        x = self.act(x)\n","        x = self.drop(x)\n","        x = self.fc2(x)\n","        x = self.drop(x)\n","        return x\n","\n","\n","class LocallyEnhancedFeedForward(nn.Module):\n","    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.,\n","                 kernel_size=3, with_bn=True):\n","        super().__init__()\n","        out_features = out_features or in_features\n","        hidden_features = hidden_features or in_features\n","        # pointwise\n","        self.conv1 = nn.Conv2d(in_features, hidden_features, kernel_size=1, stride=1, padding=0)\n","        # depthwise\n","        self.conv2 = nn.Conv2d(\n","            hidden_features, hidden_features, kernel_size=kernel_size, stride=1,\n","            padding=(kernel_size - 1) // 2, groups=hidden_features\n","        )\n","        # pointwise\n","        self.conv3 = nn.Conv2d(hidden_features, out_features, kernel_size=1, stride=1, padding=0)\n","        self.act = act_layer()\n","        # self.drop = nn.Dropout(drop)\n","\n","        self.with_bn = with_bn\n","        if self.with_bn:\n","            self.bn1 = nn.BatchNorm2d(hidden_features)\n","            self.bn2 = nn.BatchNorm2d(hidden_features)\n","            self.bn3 = nn.BatchNorm2d(out_features)\n","\n","    def forward(self, x):\n","        b, n, k = x.size()\n","        cls_token, tokens = torch.split(x, [1, n - 1], dim=1)\n","        x = tokens.reshape(b, int(math.sqrt(n - 1)), int(math.sqrt(n - 1)), k).permute(0, 3, 1, 2)\n","        if self.with_bn:\n","            x = self.conv1(x)\n","            x = self.bn1(x)\n","            x = self.act(x)\n","            x = self.conv2(x)\n","            x = self.bn2(x)\n","            x = self.act(x)\n","            x = self.conv3(x)\n","            x = self.bn3(x)\n","        else:\n","            x = self.conv1(x)\n","            x = self.act(x)\n","            x = self.conv2(x)\n","            x = self.act(x)\n","            x = self.conv3(x)\n","\n","        tokens = x.flatten(2).permute(0, 2, 1)\n","        out = torch.cat((cls_token, tokens), dim=1)\n","        return out\n","\n","\n","class Attention(nn.Module):\n","    def __init__(self, dim, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0., proj_drop=0.):\n","        super().__init__()\n","        self.num_heads = num_heads\n","        head_dim = dim // num_heads\n","        # NOTE scale factor was wrong in my original version, can set manually to be compat with prev weights\n","        self.scale = qk_scale or head_dim ** -0.5\n","\n","        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n","        self.attn_drop = nn.Dropout(attn_drop)\n","        self.proj = nn.Linear(dim, dim)\n","        self.proj_drop = nn.Dropout(proj_drop)\n","        self.attention_map = None\n","\n","    def forward(self, x):\n","        B, N, C = x.shape\n","        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n","        q, k, v = qkv[0], qkv[1], qkv[2]   # make torchscript happy (cannot use tensor as tuple)\n","\n","        attn = (q @ k.transpose(-2, -1)) * self.scale\n","        attn = attn.softmax(dim=-1)\n","        # self.attention_map = attn\n","        attn = self.attn_drop(attn)\n","\n","        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n","        x = self.proj(x)\n","        x = self.proj_drop(x)\n","        return x\n","\n","\n","class AttentionLCA(Attention):\n","    def __init__(self, dim, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0., proj_drop=0.):\n","        super(AttentionLCA, self).__init__(dim, num_heads, qkv_bias, qk_scale, attn_drop, proj_drop)\n","        self.dim = dim\n","        self.qkv_bias = qkv_bias\n","        \n","    def forward(self, x):\n","\n","        q_weight = self.qkv.weight[:self.dim, :]\n","        q_bias = None if not self.qkv_bias else self.qkv.bias[:self.dim]\n","        kv_weight = self.qkv.weight[self.dim:, :]\n","        kv_bias = None if not self.qkv_bias else self.qkv.bias[self.dim:]\n","        \n","        B, N, C = x.shape\n","        _, last_token = torch.split(x, [N-1, 1], dim=1)\n","        \n","        q = F.linear(last_token, q_weight, q_bias)\\\n","             .reshape(B, 1, self.num_heads, C // self.num_heads).permute(0, 2, 1, 3)\n","        kv = F.linear(x, kv_weight, kv_bias)\\\n","              .reshape(B, N, 2, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n","        k, v = kv[0], kv[1]\n","\n","        attn = (q @ k.transpose(-2, -1)) * self.scale\n","        attn = attn.softmax(dim=-1)\n","        # self.attention_map = attn\n","        attn = self.attn_drop(attn)\n","\n","        x = (attn @ v).transpose(1, 2).reshape(B, 1, C)\n","        x = self.proj(x)\n","        x = self.proj_drop(x)\n","        return x\n","\n","\n","class Block(nn.Module):\n","\n","    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop=0., attn_drop=0.,\n","                 drop_path=0., act_layer=nn.GELU, norm_layer=nn.LayerNorm, kernel_size=3, with_bn=True, \n","                 feedforward_type='leff'):\n","        super().__init__()\n","        # NOTE: drop path for stochastic depth, we shall see if this is better than dropout here\n","        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n","        self.norm2 = norm_layer(dim)\n","        mlp_hidden_dim = int(dim * mlp_ratio)\n","        self.norm1 = norm_layer(dim)\n","        self.feedforward_type = feedforward_type\n","\n","        if feedforward_type == 'leff':\n","            self.attn = Attention(\n","                dim, num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop)\n","            self.leff = LocallyEnhancedFeedForward(\n","                in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop,\n","                kernel_size=kernel_size, with_bn=with_bn,\n","            )\n","        else:  # LCA\n","            self.attn = AttentionLCA(\n","                dim, num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop)\n","            self.feedforward = Mlp(\n","                in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop\n","            )\n","\n","    def forward(self, x):\n","        if self.feedforward_type == 'leff':\n","            x = x + self.drop_path(self.attn(self.norm1(x)))\n","            x = x + self.drop_path(self.leff(self.norm2(x)))\n","            return x, x[:, 0]\n","        else:  # LCA\n","            _, last_token = torch.split(x, [x.size(1)-1, 1], dim=1)\n","            x = last_token + self.drop_path(self.attn(self.norm1(x)))\n","            x = x + self.drop_path(self.feedforward(self.norm2(x)))\n","            return x\n","\n","\n","class HybridEmbed(nn.Module):\n","    \"\"\" CNN Feature Map Embedding\n","    Extract feature map from CNN, flatten, project to embedding dim.\n","    \"\"\"\n","    def __init__(self, backbone, img_size=224, patch_size=16, feature_size=None, in_chans=3, embed_dim=768):\n","        super().__init__()\n","        assert isinstance(backbone, nn.Module)\n","        img_size = to_2tuple(img_size)\n","        self.img_size = img_size\n","        self.backbone = backbone\n","        if feature_size is None:\n","            with torch.no_grad():\n","                # FIXME this is hacky, but most reliable way of determining the exact dim of the output feature\n","                # map for all networks, the feature metadata has reliable channel and stride info, but using\n","                # stride to calc feature dim requires info about padding of each stage that isn't captured.\n","                training = backbone.training\n","                if training:\n","                    backbone.eval()\n","                o = self.backbone(torch.zeros(1, in_chans, img_size[0], img_size[1]))\n","                if isinstance(o, (list, tuple)):\n","                    o = o[-1]  # last feature if backbone outputs list/tuple of features\n","                feature_size = o.shape[-2:]\n","                feature_dim = o.shape[1]\n","                backbone.train(training)\n","        else:\n","            feature_size = to_2tuple(feature_size)\n","            feature_dim = self.backbone.feature_info.channels()[-1]\n","        print('feature_size is {}, feature_dim is {}, patch_size is {}'.format(\n","            feature_size, feature_dim, patch_size\n","        ))\n","        self.num_patches = (feature_size[0] // patch_size) * (feature_size[1] // patch_size)\n","        self.proj = nn.Conv2d(feature_dim, embed_dim, kernel_size=patch_size, stride=patch_size)\n","\n","    def forward(self, x):\n","        x = self.backbone(x)\n","        if isinstance(x, (list, tuple)):\n","            x = x[-1]  # last feature if backbone outputs list/tuple of features\n","        x = self.proj(x).flatten(2).transpose(1, 2)\n","        return x\n","\n","\n","class CeIT(nn.Module):\n","    def __init__(self,\n","                 img_size=224,\n","                 patch_size=16,\n","                 in_chans=3,\n","                 num_classes=1000,\n","                 embed_dim=768,\n","                 depth=12,\n","                 num_heads=12,\n","                 mlp_ratio=4.,\n","                 qkv_bias=False,\n","                 qk_scale=None,\n","                 drop_rate=0.,\n","                 attn_drop_rate=0.,\n","                 drop_path_rate=0.,\n","                 hybrid_backbone=None,\n","                 norm_layer=nn.LayerNorm,\n","                 leff_local_size=3,\n","                 leff_with_bn=True):\n","        \"\"\"\n","        args:\n","            - img_size (:obj:`int`): input image size\n","            - patch_size (:obj:`int`): patch size\n","            - in_chans (:obj:`int`): input channels\n","            - num_classes (:obj:`int`): number of classes\n","            - embed_dim (:obj:`int`): embedding dimensions for tokens\n","            - depth (:obj:`int`): depth of encoder\n","            - num_heads (:obj:`int`): number of heads in multi-head self-attention\n","            - mlp_ratio (:obj:`float`): expand ratio in feedforward\n","            - qkv_bias (:obj:`bool`): whether to add bias for mlp of qkv\n","            - qk_scale (:obj:`float`): scale ratio for qk, default is head_dim ** -0.5\n","            - drop_rate (:obj:`float`): dropout rate in feedforward module after linear operation\n","                and projection drop rate in attention\n","            - attn_drop_rate (:obj:`float`): dropout rate for attention\n","            - drop_path_rate (:obj:`float`): drop_path rate after attention\n","            - hybrid_backbone (:obj:`nn.Module`): backbone e.g. resnet\n","            - norm_layer (:obj:`nn.Module`): normalization type\n","            - leff_local_size (:obj:`int`): kernel size in LocallyEnhancedFeedForward\n","            - leff_with_bn (:obj:`bool`): whether add bn in LocallyEnhancedFeedForward\n","        \"\"\"\n","        super().__init__()\n","        self.num_classes = num_classes\n","        self.num_features = self.embed_dim = embed_dim  # num_features for consistency with other models\n","\n","        self.i2t = HybridEmbed(\n","            hybrid_backbone, img_size=img_size, patch_size=patch_size, in_chans=in_chans, embed_dim=embed_dim)\n","        num_patches = self.i2t.num_patches\n","\n","        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n","        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, embed_dim))\n","        self.pos_drop = nn.Dropout(p=drop_rate)\n","\n","        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]  # stochastic depth decay rule\n","        self.blocks = nn.ModuleList([\n","            Block(\n","                dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale,\n","                drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[i], norm_layer=norm_layer,\n","                kernel_size=leff_local_size, with_bn=leff_with_bn)\n","            for i in range(depth)])\n","\n","        # without droppath\n","        self.lca = Block(\n","            dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale,\n","            drop=drop_rate, attn_drop=attn_drop_rate, drop_path=0., norm_layer=norm_layer,\n","            feedforward_type = 'lca'\n","        )\n","        self.pos_layer_embed = nn.Parameter(torch.zeros(1, depth, embed_dim))\n","\n","        self.norm = norm_layer(embed_dim)\n","\n","        # NOTE as per official impl, we could have a pre-logits representation dense layer + tanh here\n","        # self.repr = nn.Linear(embed_dim, representation_size)\n","        # self.repr_act = nn.Tanh()\n","\n","        # Classifier head\n","        self.head = nn.Linear(embed_dim, num_classes) if num_classes > 0 else nn.Identity()\n","\n","        trunc_normal_(self.pos_embed, std=.02)\n","        trunc_normal_(self.cls_token, std=.02)\n","        self.apply(self._init_weights)\n","\n","    def _init_weights(self, m):\n","        if isinstance(m, nn.Linear):\n","            trunc_normal_(m.weight, std=.02)\n","            if isinstance(m, nn.Linear) and m.bias is not None:\n","                nn.init.constant_(m.bias, 0)\n","        elif isinstance(m, nn.LayerNorm):\n","            nn.init.constant_(m.bias, 0)\n","            nn.init.constant_(m.weight, 1.0)\n","\n","    @torch.jit.ignore\n","    def no_weight_decay(self):\n","        return {'pos_embed', 'cls_token'}\n","\n","    def get_classifier(self):\n","        return self.head\n","\n","    def reset_classifier(self, num_classes, global_pool=''):\n","        self.num_classes = num_classes\n","        self.head = nn.Linear(self.embed_dim, num_classes) if num_classes > 0 else nn.Identity()\n","\n","    def forward_features(self, x):\n","        B = x.shape[0]\n","        x = self.i2t(x)\n","\n","        cls_tokens = self.cls_token.expand(B, -1, -1)  # stole cls_tokens impl from Phil Wang, thanks\n","        x = torch.cat((cls_tokens, x), dim=1)\n","        x = x + self.pos_embed\n","        x = self.pos_drop(x)\n","\n","        cls_token_list = []\n","        for blk in self.blocks:\n","            x, curr_cls_token = blk(x)\n","            cls_token_list.append(curr_cls_token)\n","\n","        all_cls_token = torch.stack(cls_token_list, dim=1)  # B*D*K\n","        all_cls_token = all_cls_token + self.pos_layer_embed\n","        # attention over cls tokens\n","        last_cls_token = self.lca(all_cls_token)\n","        last_cls_token = self.norm(last_cls_token)\n","\n","        return last_cls_token.view(B, -1)\n","\n","    def forward(self, x):\n","        x = self.forward_features(x)\n","        x = self.head(x)\n","        return x\n","\n","\n","@register_model\n","def ceit_tiny_patch16_224(pretrained=False, **kwargs):\n","    \"\"\"\n","    convolutional + pooling stem\n","    local enhanced feedforward\n","    attention over cls_tokens\n","    \"\"\"\n","    i2t = Image2Tokens()\n","    model = CeIT(\n","        hybrid_backbone=i2t,\n","        patch_size=4, embed_dim=192, depth=12, num_heads=3, mlp_ratio=4, qkv_bias=True,\n","        norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n","    model.default_cfg = _cfg()\n","    return model\n","\n","\n","@register_model\n","def ceit_small_patch16_224(pretrained=False, **kwargs):\n","    \"\"\"\n","    convolutional + pooling stem\n","    local enhanced feedforward\n","    attention over cls_tokens\n","    \"\"\"\n","    i2t = Image2Tokens()\n","    model = CeIT(\n","        hybrid_backbone=i2t,\n","        patch_size=4, embed_dim=384, depth=12, num_heads=6, mlp_ratio=4, qkv_bias=True,\n","        norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n","    model.default_cfg = _cfg()\n","    return model\n","\n","\n","@register_model\n","def ceit_base_patch16_224(pretrained=False, **kwargs):\n","    \"\"\"\n","    convolutional + pooling stem\n","    local enhanced feedforward\n","    attention over cls_tokens\n","    \"\"\"\n","    i2t = Image2Tokens()\n","    model = CeIT(\n","        hybrid_backbone=i2t,\n","        patch_size=4, embed_dim=768, depth=12, num_heads=12, mlp_ratio=4, qkv_bias=True,\n","        norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n","    model.default_cfg = _cfg()\n","    return model\n","\n","\n","@register_model\n","def ceit_tiny_patch16_384(pretrained=False, **kwargs):\n","    \"\"\"\n","    convolutional + pooling stem\n","    local enhanced feedforward\n","    attention over cls_tokens\n","    \"\"\"\n","    i2t = Image2Tokens()\n","    model = CeIT(\n","        hybrid_backbone=i2t, img_size=384,\n","        patch_size=4, embed_dim=192, depth=12, num_heads=3, mlp_ratio=4, qkv_bias=True,\n","        norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n","    model.default_cfg = _cfg()\n","    return model\n","\n","\n","@register_model\n","def ceit_small_patch16_384(pretrained=False, **kwargs):\n","    \"\"\"\n","    convolutional + pooling stem\n","    local enhanced feedforward\n","    attention over cls_tokens\n","    \"\"\"\n","    i2t = Image2Tokens()\n","    model = CeIT(\n","        hybrid_backbone=i2t, img_size=384,\n","        patch_size=4, embed_dim=384, depth=12, num_heads=6, mlp_ratio=4, qkv_bias=True,\n","        norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n","    model.default_cfg = _cfg()\n","    return model"]},{"cell_type":"code","execution_count":25,"metadata":{"execution":{"iopub.execute_input":"2024-03-08T04:10:46.957790Z","iopub.status.busy":"2024-03-08T04:10:46.956851Z","iopub.status.idle":"2024-03-08T04:10:47.209248Z","shell.execute_reply":"2024-03-08T04:10:47.208259Z","shell.execute_reply.started":"2024-03-08T04:10:46.957741Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["feature_size is torch.Size([56, 56]), feature_dim is 64, patch_size is 4\n"]},{"data":{"text/plain":["====================================================================================================\n","Layer (type:depth-idx)                             Output Shape              Param #\n","====================================================================================================\n","CeIT                                               [32, 4]                   40,320\n","├─HybridEmbed: 1-1                                 [32, 196, 192]            --\n","│    └─Image2Tokens: 2-1                           [32, 64, 56, 56]          --\n","│    │    └─Conv2d: 3-1                            [32, 64, 112, 112]        9,408\n","│    │    └─BatchNorm2d: 3-2                       [32, 64, 112, 112]        128\n","│    │    └─MaxPool2d: 3-3                         [32, 64, 56, 56]          --\n","│    └─Conv2d: 2-2                                 [32, 192, 14, 14]         196,800\n","├─Dropout: 1-2                                     [32, 197, 192]            --\n","├─ModuleList: 1-3                                  --                        --\n","│    └─Block: 2-3                                  [32, 197, 192]            --\n","│    │    └─LayerNorm: 3-4                         [32, 197, 192]            384\n","│    │    └─Attention: 3-5                         [32, 197, 192]            148,224\n","│    │    └─Identity: 3-6                          [32, 197, 192]            --\n","│    │    └─LayerNorm: 3-7                         [32, 197, 192]            384\n","│    │    └─LocallyEnhancedFeedForward: 3-8        [32, 197, 192]            307,008\n","│    │    └─Identity: 3-9                          [32, 197, 192]            --\n","│    └─Block: 2-4                                  [32, 197, 192]            --\n","│    │    └─LayerNorm: 3-10                        [32, 197, 192]            384\n","│    │    └─Attention: 3-11                        [32, 197, 192]            148,224\n","│    │    └─Identity: 3-12                         [32, 197, 192]            --\n","│    │    └─LayerNorm: 3-13                        [32, 197, 192]            384\n","│    │    └─LocallyEnhancedFeedForward: 3-14       [32, 197, 192]            307,008\n","│    │    └─Identity: 3-15                         [32, 197, 192]            --\n","│    └─Block: 2-5                                  [32, 197, 192]            --\n","│    │    └─LayerNorm: 3-16                        [32, 197, 192]            384\n","│    │    └─Attention: 3-17                        [32, 197, 192]            148,224\n","│    │    └─Identity: 3-18                         [32, 197, 192]            --\n","│    │    └─LayerNorm: 3-19                        [32, 197, 192]            384\n","│    │    └─LocallyEnhancedFeedForward: 3-20       [32, 197, 192]            307,008\n","│    │    └─Identity: 3-21                         [32, 197, 192]            --\n","│    └─Block: 2-6                                  [32, 197, 192]            --\n","│    │    └─LayerNorm: 3-22                        [32, 197, 192]            384\n","│    │    └─Attention: 3-23                        [32, 197, 192]            148,224\n","│    │    └─Identity: 3-24                         [32, 197, 192]            --\n","│    │    └─LayerNorm: 3-25                        [32, 197, 192]            384\n","│    │    └─LocallyEnhancedFeedForward: 3-26       [32, 197, 192]            307,008\n","│    │    └─Identity: 3-27                         [32, 197, 192]            --\n","│    └─Block: 2-7                                  [32, 197, 192]            --\n","│    │    └─LayerNorm: 3-28                        [32, 197, 192]            384\n","│    │    └─Attention: 3-29                        [32, 197, 192]            148,224\n","│    │    └─Identity: 3-30                         [32, 197, 192]            --\n","│    │    └─LayerNorm: 3-31                        [32, 197, 192]            384\n","│    │    └─LocallyEnhancedFeedForward: 3-32       [32, 197, 192]            307,008\n","│    │    └─Identity: 3-33                         [32, 197, 192]            --\n","│    └─Block: 2-8                                  [32, 197, 192]            --\n","│    │    └─LayerNorm: 3-34                        [32, 197, 192]            384\n","│    │    └─Attention: 3-35                        [32, 197, 192]            148,224\n","│    │    └─Identity: 3-36                         [32, 197, 192]            --\n","│    │    └─LayerNorm: 3-37                        [32, 197, 192]            384\n","│    │    └─LocallyEnhancedFeedForward: 3-38       [32, 197, 192]            307,008\n","│    │    └─Identity: 3-39                         [32, 197, 192]            --\n","│    └─Block: 2-9                                  [32, 197, 192]            --\n","│    │    └─LayerNorm: 3-40                        [32, 197, 192]            384\n","│    │    └─Attention: 3-41                        [32, 197, 192]            148,224\n","│    │    └─Identity: 3-42                         [32, 197, 192]            --\n","│    │    └─LayerNorm: 3-43                        [32, 197, 192]            384\n","│    │    └─LocallyEnhancedFeedForward: 3-44       [32, 197, 192]            307,008\n","│    │    └─Identity: 3-45                         [32, 197, 192]            --\n","│    └─Block: 2-10                                 [32, 197, 192]            --\n","│    │    └─LayerNorm: 3-46                        [32, 197, 192]            384\n","│    │    └─Attention: 3-47                        [32, 197, 192]            148,224\n","│    │    └─Identity: 3-48                         [32, 197, 192]            --\n","│    │    └─LayerNorm: 3-49                        [32, 197, 192]            384\n","│    │    └─LocallyEnhancedFeedForward: 3-50       [32, 197, 192]            307,008\n","│    │    └─Identity: 3-51                         [32, 197, 192]            --\n","│    └─Block: 2-11                                 [32, 197, 192]            --\n","│    │    └─LayerNorm: 3-52                        [32, 197, 192]            384\n","│    │    └─Attention: 3-53                        [32, 197, 192]            148,224\n","│    │    └─Identity: 3-54                         [32, 197, 192]            --\n","│    │    └─LayerNorm: 3-55                        [32, 197, 192]            384\n","│    │    └─LocallyEnhancedFeedForward: 3-56       [32, 197, 192]            307,008\n","│    │    └─Identity: 3-57                         [32, 197, 192]            --\n","│    └─Block: 2-12                                 [32, 197, 192]            --\n","│    │    └─LayerNorm: 3-58                        [32, 197, 192]            384\n","│    │    └─Attention: 3-59                        [32, 197, 192]            148,224\n","│    │    └─Identity: 3-60                         [32, 197, 192]            --\n","│    │    └─LayerNorm: 3-61                        [32, 197, 192]            384\n","│    │    └─LocallyEnhancedFeedForward: 3-62       [32, 197, 192]            307,008\n","│    │    └─Identity: 3-63                         [32, 197, 192]            --\n","│    └─Block: 2-13                                 [32, 197, 192]            --\n","│    │    └─LayerNorm: 3-64                        [32, 197, 192]            384\n","│    │    └─Attention: 3-65                        [32, 197, 192]            148,224\n","│    │    └─Identity: 3-66                         [32, 197, 192]            --\n","│    │    └─LayerNorm: 3-67                        [32, 197, 192]            384\n","│    │    └─LocallyEnhancedFeedForward: 3-68       [32, 197, 192]            307,008\n","│    │    └─Identity: 3-69                         [32, 197, 192]            --\n","│    └─Block: 2-14                                 [32, 197, 192]            --\n","│    │    └─LayerNorm: 3-70                        [32, 197, 192]            384\n","│    │    └─Attention: 3-71                        [32, 197, 192]            148,224\n","│    │    └─Identity: 3-72                         [32, 197, 192]            --\n","│    │    └─LayerNorm: 3-73                        [32, 197, 192]            384\n","│    │    └─LocallyEnhancedFeedForward: 3-74       [32, 197, 192]            307,008\n","│    │    └─Identity: 3-75                         [32, 197, 192]            --\n","├─Block: 1-4                                       [32, 1, 192]              --\n","│    └─LayerNorm: 2-15                             [32, 12, 192]             384\n","│    └─AttentionLCA: 2-16                          [32, 1, 192]              111,168\n","│    │    └─Dropout: 3-76                          [32, 3, 1, 12]            --\n","│    │    └─Linear: 3-77                           [32, 1, 192]              37,056\n","│    │    └─Dropout: 3-78                          [32, 1, 192]              --\n","│    └─Identity: 2-17                              [32, 1, 192]              --\n","│    └─LayerNorm: 2-18                             [32, 1, 192]              384\n","│    └─Mlp: 2-19                                   [32, 1, 192]              --\n","│    │    └─Linear: 3-79                           [32, 1, 768]              148,224\n","│    │    └─GELU: 3-80                             [32, 1, 768]              --\n","│    │    └─Dropout: 3-81                          [32, 1, 768]              --\n","│    │    └─Linear: 3-82                           [32, 1, 192]              147,648\n","│    │    └─Dropout: 3-83                          [32, 1, 192]              --\n","│    └─Identity: 2-20                              [32, 1, 192]              --\n","├─LayerNorm: 1-5                                   [32, 1, 192]              384\n","├─Linear: 1-6                                      [32, 4]                   772\n","====================================================================================================\n","Total params: 6,164,676\n","Trainable params: 6,164,676\n","Non-trainable params: 0\n","Total mult-adds (G): 27.93\n","====================================================================================================\n","Input size (MB): 19.27\n","Forward/backward pass size (MB): 3199.73\n","Params size (MB): 24.05\n","Estimated Total Size (MB): 3243.05\n","===================================================================================================="]},"execution_count":25,"metadata":{},"output_type":"execute_result"}],"source":["import torchinfo\n","model=ceit_tiny_patch16_224(num_classes=4)\n","torchinfo.summary(model,(32,3,224,224))"]},{"cell_type":"code","execution_count":26,"metadata":{"execution":{"iopub.execute_input":"2024-03-08T04:10:52.409625Z","iopub.status.busy":"2024-03-08T04:10:52.408745Z","iopub.status.idle":"2024-03-08T04:10:52.415158Z","shell.execute_reply":"2024-03-08T04:10:52.414208Z","shell.execute_reply.started":"2024-03-08T04:10:52.409591Z"},"trusted":true},"outputs":[{"data":{"text/plain":["4"]},"execution_count":26,"metadata":{},"output_type":"execute_result"}],"source":["len(CLASSES)"]},{"cell_type":"code","execution_count":28,"metadata":{"execution":{"iopub.execute_input":"2024-03-08T04:10:57.636407Z","iopub.status.busy":"2024-03-08T04:10:57.635708Z","iopub.status.idle":"2024-03-08T04:10:57.641849Z","shell.execute_reply":"2024-03-08T04:10:57.640919Z","shell.execute_reply.started":"2024-03-08T04:10:57.636374Z"},"trusted":true},"outputs":[],"source":["\n","dataloaders = {\n","    \"train\": train_dataloader,\n","    \"val\": valid_dataloader,\n","    #\"test\": test_dataloader\n","}\n","\n","dataset_sizes = {\n","    \"train\": train_len,\n","    \"val\": valid_len,\n","    #\"test\": test_len\n","}\n"]},{"cell_type":"code","execution_count":29,"metadata":{"execution":{"iopub.execute_input":"2024-03-08T04:10:58.958657Z","iopub.status.busy":"2024-03-08T04:10:58.958280Z","iopub.status.idle":"2024-03-08T04:10:58.971919Z","shell.execute_reply":"2024-03-08T04:10:58.970860Z","shell.execute_reply.started":"2024-03-08T04:10:58.958629Z"},"trusted":true},"outputs":[],"source":["model.to(device)\n","\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(model.parameters(),lr=0.001,weight_decay=0.00001)\n","criterion = criterion.to(device)\n","exp_lr_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=15, gamma=0.97)"]},{"cell_type":"code","execution_count":30,"metadata":{"execution":{"iopub.execute_input":"2024-03-08T04:11:01.078632Z","iopub.status.busy":"2024-03-08T04:11:01.078264Z","iopub.status.idle":"2024-03-08T04:11:01.099539Z","shell.execute_reply":"2024-03-08T04:11:01.098629Z","shell.execute_reply.started":"2024-03-08T04:11:01.078605Z"},"trusted":true},"outputs":[],"source":["import time\n","import copy\n","import torch\n","import pandas as pd\n","import os\n","from tqdm import tqdm\n","\n","def train_model(model, criterion, optimizer, exp_lr_scheduler, num_epochs=75, load_checkpoint=False, checkpoint_path=None):\n","    since = time.time()\n","    best_model_wts = copy.deepcopy(model.state_dict())\n","    best_acc = 0.0\n","    df_train = pd.DataFrame(columns=['epoch', 'train_loss', 'train_acc'])\n","    df_val = pd.DataFrame(columns=['epoch', 'val_loss', 'val_acc'])\n","    if load_checkpoint:\n","        if checkpoint_path is None:\n","            raise ValueError(\"Checkpoint path is not specified.\")\n","\n","        checkpoint = torch.load(checkpoint_path)\n","        model.load_state_dict(checkpoint['model_state_dict'])\n","        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n","        original_learning_rate = optimizer.param_groups[0]['lr']\n","        print(f\"Original Learning Rate: {original_learning_rate}\")\n","        loss = checkpoint['loss']\n","\n","        start_epoch = 90\n","    else:\n","        start_epoch = 0\n","\n","    for epoch in range(start_epoch, num_epochs):\n","        print(f'Epoch {epoch}/{num_epochs - 1}')\n","        print(\"-\" * 10)\n","\n","        if epoch == 0:\n","            if not os.path.isdir(\"/kaggle/working/\"):\n","                os.mkdir(\"/kaggle/working/\")\n","\n","        for phase in ['train', 'val']:  # We do training and validation phase per epoch\n","            if phase == 'train':\n","                model.train()  # model to training mode\n","            else:\n","                model.eval()  # model to evaluate\n","\n","            running_loss = 0.0\n","            running_corrects = 0.0\n","            total_samples = 0\n","\n","            progress_bar = tqdm(dataloaders[phase], desc=f'{phase.capitalize()} Epoch {epoch}/{num_epochs - 1}', leave=False)\n","\n","            for inputs, labels in progress_bar:\n","                inputs = inputs.to(device)\n","                labels = labels.to(device)\n","\n","                optimizer.zero_grad()\n","\n","                with torch.set_grad_enabled(phase == 'train'):  # no autograd makes validation go faster\n","                    outputs = model(inputs)\n","                    _, preds = torch.max(outputs, 1)  # used for accuracy\n","                    loss = criterion(outputs, labels)\n","\n","                    if phase == 'train':\n","                        loss.backward()\n","                        optimizer.step()\n","\n","                running_loss += loss.item() * inputs.size(0)\n","                running_corrects += torch.sum(preds == labels.data)\n","                total_samples += labels.size(0)\n","\n","                # Update progress bar description with live accuracy\n","                accuracy = running_corrects.double() / total_samples\n","                progress_bar.set_postfix(loss=running_loss / total_samples, accuracy=accuracy)\n","\n","            if phase == 'train':\n","                exp_lr_scheduler.step()  # step at the end of the epoch\n","\n","            epoch_loss = running_loss / dataset_sizes[phase]\n","            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n","\n","            if phase == 'train':\n","                df_new_row = pd.DataFrame({'epoch': [epoch], 'train_loss': [epoch_loss], 'train_acc': [epoch_acc.cpu()]})\n","                df_train = pd.concat([df_train, df_new_row])\n","                df_train.to_csv('train_metrics.csv')\n","            elif phase == 'val':\n","                df_new_row = pd.DataFrame({'epoch': [epoch], 'val_loss': [epoch_loss], 'val_acc': [epoch_acc.cpu()]})\n","                df_val = pd.concat([df_val, df_new_row])\n","                df_val.to_csv('val_metrics.csv')\n","\n","            print(\"{} Loss: {:.4f} Acc: {:.4f}\".format(phase, epoch_loss, epoch_acc))\n","\n","            # Save torch model for checkpoints\n","            if epoch % 9 == 0:\n","                torch.save({\n","                    'epoch': epoch,\n","                    'model_state_dict': model.state_dict(),\n","                    'optimizer_state_dict': optimizer.state_dict(),\n","                    'loss': epoch_loss,\n","                }, f\"/kaggle/working/sav_model{epoch}.pt\")\n","\n","            if phase == 'val' and epoch_acc > best_acc:\n","                best_acc = epoch_acc\n","                best_model_wts = copy.deepcopy(model.state_dict())  # keep the best validation accuracy model\n","        print()\n","\n","    time_elapsed = time.time() - since\n","    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n","    print(\"Best Val Acc: {:.4f}\".format(best_acc))\n","\n","    model.load_state_dict(best_model_wts)\n","    return model, df_train, df_val\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-08T04:11:03.905805Z","iopub.status.busy":"2024-03-08T04:11:03.904963Z"},"trusted":true},"outputs":[],"source":["model, df_train, df_val = train_model(model, criterion, optimizer, exp_lr_scheduler, num_epochs=100, load_checkpoint=False, checkpoint_path=None)\n","# Save the best model weights at the end of training\n","torch.save(model.state_dict(), '/kaggle/working/best_model_weights.pth')\n","\n"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":4548795,"sourceId":7774637,"sourceType":"datasetVersion"}],"dockerImageVersionId":30665,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
