{"cells":[{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-03-08T03:16:08.279814Z","iopub.status.busy":"2024-03-08T03:16:08.279257Z","iopub.status.idle":"2024-03-08T03:16:08.287333Z","shell.execute_reply":"2024-03-08T03:16:08.286423Z","shell.execute_reply.started":"2024-03-08T03:16:08.279783Z"},"trusted":true},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","import pandas as pd\n","import zipfile\n","import os\n","import cv2\n","import seaborn as sns\n","from collections import Counter\n","# from google.colab.patches import cv2_imshow\n","from pathlib import Path\n","import random\n","\n","from sklearn.preprocessing import MinMaxScaler\n","from sklearn.decomposition import PCA\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn.model_selection import train_test_split\n","from sklearn.neighbors import NearestNeighbors\n","from imblearn.over_sampling import RandomOverSampler\n","from imblearn.over_sampling import ADASYN\n","import PIL\n","import torch\n","from torch import nn\n","from torch.utils.data import DataLoader\n","from torchvision import datasets, transforms\n","from torchvision.transforms import ToTensor\n","import torchvision\n","import torch.optim as optim\n","from functools import partial\n"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-03-08T03:16:11.011949Z","iopub.status.busy":"2024-03-08T03:16:11.011104Z","iopub.status.idle":"2024-03-08T03:16:11.016182Z","shell.execute_reply":"2024-03-08T03:16:11.015184Z","shell.execute_reply.started":"2024-03-08T03:16:11.011917Z"},"trusted":true},"outputs":[],"source":["def get_classes(data_dir):\n","    all_data = datasets.ImageFolder(data_dir)\n","    return all_data.classes"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-03-08T03:16:13.898166Z","iopub.status.busy":"2024-03-08T03:16:13.897794Z","iopub.status.idle":"2024-03-08T03:16:20.974742Z","shell.execute_reply":"2024-03-08T03:16:20.973810Z","shell.execute_reply.started":"2024-03-08T03:16:13.898135Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["['both', 'infection', 'ischaemia', 'none']\n"]}],"source":["test_transform = transforms.Compose([\n","                                transforms.Resize((224, 224)),\n","                                transforms.ToTensor()\n","                                    ])\n","\n","import torchvision.transforms as transforms\n","\n","transform = transforms.Compose([\n","    transforms.Resize((224, 224)),\n","    transforms.ToTensor(),\n","  \n","])\n","\n","\n","\n","\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","kwargs = {'num_workers': 1, 'pin_memory': True} if device=='cuda' else {}\n","\n","train_dataset_path = '/kaggle/input/dfuc-2021-split/A NEW DATASET SPLIT/train'\n","valid_dataset_path = '/kaggle/input/dfuc-2021-split/A NEW DATASET SPLIT/valid'\n","#test_dataset_path = '/kaggle/input/for-trial/new_tts_aug/test'\n","\n","train_dataset = datasets.ImageFolder(train_dataset_path, transform=transform)\n","valid_dataset = datasets.ImageFolder(valid_dataset_path, transform=transform)\n","#test_dataset = datasets.ImageFolder(test_dataset_path,transform=test_transform)\n","\n","train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=32,shuffle=True,**kwargs)\n","valid_dataloader = torch.utils.data.DataLoader(valid_dataset, batch_size=32,shuffle=True,**kwargs)\n","#test_dataloader =  torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=True, **kwargs)\n","\n","\n","CLASSES = train_dataset.classes\n","train_len = len(train_dataset)\n","valid_len = len(valid_dataset)\n","#test_len = len(test_dataset)\n","print(CLASSES)"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-03-08T03:16:23.524109Z","iopub.status.busy":"2024-03-08T03:16:23.523697Z","iopub.status.idle":"2024-03-08T03:16:27.274592Z","shell.execute_reply":"2024-03-08T03:16:27.273689Z","shell.execute_reply.started":"2024-03-08T03:16:23.524073Z"},"trusted":true},"outputs":[{"data":{"text/plain":["===============================================================================================\n","Layer (type:depth-idx)                        Output Shape              Param #\n","===============================================================================================\n","cait_models                                   [32, 4]                   75,648\n","├─PatchEmbed: 1-1                             [32, 196, 384]            --\n","│    └─Conv2d: 2-1                            [32, 384, 14, 14]         295,296\n","│    └─Identity: 2-2                          [32, 196, 384]            --\n","├─Dropout: 1-2                                [32, 196, 384]            --\n","├─ModuleList: 1-3                             --                        --\n","│    └─LayerScale_Block: 2-3                  [32, 196, 384]            768\n","│    │    └─LayerNorm: 3-1                    [32, 196, 384]            768\n","│    │    └─Attention_talking_head: 3-2       [32, 196, 384]            591,504\n","│    │    └─Identity: 3-3                     [32, 196, 384]            --\n","│    │    └─LayerNorm: 3-4                    [32, 196, 384]            768\n","│    │    └─Mlp: 3-5                          [32, 196, 384]            1,181,568\n","│    │    └─Identity: 3-6                     [32, 196, 384]            --\n","│    └─LayerScale_Block: 2-4                  [32, 196, 384]            768\n","│    │    └─LayerNorm: 3-7                    [32, 196, 384]            768\n","│    │    └─Attention_talking_head: 3-8       [32, 196, 384]            591,504\n","│    │    └─Identity: 3-9                     [32, 196, 384]            --\n","│    │    └─LayerNorm: 3-10                   [32, 196, 384]            768\n","│    │    └─Mlp: 3-11                         [32, 196, 384]            1,181,568\n","│    │    └─Identity: 3-12                    [32, 196, 384]            --\n","│    └─LayerScale_Block: 2-5                  [32, 196, 384]            768\n","│    │    └─LayerNorm: 3-13                   [32, 196, 384]            768\n","│    │    └─Attention_talking_head: 3-14      [32, 196, 384]            591,504\n","│    │    └─Identity: 3-15                    [32, 196, 384]            --\n","│    │    └─LayerNorm: 3-16                   [32, 196, 384]            768\n","│    │    └─Mlp: 3-17                         [32, 196, 384]            1,181,568\n","│    │    └─Identity: 3-18                    [32, 196, 384]            --\n","│    └─LayerScale_Block: 2-6                  [32, 196, 384]            768\n","│    │    └─LayerNorm: 3-19                   [32, 196, 384]            768\n","│    │    └─Attention_talking_head: 3-20      [32, 196, 384]            591,504\n","│    │    └─Identity: 3-21                    [32, 196, 384]            --\n","│    │    └─LayerNorm: 3-22                   [32, 196, 384]            768\n","│    │    └─Mlp: 3-23                         [32, 196, 384]            1,181,568\n","│    │    └─Identity: 3-24                    [32, 196, 384]            --\n","│    └─LayerScale_Block: 2-7                  [32, 196, 384]            768\n","│    │    └─LayerNorm: 3-25                   [32, 196, 384]            768\n","│    │    └─Attention_talking_head: 3-26      [32, 196, 384]            591,504\n","│    │    └─Identity: 3-27                    [32, 196, 384]            --\n","│    │    └─LayerNorm: 3-28                   [32, 196, 384]            768\n","│    │    └─Mlp: 3-29                         [32, 196, 384]            1,181,568\n","│    │    └─Identity: 3-30                    [32, 196, 384]            --\n","│    └─LayerScale_Block: 2-8                  [32, 196, 384]            768\n","│    │    └─LayerNorm: 3-31                   [32, 196, 384]            768\n","│    │    └─Attention_talking_head: 3-32      [32, 196, 384]            591,504\n","│    │    └─Identity: 3-33                    [32, 196, 384]            --\n","│    │    └─LayerNorm: 3-34                   [32, 196, 384]            768\n","│    │    └─Mlp: 3-35                         [32, 196, 384]            1,181,568\n","│    │    └─Identity: 3-36                    [32, 196, 384]            --\n","│    └─LayerScale_Block: 2-9                  [32, 196, 384]            768\n","│    │    └─LayerNorm: 3-37                   [32, 196, 384]            768\n","│    │    └─Attention_talking_head: 3-38      [32, 196, 384]            591,504\n","│    │    └─Identity: 3-39                    [32, 196, 384]            --\n","│    │    └─LayerNorm: 3-40                   [32, 196, 384]            768\n","│    │    └─Mlp: 3-41                         [32, 196, 384]            1,181,568\n","│    │    └─Identity: 3-42                    [32, 196, 384]            --\n","│    └─LayerScale_Block: 2-10                 [32, 196, 384]            768\n","│    │    └─LayerNorm: 3-43                   [32, 196, 384]            768\n","│    │    └─Attention_talking_head: 3-44      [32, 196, 384]            591,504\n","│    │    └─Identity: 3-45                    [32, 196, 384]            --\n","│    │    └─LayerNorm: 3-46                   [32, 196, 384]            768\n","│    │    └─Mlp: 3-47                         [32, 196, 384]            1,181,568\n","│    │    └─Identity: 3-48                    [32, 196, 384]            --\n","│    └─LayerScale_Block: 2-11                 [32, 196, 384]            768\n","│    │    └─LayerNorm: 3-49                   [32, 196, 384]            768\n","│    │    └─Attention_talking_head: 3-50      [32, 196, 384]            591,504\n","│    │    └─Identity: 3-51                    [32, 196, 384]            --\n","│    │    └─LayerNorm: 3-52                   [32, 196, 384]            768\n","│    │    └─Mlp: 3-53                         [32, 196, 384]            1,181,568\n","│    │    └─Identity: 3-54                    [32, 196, 384]            --\n","│    └─LayerScale_Block: 2-12                 [32, 196, 384]            768\n","│    │    └─LayerNorm: 3-55                   [32, 196, 384]            768\n","│    │    └─Attention_talking_head: 3-56      [32, 196, 384]            591,504\n","│    │    └─Identity: 3-57                    [32, 196, 384]            --\n","│    │    └─LayerNorm: 3-58                   [32, 196, 384]            768\n","│    │    └─Mlp: 3-59                         [32, 196, 384]            1,181,568\n","│    │    └─Identity: 3-60                    [32, 196, 384]            --\n","│    └─LayerScale_Block: 2-13                 [32, 196, 384]            768\n","│    │    └─LayerNorm: 3-61                   [32, 196, 384]            768\n","│    │    └─Attention_talking_head: 3-62      [32, 196, 384]            591,504\n","│    │    └─Identity: 3-63                    [32, 196, 384]            --\n","│    │    └─LayerNorm: 3-64                   [32, 196, 384]            768\n","│    │    └─Mlp: 3-65                         [32, 196, 384]            1,181,568\n","│    │    └─Identity: 3-66                    [32, 196, 384]            --\n","│    └─LayerScale_Block: 2-14                 [32, 196, 384]            768\n","│    │    └─LayerNorm: 3-67                   [32, 196, 384]            768\n","│    │    └─Attention_talking_head: 3-68      [32, 196, 384]            591,504\n","│    │    └─Identity: 3-69                    [32, 196, 384]            --\n","│    │    └─LayerNorm: 3-70                   [32, 196, 384]            768\n","│    │    └─Mlp: 3-71                         [32, 196, 384]            1,181,568\n","│    │    └─Identity: 3-72                    [32, 196, 384]            --\n","│    └─LayerScale_Block: 2-15                 [32, 196, 384]            768\n","│    │    └─LayerNorm: 3-73                   [32, 196, 384]            768\n","│    │    └─Attention_talking_head: 3-74      [32, 196, 384]            591,504\n","│    │    └─Identity: 3-75                    [32, 196, 384]            --\n","│    │    └─LayerNorm: 3-76                   [32, 196, 384]            768\n","│    │    └─Mlp: 3-77                         [32, 196, 384]            1,181,568\n","│    │    └─Identity: 3-78                    [32, 196, 384]            --\n","│    └─LayerScale_Block: 2-16                 [32, 196, 384]            768\n","│    │    └─LayerNorm: 3-79                   [32, 196, 384]            768\n","│    │    └─Attention_talking_head: 3-80      [32, 196, 384]            591,504\n","│    │    └─Identity: 3-81                    [32, 196, 384]            --\n","│    │    └─LayerNorm: 3-82                   [32, 196, 384]            768\n","│    │    └─Mlp: 3-83                         [32, 196, 384]            1,181,568\n","│    │    └─Identity: 3-84                    [32, 196, 384]            --\n","│    └─LayerScale_Block: 2-17                 [32, 196, 384]            768\n","│    │    └─LayerNorm: 3-85                   [32, 196, 384]            768\n","│    │    └─Attention_talking_head: 3-86      [32, 196, 384]            591,504\n","│    │    └─Identity: 3-87                    [32, 196, 384]            --\n","│    │    └─LayerNorm: 3-88                   [32, 196, 384]            768\n","│    │    └─Mlp: 3-89                         [32, 196, 384]            1,181,568\n","│    │    └─Identity: 3-90                    [32, 196, 384]            --\n","│    └─LayerScale_Block: 2-18                 [32, 196, 384]            768\n","│    │    └─LayerNorm: 3-91                   [32, 196, 384]            768\n","│    │    └─Attention_talking_head: 3-92      [32, 196, 384]            591,504\n","│    │    └─Identity: 3-93                    [32, 196, 384]            --\n","│    │    └─LayerNorm: 3-94                   [32, 196, 384]            768\n","│    │    └─Mlp: 3-95                         [32, 196, 384]            1,181,568\n","│    │    └─Identity: 3-96                    [32, 196, 384]            --\n","│    └─LayerScale_Block: 2-19                 [32, 196, 384]            768\n","│    │    └─LayerNorm: 3-97                   [32, 196, 384]            768\n","│    │    └─Attention_talking_head: 3-98      [32, 196, 384]            591,504\n","│    │    └─Identity: 3-99                    [32, 196, 384]            --\n","│    │    └─LayerNorm: 3-100                  [32, 196, 384]            768\n","│    │    └─Mlp: 3-101                        [32, 196, 384]            1,181,568\n","│    │    └─Identity: 3-102                   [32, 196, 384]            --\n","│    └─LayerScale_Block: 2-20                 [32, 196, 384]            768\n","│    │    └─LayerNorm: 3-103                  [32, 196, 384]            768\n","│    │    └─Attention_talking_head: 3-104     [32, 196, 384]            591,504\n","│    │    └─Identity: 3-105                   [32, 196, 384]            --\n","│    │    └─LayerNorm: 3-106                  [32, 196, 384]            768\n","│    │    └─Mlp: 3-107                        [32, 196, 384]            1,181,568\n","│    │    └─Identity: 3-108                   [32, 196, 384]            --\n","│    └─LayerScale_Block: 2-21                 [32, 196, 384]            768\n","│    │    └─LayerNorm: 3-109                  [32, 196, 384]            768\n","│    │    └─Attention_talking_head: 3-110     [32, 196, 384]            591,504\n","│    │    └─Identity: 3-111                   [32, 196, 384]            --\n","│    │    └─LayerNorm: 3-112                  [32, 196, 384]            768\n","│    │    └─Mlp: 3-113                        [32, 196, 384]            1,181,568\n","│    │    └─Identity: 3-114                   [32, 196, 384]            --\n","│    └─LayerScale_Block: 2-22                 [32, 196, 384]            768\n","│    │    └─LayerNorm: 3-115                  [32, 196, 384]            768\n","│    │    └─Attention_talking_head: 3-116     [32, 196, 384]            591,504\n","│    │    └─Identity: 3-117                   [32, 196, 384]            --\n","│    │    └─LayerNorm: 3-118                  [32, 196, 384]            768\n","│    │    └─Mlp: 3-119                        [32, 196, 384]            1,181,568\n","│    │    └─Identity: 3-120                   [32, 196, 384]            --\n","│    └─LayerScale_Block: 2-23                 [32, 196, 384]            768\n","│    │    └─LayerNorm: 3-121                  [32, 196, 384]            768\n","│    │    └─Attention_talking_head: 3-122     [32, 196, 384]            591,504\n","│    │    └─Identity: 3-123                   [32, 196, 384]            --\n","│    │    └─LayerNorm: 3-124                  [32, 196, 384]            768\n","│    │    └─Mlp: 3-125                        [32, 196, 384]            1,181,568\n","│    │    └─Identity: 3-126                   [32, 196, 384]            --\n","│    └─LayerScale_Block: 2-24                 [32, 196, 384]            768\n","│    │    └─LayerNorm: 3-127                  [32, 196, 384]            768\n","│    │    └─Attention_talking_head: 3-128     [32, 196, 384]            591,504\n","│    │    └─Identity: 3-129                   [32, 196, 384]            --\n","│    │    └─LayerNorm: 3-130                  [32, 196, 384]            768\n","│    │    └─Mlp: 3-131                        [32, 196, 384]            1,181,568\n","│    │    └─Identity: 3-132                   [32, 196, 384]            --\n","│    └─LayerScale_Block: 2-25                 [32, 196, 384]            768\n","│    │    └─LayerNorm: 3-133                  [32, 196, 384]            768\n","│    │    └─Attention_talking_head: 3-134     [32, 196, 384]            591,504\n","│    │    └─Identity: 3-135                   [32, 196, 384]            --\n","│    │    └─LayerNorm: 3-136                  [32, 196, 384]            768\n","│    │    └─Mlp: 3-137                        [32, 196, 384]            1,181,568\n","│    │    └─Identity: 3-138                   [32, 196, 384]            --\n","│    └─LayerScale_Block: 2-26                 [32, 196, 384]            768\n","│    │    └─LayerNorm: 3-139                  [32, 196, 384]            768\n","│    │    └─Attention_talking_head: 3-140     [32, 196, 384]            591,504\n","│    │    └─Identity: 3-141                   [32, 196, 384]            --\n","│    │    └─LayerNorm: 3-142                  [32, 196, 384]            768\n","│    │    └─Mlp: 3-143                        [32, 196, 384]            1,181,568\n","│    │    └─Identity: 3-144                   [32, 196, 384]            --\n","├─ModuleList: 1-4                             --                        --\n","│    └─LayerScale_Block_CA: 2-27              [32, 1, 384]              768\n","│    │    └─LayerNorm: 3-145                  [32, 197, 384]            768\n","│    │    └─Class_Attention: 3-146            [32, 1, 384]              591,360\n","│    │    └─Identity: 3-147                   [32, 1, 384]              --\n","│    │    └─LayerNorm: 3-148                  [32, 1, 384]              768\n","│    │    └─Mlp: 3-149                        [32, 1, 384]              1,181,568\n","│    │    └─Identity: 3-150                   [32, 1, 384]              --\n","│    └─LayerScale_Block_CA: 2-28              [32, 1, 384]              768\n","│    │    └─LayerNorm: 3-151                  [32, 197, 384]            768\n","│    │    └─Class_Attention: 3-152            [32, 1, 384]              591,360\n","│    │    └─Identity: 3-153                   [32, 1, 384]              --\n","│    │    └─LayerNorm: 3-154                  [32, 1, 384]              768\n","│    │    └─Mlp: 3-155                        [32, 1, 384]              1,181,568\n","│    │    └─Identity: 3-156                   [32, 1, 384]              --\n","├─LayerNorm: 1-5                              [32, 197, 384]            768\n","├─Linear: 1-6                                 [32, 4]                   1,540\n","===============================================================================================\n","Total params: 46,532,740\n","Trainable params: 46,532,740\n","Non-trainable params: 0\n","Total mult-adds (G): 3.33\n","===============================================================================================\n","Input size (MB): 19.27\n","Forward/backward pass size (MB): 9019.49\n","Params size (MB): 185.75\n","Estimated Total Size (MB): 9224.51\n","==============================================================================================="]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["# Copyright (c) 2015-present, Facebook, Inc.\n","# All rights reserved.\n","\n","import torch\n","import torch.nn as nn\n","from functools import partial\n","\n","from timm.models.vision_transformer import Mlp, PatchEmbed , _cfg\n","from timm.models.registry import register_model\n","from timm.models.layers import trunc_normal_, DropPath\n","\n","\n","__all__ = [\n","    'cait_M48', 'cait_M36',\n","    'cait_S36', 'cait_S24','cait_S24_224',\n","    'cait_XS24','cait_XXS24','cait_XXS24_224',\n","    'cait_XXS36','cait_XXS36_224'\n","]\n","\n","class Class_Attention(nn.Module):\n","    # taken from https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/vision_transformer.py\n","    # with slight modifications to do CA \n","    def __init__(self, dim, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0., proj_drop=0.):\n","        super().__init__()\n","        self.num_heads = num_heads\n","        head_dim = dim // num_heads\n","        self.scale = qk_scale or head_dim ** -0.5\n","\n","        self.q = nn.Linear(dim, dim, bias=qkv_bias)\n","        self.k = nn.Linear(dim, dim, bias=qkv_bias)\n","        self.v = nn.Linear(dim, dim, bias=qkv_bias)\n","        self.attn_drop = nn.Dropout(attn_drop)\n","        self.proj = nn.Linear(dim, dim)\n","        self.proj_drop = nn.Dropout(proj_drop)\n","        \n","\n","    \n","    def forward(self, x ):\n","        \n","        B, N, C = x.shape\n","        q = self.q(x[:,0]).unsqueeze(1).reshape(B, 1, self.num_heads, C // self.num_heads).permute(0, 2, 1, 3)\n","        k = self.k(x).reshape(B, N, self.num_heads, C // self.num_heads).permute(0, 2, 1, 3)\n","\n","        q = q * self.scale\n","        v = self.v(x).reshape(B, N, self.num_heads, C // self.num_heads).permute(0, 2, 1, 3)\n","\n","        attn = (q @ k.transpose(-2, -1)) \n","        attn = attn.softmax(dim=-1)\n","        attn = self.attn_drop(attn)\n","\n","        x_cls = (attn @ v).transpose(1, 2).reshape(B, 1, C)\n","        x_cls = self.proj(x_cls)\n","        x_cls = self.proj_drop(x_cls)\n","        \n","        return x_cls     \n","        \n","class LayerScale_Block_CA(nn.Module):\n","    # taken from https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/vision_transformer.py\n","    # with slight modifications to add CA and LayerScale\n","    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop=0., attn_drop=0.,\n","                 drop_path=0., act_layer=nn.GELU, norm_layer=nn.LayerNorm, Attention_block = Class_Attention,\n","                 Mlp_block=Mlp,init_values=1e-4):\n","        super().__init__()\n","        self.norm1 = norm_layer(dim)\n","        self.attn = Attention_block(\n","            dim, num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop)\n","        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n","        self.norm2 = norm_layer(dim)\n","        mlp_hidden_dim = int(dim * mlp_ratio)\n","        self.mlp = Mlp_block(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n","        self.gamma_1 = nn.Parameter(init_values * torch.ones((dim)),requires_grad=True)\n","        self.gamma_2 = nn.Parameter(init_values * torch.ones((dim)),requires_grad=True)\n","\n","    \n","    def forward(self, x, x_cls):\n","        \n","        u = torch.cat((x_cls,x),dim=1)\n","        \n","        \n","        x_cls = x_cls + self.drop_path(self.gamma_1 * self.attn(self.norm1(u)))\n","        \n","        x_cls = x_cls + self.drop_path(self.gamma_2 * self.mlp(self.norm2(x_cls)))\n","        \n","        return x_cls \n","        \n","        \n","class Attention_talking_head(nn.Module):\n","    # taken from https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/vision_transformer.py\n","    # with slight modifications to add Talking Heads Attention (https://arxiv.org/pdf/2003.02436v1.pdf)\n","    def __init__(self, dim, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0., proj_drop=0.):\n","        super().__init__()\n","        \n","        self.num_heads = num_heads\n","        \n","        head_dim = dim // num_heads\n","        \n","        self.scale = qk_scale or head_dim ** -0.5\n","\n","        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n","        self.attn_drop = nn.Dropout(attn_drop)\n","        \n","        self.proj = nn.Linear(dim, dim)\n","        \n","        self.proj_l = nn.Linear(num_heads, num_heads)\n","        self.proj_w = nn.Linear(num_heads, num_heads)\n","        \n","        self.proj_drop = nn.Dropout(proj_drop)\n","\n","\n","    \n","    def forward(self, x):\n","        B, N, C = x.shape\n","        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n","        q, k, v = qkv[0] * self.scale , qkv[1], qkv[2] \n","    \n","        attn = (q @ k.transpose(-2, -1)) \n","        \n","        attn = self.proj_l(attn.permute(0,2,3,1)).permute(0,3,1,2)\n","                \n","        attn = attn.softmax(dim=-1)\n","  \n","        attn = self.proj_w(attn.permute(0,2,3,1)).permute(0,3,1,2)\n","        attn = self.attn_drop(attn)\n","\n","        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n","        x = self.proj(x)\n","        x = self.proj_drop(x)\n","        return x\n","    \n","class LayerScale_Block(nn.Module):\n","    # taken from https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/vision_transformer.py\n","    # with slight modifications to add layerScale\n","    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop=0., attn_drop=0.,\n","                 drop_path=0., act_layer=nn.GELU, norm_layer=nn.LayerNorm,Attention_block = Attention_talking_head,\n","                 Mlp_block=Mlp,init_values=1e-4):\n","        super().__init__()\n","        self.norm1 = norm_layer(dim)\n","        self.attn = Attention_block(\n","            dim, num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop)\n","        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n","        self.norm2 = norm_layer(dim)\n","        mlp_hidden_dim = int(dim * mlp_ratio)\n","        self.mlp = Mlp_block(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n","        self.gamma_1 = nn.Parameter(init_values * torch.ones((dim)),requires_grad=True)\n","        self.gamma_2 = nn.Parameter(init_values * torch.ones((dim)),requires_grad=True)\n","\n","    def forward(self, x):        \n","        x = x + self.drop_path(self.gamma_1 * self.attn(self.norm1(x)))\n","        x = x + self.drop_path(self.gamma_2 * self.mlp(self.norm2(x)))\n","        return x \n","    \n","    \n","    \n","    \n","class cait_models(nn.Module):\n","    # taken from https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/vision_transformer.py\n","    # with slight modifications to adapt to our cait models\n","    def __init__(self, img_size=224, patch_size=16, in_chans=3, num_classes=1000, embed_dim=768, depth=12,\n","                 num_heads=12, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop_rate=0., attn_drop_rate=0.,\n","                 drop_path_rate=0., norm_layer=nn.LayerNorm, global_pool=None,\n","                 block_layers = LayerScale_Block,\n","                 block_layers_token = LayerScale_Block_CA,\n","                 Patch_layer=PatchEmbed,act_layer=nn.GELU,\n","                 Attention_block = Attention_talking_head,Mlp_block=Mlp,\n","                init_scale=1e-4,\n","                Attention_block_token_only=Class_Attention,\n","                Mlp_block_token_only= Mlp, \n","                depth_token_only=2,\n","                mlp_ratio_clstk = 4.0):\n","        super().__init__()\n","        \n","\n","            \n","        self.num_classes = num_classes\n","        self.num_features = self.embed_dim = embed_dim  \n","\n","        self.patch_embed = Patch_layer(\n","                img_size=img_size, patch_size=patch_size, in_chans=in_chans, embed_dim=embed_dim)\n","        \n","        num_patches = self.patch_embed.num_patches\n","\n","        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n","        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches, embed_dim))\n","        self.pos_drop = nn.Dropout(p=drop_rate)\n","\n","        dpr = [drop_path_rate for i in range(depth)] \n","        self.blocks = nn.ModuleList([\n","            block_layers(\n","                dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale,\n","                drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[i], norm_layer=norm_layer,\n","                act_layer=act_layer,Attention_block=Attention_block,Mlp_block=Mlp_block,init_values=init_scale)\n","            for i in range(depth)])\n","        \n","\n","        self.blocks_token_only = nn.ModuleList([\n","            block_layers_token(\n","                dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio_clstk, qkv_bias=qkv_bias, qk_scale=qk_scale,\n","                drop=0.0, attn_drop=0.0, drop_path=0.0, norm_layer=norm_layer,\n","                act_layer=act_layer,Attention_block=Attention_block_token_only,\n","                Mlp_block=Mlp_block_token_only,init_values=init_scale)\n","            for i in range(depth_token_only)])\n","            \n","        self.norm = norm_layer(embed_dim)\n","\n","\n","        self.feature_info = [dict(num_chs=embed_dim, reduction=0, module='head')]\n","        self.head = nn.Linear(embed_dim, num_classes) if num_classes > 0 else nn.Identity()\n","\n","        trunc_normal_(self.pos_embed, std=.02)\n","        trunc_normal_(self.cls_token, std=.02)\n","        self.apply(self._init_weights)\n","\n","    def _init_weights(self, m):\n","        if isinstance(m, nn.Linear):\n","            trunc_normal_(m.weight, std=.02)\n","            if m.bias is not None:\n","                nn.init.constant_(m.bias, 0)\n","        elif isinstance(m, nn.LayerNorm):\n","            nn.init.constant_(m.bias, 0)\n","            nn.init.constant_(m.weight, 1.0)\n","\n","    @torch.jit.ignore\n","    def no_weight_decay(self):\n","        return {'pos_embed', 'cls_token'}\n","\n","\n","    def forward_features(self, x):\n","        B = x.shape[0]\n","        x = self.patch_embed(x)\n","\n","        cls_tokens = self.cls_token.expand(B, -1, -1)  \n","        \n","        x = x + self.pos_embed\n","        x = self.pos_drop(x)\n","\n","        for i , blk in enumerate(self.blocks):\n","            x = blk(x)\n","            \n","        for i , blk in enumerate(self.blocks_token_only):\n","            cls_tokens = blk(x,cls_tokens)\n","\n","        x = torch.cat((cls_tokens, x), dim=1)\n","            \n","                \n","        x = self.norm(x)\n","        return x[:, 0]\n","\n","    def forward(self, x):\n","        x = self.forward_features(x)\n","        \n","        x = self.head(x)\n","\n","        return x \n","        \n","@register_model\n","def cait_XXS24_224(pretrained=False, **kwargs):\n","    model = cait_models(\n","        img_size= 224,patch_size=16, embed_dim=192, depth=24, num_heads=4, mlp_ratio=4, qkv_bias=True,\n","        norm_layer=partial(nn.LayerNorm, eps=1e-6),\n","        init_scale=1e-5,\n","        depth_token_only=2,**kwargs)\n","    \n","    model.default_cfg = _cfg()\n","    if pretrained:\n","        checkpoint = torch.hub.load_state_dict_from_url(\n","            url=\"https://dl.fbaipublicfiles.com/deit/XXS24_224.pth\",\n","            map_location=\"cpu\", check_hash=True\n","        )\n","        checkpoint_no_module = {}\n","        for k in model.state_dict().keys():\n","            checkpoint_no_module[k] = checkpoint[\"model\"]['module.'+k]\n","            \n","        model.load_state_dict(checkpoint_no_module)\n","        \n","    return model \n","\n","@register_model\n","def cait_XXS24(pretrained=False, **kwargs):\n","    model = cait_models(\n","        img_size= 384,patch_size=16, embed_dim=192, depth=24, num_heads=4, mlp_ratio=4, qkv_bias=True,\n","        norm_layer=partial(nn.LayerNorm, eps=1e-6),\n","        init_scale=1e-5,\n","        depth_token_only=2,**kwargs)\n","    \n","    model.default_cfg = _cfg()\n","    if pretrained:\n","        checkpoint = torch.hub.load_state_dict_from_url(\n","            url=\"https://dl.fbaipublicfiles.com/deit/XXS24_384.pth\",\n","            map_location=\"cpu\", check_hash=True\n","        )\n","        checkpoint_no_module = {}\n","        for k in model.state_dict().keys():\n","            checkpoint_no_module[k] = checkpoint[\"model\"]['module.'+k]\n","            \n","        model.load_state_dict(checkpoint_no_module)\n","        \n","    return model \n","@register_model\n","def cait_XXS36_224(pretrained=False, **kwargs):\n","    model = cait_models(\n","        img_size= 224,patch_size=16, embed_dim=192, depth=36, num_heads=4, mlp_ratio=4, qkv_bias=True,\n","        norm_layer=partial(nn.LayerNorm, eps=1e-6),\n","        init_scale=1e-5,\n","        depth_token_only=2,**kwargs)\n","    \n","    model.default_cfg = _cfg()\n","    if pretrained:\n","        checkpoint = torch.hub.load_state_dict_from_url(\n","            url=\"https://dl.fbaipublicfiles.com/deit/XXS36_224.pth\",\n","            map_location=\"cpu\", check_hash=True\n","        )\n","        checkpoint_no_module = {}\n","        for k in model.state_dict().keys():\n","            checkpoint_no_module[k] = checkpoint[\"model\"]['module.'+k]\n","            \n","        model.load_state_dict(checkpoint_no_module)\n","        \n","    return model \n","\n","@register_model\n","def cait_XXS36(pretrained=False, **kwargs):\n","    model = cait_models(\n","        img_size= 384,patch_size=16, embed_dim=192, depth=36, num_heads=4, mlp_ratio=4, qkv_bias=True,\n","        norm_layer=partial(nn.LayerNorm, eps=1e-6),\n","        init_scale=1e-5,\n","        depth_token_only=2,**kwargs)\n","    \n","    model.default_cfg = _cfg()\n","    if pretrained:\n","        checkpoint = torch.hub.load_state_dict_from_url(\n","            url=\"https://dl.fbaipublicfiles.com/deit/XXS36_384.pth\",\n","            map_location=\"cpu\", check_hash=True\n","        )\n","        checkpoint_no_module = {}\n","        for k in model.state_dict().keys():\n","            checkpoint_no_module[k] = checkpoint[\"model\"]['module.'+k]\n","            \n","        model.load_state_dict(checkpoint_no_module)\n","        \n","    return model \n","\n","@register_model\n","def cait_XS24(pretrained=False, **kwargs):\n","    model = cait_models(\n","        img_size= 384,patch_size=16, embed_dim=288, depth=24, num_heads=6, mlp_ratio=4, qkv_bias=True,\n","        norm_layer=partial(nn.LayerNorm, eps=1e-6),\n","        init_scale=1e-5,\n","        depth_token_only=2,**kwargs)\n","    \n","    model.default_cfg = _cfg()\n","    if pretrained:\n","        checkpoint = torch.hub.load_state_dict_from_url(\n","            url=\"https://dl.fbaipublicfiles.com/deit/XS24_384.pth\",\n","            map_location=\"cpu\", check_hash=True\n","        )\n","        checkpoint_no_module = {}\n","        for k in model.state_dict().keys():\n","            checkpoint_no_module[k] = checkpoint[\"model\"]['module.'+k]\n","            \n","        model.load_state_dict(checkpoint_no_module)\n","        \n","    return model \n","\n","\n","\n","\n","@register_model\n","def cait_S24_224(pretrained=False, **kwargs):\n","    model = cait_models(\n","        img_size= 224,patch_size=16, embed_dim=384, depth=24, num_heads=8, mlp_ratio=4, qkv_bias=True,\n","        norm_layer=partial(nn.LayerNorm, eps=1e-6),\n","        init_scale=1e-5,\n","        depth_token_only=2,**kwargs)\n","    \n","    model.default_cfg = _cfg()\n","    if pretrained:\n","        checkpoint = torch.hub.load_state_dict_from_url(\n","            url=\"https://dl.fbaipublicfiles.com/deit/S24_224.pth\",\n","            map_location=\"cpu\", check_hash=True\n","        )\n","        checkpoint_no_module = {}\n","        for k in model.state_dict().keys():\n","            checkpoint_no_module[k] = checkpoint[\"model\"]['module.'+k]\n","            \n","        model.load_state_dict(checkpoint_no_module)\n","        \n","    return model \n","\n","@register_model\n","def cait_S24(pretrained=False, **kwargs):\n","    model = cait_models(\n","        img_size= 384,patch_size=16, embed_dim=384, depth=24, num_heads=8, mlp_ratio=4, qkv_bias=True,\n","        norm_layer=partial(nn.LayerNorm, eps=1e-6),\n","        init_scale=1e-5,\n","        depth_token_only=2,**kwargs)\n","    \n","    model.default_cfg = _cfg()\n","    if pretrained:\n","        checkpoint = torch.hub.load_state_dict_from_url(\n","            url=\"https://dl.fbaipublicfiles.com/deit/S24_384.pth\",\n","            map_location=\"cpu\", check_hash=True\n","        )\n","        checkpoint_no_module = {}\n","        for k in model.state_dict().keys():\n","            checkpoint_no_module[k] = checkpoint[\"model\"]['module.'+k]\n","            \n","        model.load_state_dict(checkpoint_no_module)\n","        \n","    return model \n","\n","@register_model\n","def cait_S36(pretrained=False, **kwargs):\n","    model = cait_models(\n","        img_size= 384,patch_size=16, embed_dim=384, depth=36, num_heads=8, mlp_ratio=4, qkv_bias=True,\n","        norm_layer=partial(nn.LayerNorm, eps=1e-6),\n","        init_scale=1e-6,\n","        depth_token_only=2,**kwargs)\n","    \n","    model.default_cfg = _cfg()\n","    if pretrained:\n","        checkpoint = torch.hub.load_state_dict_from_url(\n","            url=\"https://dl.fbaipublicfiles.com/deit/S36_384.pth\",\n","            map_location=\"cpu\", check_hash=True\n","        )\n","        checkpoint_no_module = {}\n","        for k in model.state_dict().keys():\n","            checkpoint_no_module[k] = checkpoint[\"model\"]['module.'+k]\n","            \n","        model.load_state_dict(checkpoint_no_module)\n","\n","    return model \n","\n","\n","\n","\n","\n","@register_model\n","def cait_M36(pretrained=False, **kwargs):\n","    model = cait_models(\n","        img_size= 384, patch_size=16, embed_dim=768, depth=36, num_heads=16, mlp_ratio=4, qkv_bias=True,\n","        norm_layer=partial(nn.LayerNorm, eps=1e-6),\n","        init_scale=1e-6,\n","        depth_token_only=2,**kwargs)\n","    \n","    model.default_cfg = _cfg()\n","    if pretrained:\n","        checkpoint = torch.hub.load_state_dict_from_url(\n","            url=\"https://dl.fbaipublicfiles.com/deit/M36_384.pth\",\n","            map_location=\"cpu\", check_hash=True\n","        )\n","        checkpoint_no_module = {}\n","        for k in model.state_dict().keys():\n","            checkpoint_no_module[k] = checkpoint[\"model\"]['module.'+k]\n","            \n","        model.load_state_dict(checkpoint_no_module)\n","\n","    return model \n","\n","\n","@register_model\n","def cait_M48(pretrained=False, **kwargs):\n","    model = cait_models(\n","        img_size= 448 , patch_size=16, embed_dim=768, depth=48, num_heads=16, mlp_ratio=4, qkv_bias=True,\n","        norm_layer=partial(nn.LayerNorm, eps=1e-6),\n","        init_scale=1e-6,\n","        depth_token_only=2,**kwargs)\n","    \n","    model.default_cfg = _cfg()\n","    if pretrained:\n","        checkpoint = torch.hub.load_state_dict_from_url(\n","            url=\"https://dl.fbaipublicfiles.com/deit/M48_448.pth\",\n","            map_location=\"cpu\", check_hash=True\n","        )\n","        checkpoint_no_module = {}\n","        for k in model.state_dict().keys():\n","            checkpoint_no_module[k] = checkpoint[\"model\"]['module.'+k]\n","            \n","        model.load_state_dict(checkpoint_no_module)\n","        \n","    return model   \n","\n","\n","model=cait_S24_224(num_classes=4)\n","import torchinfo\n","torchinfo.summary(model,(32,3,224,224))"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-03-08T03:16:35.447699Z","iopub.status.busy":"2024-03-08T03:16:35.447345Z","iopub.status.idle":"2024-03-08T03:16:35.454874Z","shell.execute_reply":"2024-03-08T03:16:35.453983Z","shell.execute_reply.started":"2024-03-08T03:16:35.447671Z"},"trusted":true},"outputs":[],"source":["len(CLASSES)\n","dataloaders = {\n","    \"train\": train_dataloader,\n","    \"val\": valid_dataloader,\n","    #\"test\": test_dataloader\n","}\n","\n","dataset_sizes = {\n","    \"train\": train_len,\n","    \"val\": valid_len,\n","    #\"test\": test_len\n","}\n"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-03-08T03:16:39.670583Z","iopub.status.busy":"2024-03-08T03:16:39.670235Z","iopub.status.idle":"2024-03-08T03:16:39.686772Z","shell.execute_reply":"2024-03-08T03:16:39.685883Z","shell.execute_reply.started":"2024-03-08T03:16:39.670555Z"},"trusted":true},"outputs":[],"source":["model.to(device)\n","\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(model.parameters(),lr=0.001,weight_decay=0.00001)\n","criterion = criterion.to(device)\n","exp_lr_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=15, gamma=0.97)"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-03-08T03:16:44.671817Z","iopub.status.busy":"2024-03-08T03:16:44.671104Z","iopub.status.idle":"2024-03-08T03:16:44.691629Z","shell.execute_reply":"2024-03-08T03:16:44.690384Z","shell.execute_reply.started":"2024-03-08T03:16:44.671789Z"},"trusted":true},"outputs":[],"source":["import time\n","import copy\n","import torch\n","import pandas as pd\n","import os\n","from tqdm import tqdm\n","\n","def train_model(model, criterion, optimizer, exp_lr_scheduler, num_epochs=75, load_checkpoint=False, checkpoint_path=None):\n","    since = time.time()\n","    best_model_wts = copy.deepcopy(model.state_dict())\n","    best_acc = 0.0\n","    df_train = pd.DataFrame(columns=['epoch', 'train_loss', 'train_acc'])\n","    df_val = pd.DataFrame(columns=['epoch', 'val_loss', 'val_acc'])\n","    if load_checkpoint:\n","        if checkpoint_path is None:\n","            raise ValueError(\"Checkpoint path is not specified.\")\n","\n","        checkpoint = torch.load(checkpoint_path)\n","        model.load_state_dict(checkpoint['model_state_dict'])\n","        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n","        original_learning_rate = optimizer.param_groups[0]['lr']\n","        print(f\"Original Learning Rate: {original_learning_rate}\")\n","        loss = checkpoint['loss']\n","\n","        start_epoch = 73\n","    else:\n","        start_epoch = 0\n","\n","    for epoch in range(start_epoch, num_epochs):\n","        print(f'Epoch {epoch}/{num_epochs - 1}')\n","        print(\"-\" * 10)\n","\n","        if epoch == 0:\n","            if not os.path.isdir(\"/kaggle/working/\"):\n","                os.mkdir(\"/kaggle/working/\")\n","\n","        for phase in ['train', 'val']:  # We do training and validation phase per epoch\n","            if phase == 'train':\n","                model.train()  # model to training mode\n","            else:\n","                model.eval()  # model to evaluate\n","\n","            running_loss = 0.0\n","            running_corrects = 0.0\n","            total_samples = 0\n","\n","            progress_bar = tqdm(dataloaders[phase], desc=f'{phase.capitalize()} Epoch {epoch}/{num_epochs - 1}', leave=False)\n","\n","            for inputs, labels in progress_bar:\n","                inputs = inputs.to(device)\n","                labels = labels.to(device)\n","\n","                optimizer.zero_grad()\n","\n","                with torch.set_grad_enabled(phase == 'train'):  # no autograd makes validation go faster\n","                    outputs = model(inputs)\n","                    _, preds = torch.max(outputs, 1)  # used for accuracy\n","                    loss = criterion(outputs, labels)\n","\n","                    if phase == 'train':\n","                        loss.backward()\n","                        optimizer.step()\n","\n","                running_loss += loss.item() * inputs.size(0)\n","                running_corrects += torch.sum(preds == labels.data)\n","                total_samples += labels.size(0)\n","\n","                # Update progress bar description with live accuracy\n","                accuracy = running_corrects.double() / total_samples\n","                progress_bar.set_postfix(loss=running_loss / total_samples, accuracy=accuracy)\n","\n","            if phase == 'train':\n","                exp_lr_scheduler.step()  # step at the end of the epoch\n","\n","            epoch_loss = running_loss / dataset_sizes[phase]\n","            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n","\n","            if phase == 'train':\n","                df_new_row = pd.DataFrame({'epoch': [epoch], 'train_loss': [epoch_loss], 'train_acc': [epoch_acc.cpu()]})\n","                df_train = pd.concat([df_train, df_new_row])\n","                df_train.to_csv('train_metrics.csv')\n","            elif phase == 'val':\n","                df_new_row = pd.DataFrame({'epoch': [epoch], 'val_loss': [epoch_loss], 'val_acc': [epoch_acc.cpu()]})\n","                df_val = pd.concat([df_val, df_new_row])\n","                df_val.to_csv('val_metrics.csv')\n","\n","            print(\"{} Loss: {:.4f} Acc: {:.4f}\".format(phase, epoch_loss, epoch_acc))\n","\n","            # Save torch model for checkpoints\n","            if epoch % 9 == 0:\n","                torch.save({\n","                    'epoch': epoch,\n","                    'model_state_dict': model.state_dict(),\n","                    'optimizer_state_dict': optimizer.state_dict(),\n","                    'loss': epoch_loss,\n","                }, f\"/kaggle/working/sav_model{epoch}.pt\")\n","\n","            if phase == 'val' and epoch_acc > best_acc:\n","                best_acc = epoch_acc\n","                best_model_wts = copy.deepcopy(model.state_dict())  # keep the best validation accuracy model\n","        print()\n","\n","    time_elapsed = time.time() - since\n","    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n","    print(\"Best Val Acc: {:.4f}\".format(best_acc))\n","\n","    model.load_state_dict(best_model_wts)\n","    return model, df_train, df_val\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-08T03:28:26.205213Z","iopub.status.busy":"2024-03-08T03:28:26.204484Z"},"trusted":true},"outputs":[],"source":["import sys\n","from tqdm import tqdm\n","import time\n","import copy\n","\n","model, df_train, df_val = train_model(model, criterion, optimizer,exp_lr_scheduler, num_epochs=100, load_checkpoint=True, checkpoint_path='/kaggle/input/sav-model72/sav_model72.pt')\n","# Save the best model weights at the end of training\n","torch.save(model.state_dict(), '/kaggle/working/best_model_weights.pth')\n"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2024-03-07T18:26:59.268238Z","iopub.status.busy":"2024-03-07T18:26:59.267399Z","iopub.status.idle":"2024-03-07T18:26:59.274951Z","shell.execute_reply":"2024-03-07T18:26:59.273884Z","shell.execute_reply.started":"2024-03-07T18:26:59.268202Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["/kaggle/working\n"]}],"source":["%cd /kaggle/working/"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2024-03-07T18:27:15.038759Z","iopub.status.busy":"2024-03-07T18:27:15.038372Z","iopub.status.idle":"2024-03-07T18:27:15.044960Z","shell.execute_reply":"2024-03-07T18:27:15.044122Z","shell.execute_reply.started":"2024-03-07T18:27:15.038730Z"},"trusted":true},"outputs":[{"data":{"text/html":["<a href='sav_model72.pt' target='_blank'>sav_model72.pt</a><br>"],"text/plain":["/kaggle/working/sav_model72.pt"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["from IPython.display import FileLink\n","FileLink(r'sav_model72.pt')"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":4548795,"sourceId":7774637,"sourceType":"datasetVersion"},{"datasetId":4559186,"sourceId":7789198,"sourceType":"datasetVersion"}],"dockerImageVersionId":30665,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
