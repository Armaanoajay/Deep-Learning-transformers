{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-03-07T06:15:44.367690Z","iopub.status.busy":"2024-03-07T06:15:44.367281Z","iopub.status.idle":"2024-03-07T06:15:52.323081Z","shell.execute_reply":"2024-03-07T06:15:52.322276Z","shell.execute_reply.started":"2024-03-07T06:15:44.367660Z"},"trusted":true},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","import pandas as pd\n","import zipfile\n","import os\n","import cv2\n","import seaborn as sns\n","from collections import Counter\n","# from google.colab.patches import cv2_imshow\n","from pathlib import Path\n","import random\n","\n","from sklearn.preprocessing import MinMaxScaler\n","from sklearn.decomposition import PCA\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn.model_selection import train_test_split\n","from sklearn.neighbors import NearestNeighbors\n","from imblearn.over_sampling import RandomOverSampler\n","from imblearn.over_sampling import ADASYN\n","import PIL\n","import torch\n","from torch import nn\n","from torch.utils.data import DataLoader\n","from torchvision import datasets, transforms\n","from torchvision.transforms import ToTensor\n","import torchvision\n","import torch.optim as optim\n","from functools import partial\n"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-03-07T06:15:52.325501Z","iopub.status.busy":"2024-03-07T06:15:52.324932Z","iopub.status.idle":"2024-03-07T06:15:52.330466Z","shell.execute_reply":"2024-03-07T06:15:52.329393Z","shell.execute_reply.started":"2024-03-07T06:15:52.325468Z"},"trusted":true},"outputs":[],"source":["def get_classes(data_dir):\n","    all_data = datasets.ImageFolder(data_dir)\n","    return all_data.classes"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-03-07T06:16:04.670350Z","iopub.status.busy":"2024-03-07T06:16:04.669672Z","iopub.status.idle":"2024-03-07T06:16:11.649990Z","shell.execute_reply":"2024-03-07T06:16:11.649068Z","shell.execute_reply.started":"2024-03-07T06:16:04.670322Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["['both', 'infection', 'ischaemia', 'none']\n"]}],"source":["test_transform = transforms.Compose([\n","                                transforms.Resize((224, 224)),\n","                                transforms.ToTensor()\n","                                    ])\n","\n","import torchvision.transforms as transforms\n","\n","transform = transforms.Compose([\n","    transforms.Resize((224, 224)),\n","    transforms.ToTensor(),\n","  \n","])\n","\n","\n","\n","\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","kwargs = {'num_workers': 1, 'pin_memory': True} if device=='cuda' else {}\n","\n","train_dataset_path = '/kaggle/input/dfuc-2021-split/A NEW DATASET SPLIT/train'\n","valid_dataset_path = '/kaggle/input/dfuc-2021-split/A NEW DATASET SPLIT/valid'\n","#test_dataset_path = '/kaggle/input/for-trial/new_tts_aug/test'\n","\n","train_dataset = datasets.ImageFolder(train_dataset_path, transform=transform)\n","valid_dataset = datasets.ImageFolder(valid_dataset_path, transform=transform)\n","#test_dataset = datasets.ImageFolder(test_dataset_path,transform=test_transform)\n","\n","train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=32,shuffle=True,**kwargs)\n","valid_dataloader = torch.utils.data.DataLoader(valid_dataset, batch_size=32,shuffle=True,**kwargs)\n","#test_dataloader =  torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=True, **kwargs)\n","\n","\n","CLASSES = train_dataset.classes\n","train_len = len(train_dataset)\n","valid_len = len(valid_dataset)\n","#test_len = len(test_dataset)\n","print(CLASSES)"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-03-07T06:17:47.336504Z","iopub.status.busy":"2024-03-07T06:17:47.336143Z","iopub.status.idle":"2024-03-07T06:17:47.934235Z","shell.execute_reply":"2024-03-07T06:17:47.933280Z","shell.execute_reply.started":"2024-03-07T06:17:47.336471Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/tmp/ipykernel_34/2822580298.py:61: UserWarning: Overwriting deit_tiny_patch16_224 in registry with __main__.deit_tiny_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n","  def deit_tiny_patch16_224(pretrained=False, **kwargs):\n","/tmp/ipykernel_34/2822580298.py:76: UserWarning: Overwriting deit_small_patch16_224 in registry with __main__.deit_small_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n","  def deit_small_patch16_224(pretrained=False, **kwargs):\n","/tmp/ipykernel_34/2822580298.py:91: UserWarning: Overwriting deit_base_patch16_224 in registry with __main__.deit_base_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n","  def deit_base_patch16_224(pretrained=False, **kwargs):\n","/tmp/ipykernel_34/2822580298.py:106: UserWarning: Overwriting deit_tiny_distilled_patch16_224 in registry with __main__.deit_tiny_distilled_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n","  def deit_tiny_distilled_patch16_224(pretrained=False, **kwargs):\n","/tmp/ipykernel_34/2822580298.py:121: UserWarning: Overwriting deit_small_distilled_patch16_224 in registry with __main__.deit_small_distilled_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n","  def deit_small_distilled_patch16_224(pretrained=False, **kwargs):\n","/tmp/ipykernel_34/2822580298.py:136: UserWarning: Overwriting deit_base_distilled_patch16_224 in registry with __main__.deit_base_distilled_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n","  def deit_base_distilled_patch16_224(pretrained=False, **kwargs):\n","/tmp/ipykernel_34/2822580298.py:151: UserWarning: Overwriting deit_base_patch16_384 in registry with __main__.deit_base_patch16_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n","  def deit_base_patch16_384(pretrained=False, **kwargs):\n","/tmp/ipykernel_34/2822580298.py:166: UserWarning: Overwriting deit_base_distilled_patch16_384 in registry with __main__.deit_base_distilled_patch16_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n","  def deit_base_distilled_patch16_384(pretrained=False, **kwargs):\n"]},{"data":{"text/plain":["==========================================================================================\n","Layer (type:depth-idx)                   Output Shape              Param #\n","==========================================================================================\n","DistilledVisionTransformer               [32, 4]                   76,800\n","├─PatchEmbed: 1-1                        [32, 196, 384]            --\n","│    └─Conv2d: 2-1                       [32, 384, 14, 14]         295,296\n","│    └─Identity: 2-2                     [32, 196, 384]            --\n","├─Dropout: 1-2                           [32, 198, 384]            --\n","├─Sequential: 1-3                        --                        --\n","│    └─Block: 2-3                        [32, 198, 384]            --\n","│    │    └─LayerNorm: 3-1               [32, 198, 384]            768\n","│    │    └─Attention: 3-2               [32, 198, 384]            591,360\n","│    │    └─Identity: 3-3                [32, 198, 384]            --\n","│    │    └─Identity: 3-4                [32, 198, 384]            --\n","│    │    └─LayerNorm: 3-5               [32, 198, 384]            768\n","│    │    └─Mlp: 3-6                     [32, 198, 384]            1,181,568\n","│    │    └─Identity: 3-7                [32, 198, 384]            --\n","│    │    └─Identity: 3-8                [32, 198, 384]            --\n","│    └─Block: 2-4                        [32, 198, 384]            --\n","│    │    └─LayerNorm: 3-9               [32, 198, 384]            768\n","│    │    └─Attention: 3-10              [32, 198, 384]            591,360\n","│    │    └─Identity: 3-11               [32, 198, 384]            --\n","│    │    └─Identity: 3-12               [32, 198, 384]            --\n","│    │    └─LayerNorm: 3-13              [32, 198, 384]            768\n","│    │    └─Mlp: 3-14                    [32, 198, 384]            1,181,568\n","│    │    └─Identity: 3-15               [32, 198, 384]            --\n","│    │    └─Identity: 3-16               [32, 198, 384]            --\n","│    └─Block: 2-5                        [32, 198, 384]            --\n","│    │    └─LayerNorm: 3-17              [32, 198, 384]            768\n","│    │    └─Attention: 3-18              [32, 198, 384]            591,360\n","│    │    └─Identity: 3-19               [32, 198, 384]            --\n","│    │    └─Identity: 3-20               [32, 198, 384]            --\n","│    │    └─LayerNorm: 3-21              [32, 198, 384]            768\n","│    │    └─Mlp: 3-22                    [32, 198, 384]            1,181,568\n","│    │    └─Identity: 3-23               [32, 198, 384]            --\n","│    │    └─Identity: 3-24               [32, 198, 384]            --\n","│    └─Block: 2-6                        [32, 198, 384]            --\n","│    │    └─LayerNorm: 3-25              [32, 198, 384]            768\n","│    │    └─Attention: 3-26              [32, 198, 384]            591,360\n","│    │    └─Identity: 3-27               [32, 198, 384]            --\n","│    │    └─Identity: 3-28               [32, 198, 384]            --\n","│    │    └─LayerNorm: 3-29              [32, 198, 384]            768\n","│    │    └─Mlp: 3-30                    [32, 198, 384]            1,181,568\n","│    │    └─Identity: 3-31               [32, 198, 384]            --\n","│    │    └─Identity: 3-32               [32, 198, 384]            --\n","│    └─Block: 2-7                        [32, 198, 384]            --\n","│    │    └─LayerNorm: 3-33              [32, 198, 384]            768\n","│    │    └─Attention: 3-34              [32, 198, 384]            591,360\n","│    │    └─Identity: 3-35               [32, 198, 384]            --\n","│    │    └─Identity: 3-36               [32, 198, 384]            --\n","│    │    └─LayerNorm: 3-37              [32, 198, 384]            768\n","│    │    └─Mlp: 3-38                    [32, 198, 384]            1,181,568\n","│    │    └─Identity: 3-39               [32, 198, 384]            --\n","│    │    └─Identity: 3-40               [32, 198, 384]            --\n","│    └─Block: 2-8                        [32, 198, 384]            --\n","│    │    └─LayerNorm: 3-41              [32, 198, 384]            768\n","│    │    └─Attention: 3-42              [32, 198, 384]            591,360\n","│    │    └─Identity: 3-43               [32, 198, 384]            --\n","│    │    └─Identity: 3-44               [32, 198, 384]            --\n","│    │    └─LayerNorm: 3-45              [32, 198, 384]            768\n","│    │    └─Mlp: 3-46                    [32, 198, 384]            1,181,568\n","│    │    └─Identity: 3-47               [32, 198, 384]            --\n","│    │    └─Identity: 3-48               [32, 198, 384]            --\n","│    └─Block: 2-9                        [32, 198, 384]            --\n","│    │    └─LayerNorm: 3-49              [32, 198, 384]            768\n","│    │    └─Attention: 3-50              [32, 198, 384]            591,360\n","│    │    └─Identity: 3-51               [32, 198, 384]            --\n","│    │    └─Identity: 3-52               [32, 198, 384]            --\n","│    │    └─LayerNorm: 3-53              [32, 198, 384]            768\n","│    │    └─Mlp: 3-54                    [32, 198, 384]            1,181,568\n","│    │    └─Identity: 3-55               [32, 198, 384]            --\n","│    │    └─Identity: 3-56               [32, 198, 384]            --\n","│    └─Block: 2-10                       [32, 198, 384]            --\n","│    │    └─LayerNorm: 3-57              [32, 198, 384]            768\n","│    │    └─Attention: 3-58              [32, 198, 384]            591,360\n","│    │    └─Identity: 3-59               [32, 198, 384]            --\n","│    │    └─Identity: 3-60               [32, 198, 384]            --\n","│    │    └─LayerNorm: 3-61              [32, 198, 384]            768\n","│    │    └─Mlp: 3-62                    [32, 198, 384]            1,181,568\n","│    │    └─Identity: 3-63               [32, 198, 384]            --\n","│    │    └─Identity: 3-64               [32, 198, 384]            --\n","│    └─Block: 2-11                       [32, 198, 384]            --\n","│    │    └─LayerNorm: 3-65              [32, 198, 384]            768\n","│    │    └─Attention: 3-66              [32, 198, 384]            591,360\n","│    │    └─Identity: 3-67               [32, 198, 384]            --\n","│    │    └─Identity: 3-68               [32, 198, 384]            --\n","│    │    └─LayerNorm: 3-69              [32, 198, 384]            768\n","│    │    └─Mlp: 3-70                    [32, 198, 384]            1,181,568\n","│    │    └─Identity: 3-71               [32, 198, 384]            --\n","│    │    └─Identity: 3-72               [32, 198, 384]            --\n","│    └─Block: 2-12                       [32, 198, 384]            --\n","│    │    └─LayerNorm: 3-73              [32, 198, 384]            768\n","│    │    └─Attention: 3-74              [32, 198, 384]            591,360\n","│    │    └─Identity: 3-75               [32, 198, 384]            --\n","│    │    └─Identity: 3-76               [32, 198, 384]            --\n","│    │    └─LayerNorm: 3-77              [32, 198, 384]            768\n","│    │    └─Mlp: 3-78                    [32, 198, 384]            1,181,568\n","│    │    └─Identity: 3-79               [32, 198, 384]            --\n","│    │    └─Identity: 3-80               [32, 198, 384]            --\n","│    └─Block: 2-13                       [32, 198, 384]            --\n","│    │    └─LayerNorm: 3-81              [32, 198, 384]            768\n","│    │    └─Attention: 3-82              [32, 198, 384]            591,360\n","│    │    └─Identity: 3-83               [32, 198, 384]            --\n","│    │    └─Identity: 3-84               [32, 198, 384]            --\n","│    │    └─LayerNorm: 3-85              [32, 198, 384]            768\n","│    │    └─Mlp: 3-86                    [32, 198, 384]            1,181,568\n","│    │    └─Identity: 3-87               [32, 198, 384]            --\n","│    │    └─Identity: 3-88               [32, 198, 384]            --\n","│    └─Block: 2-14                       [32, 198, 384]            --\n","│    │    └─LayerNorm: 3-89              [32, 198, 384]            768\n","│    │    └─Attention: 3-90              [32, 198, 384]            591,360\n","│    │    └─Identity: 3-91               [32, 198, 384]            --\n","│    │    └─Identity: 3-92               [32, 198, 384]            --\n","│    │    └─LayerNorm: 3-93              [32, 198, 384]            768\n","│    │    └─Mlp: 3-94                    [32, 198, 384]            1,181,568\n","│    │    └─Identity: 3-95               [32, 198, 384]            --\n","│    │    └─Identity: 3-96               [32, 198, 384]            --\n","├─LayerNorm: 1-4                         [32, 198, 384]            768\n","├─Linear: 1-5                            [32, 4]                   1,540\n","├─Linear: 1-6                            [32, 4]                   1,540\n","==========================================================================================\n","Total params: 21,669,512\n","Trainable params: 21,669,512\n","Non-trainable params: 0\n","Total mult-adds (G): 2.53\n","==========================================================================================\n","Input size (MB): 19.27\n","Forward/backward pass size (MB): 2608.01\n","Params size (MB): 86.37\n","Estimated Total Size (MB): 2713.65\n","=========================================================================================="]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["# Copyright (c) 2015-present, Facebook, Inc.\n","# All rights reserved.\n","import torch\n","import torch.nn as nn\n","from functools import partial\n","\n","from timm.models.vision_transformer import VisionTransformer, _cfg\n","from timm.models.registry import register_model\n","from timm.models.layers import trunc_normal_\n","\n","\n","__all__ = [\n","    'deit_tiny_patch16_224', 'deit_small_patch16_224', 'deit_base_patch16_224',\n","    'deit_tiny_distilled_patch16_224', 'deit_small_distilled_patch16_224',\n","    'deit_base_distilled_patch16_224', 'deit_base_patch16_384',\n","    'deit_base_distilled_patch16_384',\n","]\n","\n","\n","class DistilledVisionTransformer(VisionTransformer):\n","    def __init__(self, *args, **kwargs):\n","        super().__init__(*args, **kwargs)\n","        self.dist_token = nn.Parameter(torch.zeros(1, 1, self.embed_dim))\n","        num_patches = self.patch_embed.num_patches\n","        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + 2, self.embed_dim))\n","        self.head_dist = nn.Linear(self.embed_dim, self.num_classes) if self.num_classes > 0 else nn.Identity()\n","\n","        trunc_normal_(self.dist_token, std=.02)\n","        trunc_normal_(self.pos_embed, std=.02)\n","        self.head_dist.apply(self._init_weights)\n","\n","    def forward_features(self, x):\n","        # taken from https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/vision_transformer.py\n","        # with slight modifications to add the dist_token\n","        B = x.shape[0]\n","        x = self.patch_embed(x)\n","\n","        cls_tokens = self.cls_token.expand(B, -1, -1)  # stole cls_tokens impl from Phil Wang, thanks\n","        dist_token = self.dist_token.expand(B, -1, -1)\n","        x = torch.cat((cls_tokens, dist_token, x), dim=1)\n","\n","        x = x + self.pos_embed\n","        x = self.pos_drop(x)\n","\n","        for blk in self.blocks:\n","            x = blk(x)\n","\n","        x = self.norm(x)\n","        return x[:, 0], x[:, 1]\n","\n","    def forward(self, x):\n","        x, x_dist = self.forward_features(x)\n","        # Remove classification head\n","        x = self.head(x)\n","        x_dist = self.head_dist(x_dist)\n","        return x\n","        \n","\n","\n","@register_model\n","def deit_tiny_patch16_224(pretrained=False, **kwargs):\n","    model = VisionTransformer(\n","        patch_size=16, embed_dim=192, depth=12, num_heads=3, mlp_ratio=4, qkv_bias=True,\n","        norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n","    model.default_cfg = _cfg()\n","    if pretrained:\n","        checkpoint = torch.hub.load_state_dict_from_url(\n","            url=\"https://dl.fbaipublicfiles.com/deit/deit_tiny_patch16_224-a1311bcf.pth\",\n","            map_location=\"cpu\", check_hash=True\n","        )\n","        model.load_state_dict(checkpoint[\"model\"])\n","    return model\n","\n","\n","@register_model\n","def deit_small_patch16_224(pretrained=False, **kwargs):\n","    model = VisionTransformer(\n","        patch_size=16, embed_dim=384, depth=12, num_heads=6, mlp_ratio=4, qkv_bias=True,\n","        norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n","    model.default_cfg = _cfg()\n","    if pretrained:\n","        checkpoint = torch.hub.load_state_dict_from_url(\n","            url=\"https://dl.fbaipublicfiles.com/deit/deit_small_patch16_224-cd65a155.pth\",\n","            map_location=\"cpu\", check_hash=True\n","        )\n","        model.load_state_dict(checkpoint[\"model\"])\n","    return model\n","\n","\n","@register_model\n","def deit_base_patch16_224(pretrained=False, **kwargs):\n","    model = VisionTransformer(\n","        patch_size=16, embed_dim=768, depth=12, num_heads=12, mlp_ratio=4, qkv_bias=True,\n","        norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n","    model.default_cfg = _cfg()\n","    if pretrained:\n","        checkpoint = torch.hub.load_state_dict_from_url(\n","            url=\"https://dl.fbaipublicfiles.com/deit/deit_base_patch16_224-b5f2ef4d.pth\",\n","            map_location=\"cpu\", check_hash=True\n","        )\n","        model.load_state_dict(checkpoint[\"model\"])\n","    return model\n","\n","\n","@register_model\n","def deit_tiny_distilled_patch16_224(pretrained=False, **kwargs):\n","    model = DistilledVisionTransformer(\n","        patch_size=16, embed_dim=192, depth=12, num_heads=3, mlp_ratio=4, qkv_bias=True,\n","        norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n","    model.default_cfg = _cfg()\n","    if pretrained:\n","        checkpoint = torch.hub.load_state_dict_from_url(\n","            url=\"https://dl.fbaipublicfiles.com/deit/deit_tiny_distilled_patch16_224-b40b3cf7.pth\",\n","            map_location=\"cpu\", check_hash=True\n","        )\n","        model.load_state_dict(checkpoint[\"model\"])\n","    return model\n","\n","\n","@register_model\n","def deit_small_distilled_patch16_224(pretrained=False, **kwargs):\n","    model = DistilledVisionTransformer(\n","        patch_size=16, embed_dim=384, depth=12, num_heads=6, mlp_ratio=4, qkv_bias=True,\n","        norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n","    model.default_cfg = _cfg()\n","    if pretrained:\n","        checkpoint = torch.hub.load_state_dict_from_url(\n","            url=\"https://dl.fbaipublicfiles.com/deit/deit_small_distilled_patch16_224-649709d9.pth\",\n","            map_location=\"cpu\", check_hash=True\n","        )\n","        model.load_state_dict(checkpoint[\"model\"])\n","    return model\n","\n","\n","@register_model\n","def deit_base_distilled_patch16_224(pretrained=False, **kwargs):\n","    model = DistilledVisionTransformer(\n","        patch_size=16, embed_dim=768, depth=12, num_heads=12, mlp_ratio=4, qkv_bias=True,\n","        norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n","    model.default_cfg = _cfg()\n","    if pretrained:\n","        checkpoint = torch.hub.load_state_dict_from_url(\n","            url=\"https://dl.fbaipublicfiles.com/deit/deit_base_distilled_patch16_224-df68dfff.pth\",\n","            map_location=\"cpu\", check_hash=True\n","        )\n","        model.load_state_dict(checkpoint[\"model\"])\n","    return model\n","\n","\n","@register_model\n","def deit_base_patch16_384(pretrained=False, **kwargs):\n","    model = VisionTransformer(\n","        img_size=384, patch_size=16, embed_dim=768, depth=12, num_heads=12, mlp_ratio=4, qkv_bias=True,\n","        norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n","    model.default_cfg = _cfg()\n","    if pretrained:\n","        checkpoint = torch.hub.load_state_dict_from_url(\n","            url=\"https://dl.fbaipublicfiles.com/deit/deit_base_patch16_384-8de9b5d1.pth\",\n","            map_location=\"cpu\", check_hash=True\n","        )\n","        model.load_state_dict(checkpoint[\"model\"])\n","    return model\n","\n","\n","@register_model\n","def deit_base_distilled_patch16_384(pretrained=False, **kwargs):\n","    model = DistilledVisionTransformer(\n","        img_size=384, patch_size=16, embed_dim=768, depth=12, num_heads=12, mlp_ratio=4, qkv_bias=True,\n","        norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n","    model.default_cfg = _cfg()\n","    if pretrained:\n","        checkpoint = torch.hub.load_state_dict_from_url(\n","            url=\"https://dl.fbaipublicfiles.com/deit/deit_base_distilled_patch16_384-d0272ac0.pth\",\n","            map_location=\"cpu\", check_hash=True\n","        )\n","        model.load_state_dict(checkpoint[\"model\"])\n","    return model\n","\n","\n","import torchinfo\n","model=deit_small_distilled_patch16_224(num_classes=4)\n","torchinfo.summary(model,(32,3,224,224))"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-03-07T06:17:55.083295Z","iopub.status.busy":"2024-03-07T06:17:55.082655Z","iopub.status.idle":"2024-03-07T06:17:55.088945Z","shell.execute_reply":"2024-03-07T06:17:55.088039Z","shell.execute_reply.started":"2024-03-07T06:17:55.083263Z"},"trusted":true},"outputs":[{"data":{"text/plain":["4"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["len(CLASSES)"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-03-07T06:17:57.154133Z","iopub.status.busy":"2024-03-07T06:17:57.153780Z","iopub.status.idle":"2024-03-07T06:17:57.159981Z","shell.execute_reply":"2024-03-07T06:17:57.159008Z","shell.execute_reply.started":"2024-03-07T06:17:57.154107Z"},"trusted":true},"outputs":[],"source":["\n","dataloaders = {\n","    \"train\": train_dataloader,\n","    \"val\": valid_dataloader,\n","    #\"test\": test_dataloader\n","}\n","\n","dataset_sizes = {\n","    \"train\": train_len,\n","    \"val\": valid_len,\n","    #\"test\": test_len\n","}\n"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-03-07T06:18:00.238195Z","iopub.status.busy":"2024-03-07T06:18:00.237457Z","iopub.status.idle":"2024-03-07T06:18:00.248015Z","shell.execute_reply":"2024-03-07T06:18:00.247106Z","shell.execute_reply.started":"2024-03-07T06:18:00.238166Z"},"trusted":true},"outputs":[],"source":["model.to(device)\n","\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(model.parameters(),lr=0.001,weight_decay=0.00001)\n","criterion = criterion.to(device)\n","exp_lr_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=15, gamma=0.97)"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-03-07T06:18:04.548847Z","iopub.status.busy":"2024-03-07T06:18:04.548453Z","iopub.status.idle":"2024-03-07T06:18:04.569591Z","shell.execute_reply":"2024-03-07T06:18:04.568637Z","shell.execute_reply.started":"2024-03-07T06:18:04.548820Z"},"trusted":true},"outputs":[],"source":["import time\n","import copy\n","import torch\n","import pandas as pd\n","import os\n","from tqdm import tqdm\n","\n","def train_model(model, criterion, optimizer, exp_lr_scheduler, num_epochs=75, load_checkpoint=False, checkpoint_path=None):\n","    since = time.time()\n","    best_model_wts = copy.deepcopy(model.state_dict())\n","    best_acc = 0.0\n","    df_train = pd.DataFrame(columns=['epoch', 'train_loss', 'train_acc'])\n","    df_val = pd.DataFrame(columns=['epoch', 'val_loss', 'val_acc'])\n","    if load_checkpoint:\n","        if checkpoint_path is None:\n","            raise ValueError(\"Checkpoint path is not specified.\")\n","\n","        checkpoint = torch.load(checkpoint_path)\n","        model.load_state_dict(checkpoint['model_state_dict'])\n","        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n","        original_learning_rate = optimizer.param_groups[0]['lr']\n","        print(f\"Original Learning Rate: {original_learning_rate}\")\n","        loss = checkpoint['loss']\n","\n","        start_epoch = 90\n","    else:\n","        start_epoch = 0\n","\n","    for epoch in range(start_epoch, num_epochs):\n","        print(f'Epoch {epoch}/{num_epochs - 1}')\n","        print(\"-\" * 10)\n","\n","        if epoch == 0:\n","            if not os.path.isdir(\"/kaggle/working/\"):\n","                os.mkdir(\"/kaggle/working/\")\n","\n","        for phase in ['train', 'val']:  # We do training and validation phase per epoch\n","            if phase == 'train':\n","                model.train()  # model to training mode\n","            else:\n","                model.eval()  # model to evaluate\n","\n","            running_loss = 0.0\n","            running_corrects = 0.0\n","            total_samples = 0\n","\n","            progress_bar = tqdm(dataloaders[phase], desc=f'{phase.capitalize()} Epoch {epoch}/{num_epochs - 1}', leave=False)\n","\n","            for inputs, labels in progress_bar:\n","                inputs = inputs.to(device)\n","                labels = labels.to(device)\n","\n","                optimizer.zero_grad()\n","\n","                with torch.set_grad_enabled(phase == 'train'):  # no autograd makes validation go faster\n","                    outputs = model(inputs)\n","                    _, preds = torch.max(outputs, 1)  # used for accuracy\n","                    loss = criterion(outputs, labels)\n","\n","                    if phase == 'train':\n","                        loss.backward()\n","                        optimizer.step()\n","\n","                running_loss += loss.item() * inputs.size(0)\n","                running_corrects += torch.sum(preds == labels.data)\n","                total_samples += labels.size(0)\n","\n","                # Update progress bar description with live accuracy\n","                accuracy = running_corrects.double() / total_samples\n","                progress_bar.set_postfix(loss=running_loss / total_samples, accuracy=accuracy)\n","\n","            if phase == 'train':\n","                exp_lr_scheduler.step()  # step at the end of the epoch\n","\n","            epoch_loss = running_loss / dataset_sizes[phase]\n","            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n","\n","            if phase == 'train':\n","                df_new_row = pd.DataFrame({'epoch': [epoch], 'train_loss': [epoch_loss], 'train_acc': [epoch_acc.cpu()]})\n","                df_train = pd.concat([df_train, df_new_row])\n","                df_train.to_csv('train_metrics.csv')\n","            elif phase == 'val':\n","                df_new_row = pd.DataFrame({'epoch': [epoch], 'val_loss': [epoch_loss], 'val_acc': [epoch_acc.cpu()]})\n","                df_val = pd.concat([df_val, df_new_row])\n","                df_val.to_csv('val_metrics.csv')\n","\n","            print(\"{} Loss: {:.4f} Acc: {:.4f}\".format(phase, epoch_loss, epoch_acc))\n","\n","            # Save torch model for checkpoints\n","            if epoch % 9 == 0:\n","                torch.save({\n","                    'epoch': epoch,\n","                    'model_state_dict': model.state_dict(),\n","                    'optimizer_state_dict': optimizer.state_dict(),\n","                    'loss': epoch_loss,\n","                }, f\"/kaggle/working/sav_model{epoch}.pt\")\n","\n","            if phase == 'val' and epoch_acc > best_acc:\n","                best_acc = epoch_acc\n","                best_model_wts = copy.deepcopy(model.state_dict())  # keep the best validation accuracy model\n","        print()\n","\n","    time_elapsed = time.time() - since\n","    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n","    print(\"Best Val Acc: {:.4f}\".format(best_acc))\n","\n","    model.load_state_dict(best_model_wts)\n","    return model, df_train, df_val\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-07T06:18:07.510825Z","iopub.status.busy":"2024-03-07T06:18:07.510123Z","iopub.status.idle":"2024-03-07T08:49:50.910280Z","shell.execute_reply":"2024-03-07T08:49:50.909094Z","shell.execute_reply.started":"2024-03-07T06:18:07.510792Z"},"trusted":true},"outputs":[],"source":["model, df_train, df_val = train_model(model, criterion, optimizer, exp_lr_scheduler, num_epochs=100, load_checkpoint=False, checkpoint_path=None)\n","# Save the best model weights at the end of training\n","torch.save(model.state_dict(), '/kaggle/working/best_model_weights.pth')\n","\n"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2024-03-07T08:52:37.933322Z","iopub.status.busy":"2024-03-07T08:52:37.932457Z","iopub.status.idle":"2024-03-07T08:52:38.461271Z","shell.execute_reply":"2024-03-07T08:52:38.460394Z","shell.execute_reply.started":"2024-03-07T08:52:37.933287Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["['both', 'infection', 'ischaemia', 'none']\n"]}],"source":["import torchvision.transforms as transforms\n","mean = [0.485, 0.456, 0.406]\n","std = [0.229, 0.224, 0.225]\n","test_transform = transforms.Compose([\n","                                transforms.Resize((224, 224)),\n","                                transforms.ToTensor(),\n","                  \n","                                    ])\n","\n","\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","kwargs = {'num_workers': 1, 'pin_memory': True} if device=='cuda' else {}\n","\n","\n","test_dataset_path = '/kaggle/input/dfuc-2021-split/A NEW DATASET SPLIT/test'\n","\n","\n","# Create a test dataset\n","test_dataset = datasets.ImageFolder(test_dataset_path, transform=test_transform)\n","\n","# Create a test dataloader\n","test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=False)  # Set shuffle to False\n","CLASSES = test_dataset.classes\n","\n","test_len = len(test_dataset)\n","print(CLASSES)"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2024-03-07T08:52:41.365200Z","iopub.status.busy":"2024-03-07T08:52:41.364829Z","iopub.status.idle":"2024-03-07T08:52:41.371406Z","shell.execute_reply":"2024-03-07T08:52:41.370573Z","shell.execute_reply.started":"2024-03-07T08:52:41.365160Z"},"trusted":true},"outputs":[{"data":{"text/plain":["4"]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":["len(CLASSES)"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2024-03-07T08:52:42.685818Z","iopub.status.busy":"2024-03-07T08:52:42.684830Z","iopub.status.idle":"2024-03-07T08:52:42.690475Z","shell.execute_reply":"2024-03-07T08:52:42.689606Z","shell.execute_reply.started":"2024-03-07T08:52:42.685777Z"},"trusted":true},"outputs":[],"source":["dataloaders = {\n","    \"test\": test_dataloader\n","}\n","\n","dataset_sizes = {\n","    \"test\": test_len\n","}\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-07T08:53:23.219250Z","iopub.status.busy":"2024-03-07T08:53:23.218890Z","iopub.status.idle":"2024-03-07T08:53:34.548499Z","shell.execute_reply":"2024-03-07T08:53:34.547592Z","shell.execute_reply.started":"2024-03-07T08:53:23.219223Z"},"trusted":true},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import numpy as np\n","from sklearn.metrics import classification_report, accuracy_score\n","\n","# Assuming you have 'test_dataloader', 'test_dataset', 'device', 'model', and 'criterion' defined\n","\n","checkpoint = torch.load('/kaggle/working/best_model_weights.pth')\n","model.load_state_dict(checkpoint)\n","\n","model.eval()\n","criterion = nn.CrossEntropyLoss()\n","\n","model.to(device)\n","\n","running_corrects = 0\n","test_loss = 0\n","class_correct = [0] * len(test_dataset.classes)\n","class_total = [0] * len(test_dataset.classes)\n","\n","# Lists to store true and predicted labels\n","all_true_labels = []\n","all_pred_labels = []\n","\n","with torch.no_grad():\n","    for inputs, labels in test_dataloader:\n","        inputs = inputs.to(device)\n","        labels = labels.to(device)\n","\n","        outputs = model(inputs)\n","        _, preds = torch.max(outputs, 1)\n","\n","        running_corrects += torch.sum(preds == labels.data)\n","        loss = criterion(outputs, labels)\n","        test_loss += loss.item() * inputs.size(0)\n","\n","        for i in range(len(labels)):\n","            label = labels[i].item()\n","            class_correct[label] += int(preds[i] == label)\n","            class_total[label] += 1\n","\n","            # Collect true and predicted labels\n","            all_true_labels.append(label)\n","            all_pred_labels.append(preds[i].item())\n","\n","# Calculate metrics\n","test_acc = running_corrects.double() / len(test_dataset)\n","test_loss = test_loss / len(test_dataset)\n","\n","overall_accuracy = accuracy_score(all_true_labels, all_pred_labels)\n","classification_report_str = classification_report(all_true_labels, all_pred_labels, target_names=test_dataset.classes)\n","\n","print(f\"Test Accuracy: {test_acc:.4f}\")\n","print(f\"Test Loss: {test_loss:.4f}\")\n","print(f\"Overall Accuracy: {overall_accuracy:.4f}\")\n","print(\"Classification Report:\")\n","print(classification_report_str)\n","\n","# Calculate and print class-wise accuracy\n","for i, class_name in enumerate(test_dataset.classes):\n","    class_acc = class_correct[i] / class_total[i]\n","    print(f\"Class {class_name} Accuracy: {class_acc:.4f}\")\n"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2024-03-07T08:57:29.833674Z","iopub.status.busy":"2024-03-07T08:57:29.833000Z","iopub.status.idle":"2024-03-07T08:57:29.839930Z","shell.execute_reply":"2024-03-07T08:57:29.838953Z","shell.execute_reply.started":"2024-03-07T08:57:29.833643Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["/kaggle/working\n"]}],"source":["%cd /kaggle/working/"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2024-03-07T08:57:44.985348Z","iopub.status.busy":"2024-03-07T08:57:44.984558Z","iopub.status.idle":"2024-03-07T08:57:44.992061Z","shell.execute_reply":"2024-03-07T08:57:44.990969Z","shell.execute_reply.started":"2024-03-07T08:57:44.985316Z"},"trusted":true},"outputs":[{"data":{"text/html":["<a href='sav_model99.pt' target='_blank'>sav_model99.pt</a><br>"],"text/plain":["/kaggle/working/sav_model99.pt"]},"execution_count":17,"metadata":{},"output_type":"execute_result"}],"source":["from IPython.display import FileLink\n","FileLink(r'sav_model99.pt')"]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"datasetId":4548795,"sourceId":7774637,"sourceType":"datasetVersion"}],"dockerImageVersionId":30665,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
