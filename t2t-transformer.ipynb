{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-03-07T10:32:13.869929Z","iopub.status.busy":"2024-03-07T10:32:13.869569Z","iopub.status.idle":"2024-03-07T10:32:21.551280Z","shell.execute_reply":"2024-03-07T10:32:21.550406Z","shell.execute_reply.started":"2024-03-07T10:32:13.869901Z"},"trusted":true},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","import pandas as pd\n","import zipfile\n","import os\n","import cv2\n","import seaborn as sns\n","from collections import Counter\n","# from google.colab.patches import cv2_imshow\n","from pathlib import Path\n","import random\n","\n","from sklearn.preprocessing import MinMaxScaler\n","from sklearn.decomposition import PCA\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn.model_selection import train_test_split\n","from sklearn.neighbors import NearestNeighbors\n","from imblearn.over_sampling import RandomOverSampler\n","from imblearn.over_sampling import ADASYN\n","import PIL\n","import torch\n","from torch import nn\n","from torch.utils.data import DataLoader\n","from torchvision import datasets, transforms\n","from torchvision.transforms import ToTensor\n","import torchvision\n","import torch.optim as optim\n","from functools import partial\n"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-03-07T10:32:28.956728Z","iopub.status.busy":"2024-03-07T10:32:28.956203Z","iopub.status.idle":"2024-03-07T10:32:28.961372Z","shell.execute_reply":"2024-03-07T10:32:28.960490Z","shell.execute_reply.started":"2024-03-07T10:32:28.956699Z"},"trusted":true},"outputs":[],"source":["def get_classes(data_dir):\n","    all_data = datasets.ImageFolder(data_dir)\n","    return all_data.classes"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-03-07T10:32:31.121250Z","iopub.status.busy":"2024-03-07T10:32:31.120899Z","iopub.status.idle":"2024-03-07T10:32:42.466754Z","shell.execute_reply":"2024-03-07T10:32:42.465782Z","shell.execute_reply.started":"2024-03-07T10:32:31.121222Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["['both', 'infection', 'ischaemia', 'none']\n"]}],"source":["test_transform = transforms.Compose([\n","                                transforms.Resize((224, 224)),\n","                                transforms.ToTensor()\n","                                    ])\n","\n","import torchvision.transforms as transforms\n","\n","transform = transforms.Compose([\n","    transforms.Resize((224, 224)),\n","    transforms.ToTensor(),\n","  \n","])\n","\n","\n","\n","\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","kwargs = {'num_workers': 1, 'pin_memory': True} if device=='cuda' else {}\n","\n","train_dataset_path = '/kaggle/input/dfuc-2021-split/A NEW DATASET SPLIT/train'\n","valid_dataset_path = '/kaggle/input/dfuc-2021-split/A NEW DATASET SPLIT/valid'\n","#test_dataset_path = '/kaggle/input/for-trial/new_tts_aug/test'\n","\n","train_dataset = datasets.ImageFolder(train_dataset_path, transform=transform)\n","valid_dataset = datasets.ImageFolder(valid_dataset_path, transform=transform)\n","#test_dataset = datasets.ImageFolder(test_dataset_path,transform=test_transform)\n","\n","train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=32,shuffle=True,**kwargs)\n","valid_dataloader = torch.utils.data.DataLoader(valid_dataset, batch_size=32,shuffle=True,**kwargs)\n","#test_dataloader =  torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=True, **kwargs)\n","\n","\n","CLASSES = train_dataset.classes\n","train_len = len(train_dataset)\n","valid_len = len(valid_dataset)\n","#test_len = len(test_dataset)\n","print(CLASSES)"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-03-07T10:32:45.023605Z","iopub.status.busy":"2024-03-07T10:32:45.023225Z","iopub.status.idle":"2024-03-07T10:32:47.299614Z","shell.execute_reply":"2024-03-07T10:32:47.298616Z","shell.execute_reply.started":"2024-03-07T10:32:45.023573Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Cloning into 'T2T-ViT'...\n","remote: Enumerating objects: 213, done.\u001b[K\n","remote: Counting objects: 100% (85/85), done.\u001b[K\n","remote: Compressing objects: 100% (47/47), done.\u001b[K\n","remote: Total 213 (delta 72), reused 44 (delta 38), pack-reused 128\u001b[K\n","Receiving objects: 100% (213/213), 15.53 MiB | 25.89 MiB/s, done.\n","Resolving deltas: 100% (122/122), done.\n"]}],"source":["!git clone https://github.com/yitu-opensource/T2T-ViT"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-03-07T10:32:53.485205Z","iopub.status.busy":"2024-03-07T10:32:53.484875Z","iopub.status.idle":"2024-03-07T10:32:55.022612Z","shell.execute_reply":"2024-03-07T10:32:55.021822Z","shell.execute_reply.started":"2024-03-07T10:32:53.485179Z"},"trusted":true},"outputs":[],"source":["\"\"\"\n","Take Performer as T2T Transformer\n","\"\"\"\n","import math\n","import torch\n","import torch.nn as nn\n","\n","class Token_performer(nn.Module):\n","    def __init__(self, dim, in_dim, head_cnt=1, kernel_ratio=0.5, dp1=0.1, dp2 = 0.1):\n","        super().__init__()\n","        self.emb = in_dim * head_cnt # we use 1, so it is no need here\n","        self.kqv = nn.Linear(dim, 3 * self.emb)\n","        self.dp = nn.Dropout(dp1)\n","        self.proj = nn.Linear(self.emb, self.emb)\n","        self.head_cnt = head_cnt\n","        self.norm1 = nn.LayerNorm(dim)\n","        self.norm2 = nn.LayerNorm(self.emb)\n","        self.epsilon = 1e-8  # for stable in division\n","\n","        self.mlp = nn.Sequential(\n","            nn.Linear(self.emb, 1 * self.emb),\n","            nn.GELU(),\n","            nn.Linear(1 * self.emb, self.emb),\n","            nn.Dropout(dp2),\n","        )\n","\n","        self.m = int(self.emb * kernel_ratio)\n","        self.w = torch.randn(self.m, self.emb)\n","        self.w = nn.Parameter(nn.init.orthogonal_(self.w) * math.sqrt(self.m), requires_grad=False)\n","\n","    def prm_exp(self, x):\n","        # part of the function is borrow from https://github.com/lucidrains/performer-pytorch \n","        # and Simo Ryu (https://github.com/cloneofsimo)\n","        # ==== positive random features for gaussian kernels ====\n","        # x = (B, T, hs)\n","        # w = (m, hs)\n","        # return : x : B, T, m\n","        # SM(x, y) = E_w[exp(w^T x - |x|/2) exp(w^T y - |y|/2)]\n","        # therefore return exp(w^Tx - |x|/2)/sqrt(m)\n","        xd = ((x * x).sum(dim=-1, keepdim=True)).repeat(1, 1, self.m) / 2\n","        wtx = torch.einsum('bti,mi->btm', x.float(), self.w)\n","\n","        return torch.exp(wtx - xd) / math.sqrt(self.m)\n","\n","    def single_attn(self, x):\n","        k, q, v = torch.split(self.kqv(x), self.emb, dim=-1)\n","        kp, qp = self.prm_exp(k), self.prm_exp(q)  # (B, T, m), (B, T, m)\n","        D = torch.einsum('bti,bi->bt', qp, kp.sum(dim=1)).unsqueeze(dim=2)  # (B, T, m) * (B, m) -> (B, T, 1)\n","        kptv = torch.einsum('bin,bim->bnm', v.float(), kp)  # (B, emb, m)\n","        y = torch.einsum('bti,bni->btn', qp, kptv) / (D.repeat(1, 1, self.emb) + self.epsilon)  # (B, T, emb)/Diag\n","        # skip connection\n","        y = v + self.dp(self.proj(y))  # same as token_transformer in T2T layer, use v as skip connection\n","\n","        return y\n","\n","    def forward(self, x):\n","        x = self.single_attn(self.norm1(x))\n","        x = x + self.mlp(self.norm2(x))\n","        return x\n","    \n","# Copyright (c) [2012]-[2021] Shanghai Yitu Technology Co., Ltd.\n","#\n","# This source code is licensed under the Clear BSD License\n","# LICENSE file in the root directory of this file\n","# All rights reserved.\n","\"\"\"\n","Take the standard Transformer as T2T Transformer\n","\"\"\"\n","import torch.nn as nn\n","from timm.models.layers import DropPath\n","\n","\n","class Attention(nn.Module):\n","    def __init__(self, dim, num_heads=8, in_dim = None, qkv_bias=False, qk_scale=None, attn_drop=0., proj_drop=0.):\n","        super().__init__()\n","        self.num_heads = num_heads\n","        self.in_dim = in_dim\n","        head_dim = dim // num_heads\n","        self.scale = qk_scale or head_dim ** -0.5\n","\n","        self.qkv = nn.Linear(dim, in_dim * 3, bias=qkv_bias)\n","        self.attn_drop = nn.Dropout(attn_drop)\n","        self.proj = nn.Linear(in_dim, in_dim)\n","        self.proj_drop = nn.Dropout(proj_drop)\n","\n","    def forward(self, x):\n","        B, N, C = x.shape\n","\n","        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, self.in_dim).permute(2, 0, 3, 1, 4)\n","        q, k, v = qkv[0], qkv[1], qkv[2]\n","\n","        attn = (q * self.scale) @ k.transpose(-2, -1)\n","        attn = attn.softmax(dim=-1)\n","        attn = self.attn_drop(attn)\n","\n","        x = (attn @ v).transpose(1, 2).reshape(B, N, self.in_dim)\n","        x = self.proj(x)\n","        x = self.proj_drop(x)\n","\n","        # skip connection\n","        x = v.squeeze(1) + x   # because the original x has different size with current x, use v to do skip connection\n","\n","        return x\n","\n","class Token_transformer(nn.Module):\n","\n","    def __init__(self, dim, in_dim, num_heads, mlp_ratio=1., qkv_bias=False, qk_scale=None, drop=0., attn_drop=0.,\n","                 drop_path=0., act_layer=nn.GELU, norm_layer=nn.LayerNorm):\n","        super().__init__()\n","        self.norm1 = norm_layer(dim)\n","        self.attn = Attention(\n","            dim, in_dim=in_dim, num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop)\n","        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n","        self.norm2 = norm_layer(in_dim)\n","        self.mlp = Mlp(in_features=in_dim, hidden_features=int(in_dim*mlp_ratio), out_features=in_dim, act_layer=act_layer, drop=drop)\n","\n","    def forward(self, x):\n","        x = self.attn(self.norm1(x))\n","        x = x + self.drop_path(self.mlp(self.norm2(x)))\n","        return x\n","    \n","# Copyright (c) [2012]-[2021] Shanghai Yitu Technology Co., Ltd.\n","#\n","# This source code is licensed under the Clear BSD License\n","# LICENSE file in the root directory of this file\n","# All rights reserved.\n","\"\"\"\n","Borrow from timm(https://github.com/rwightman/pytorch-image-models)\n","\"\"\"\n","import torch\n","import torch.nn as nn\n","import numpy as np\n","from timm.models.layers import DropPath\n","\n","class Mlp(nn.Module):\n","    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n","        super().__init__()\n","        out_features = out_features or in_features\n","        hidden_features = hidden_features or in_features\n","        self.fc1 = nn.Linear(in_features, hidden_features)\n","        self.act = act_layer()\n","        self.fc2 = nn.Linear(hidden_features, out_features)\n","        self.drop = nn.Dropout(drop)\n","\n","    def forward(self, x):\n","        x = self.fc1(x)\n","        x = self.act(x)\n","        x = self.drop(x)\n","        x = self.fc2(x)\n","        x = self.drop(x)\n","        return x\n","\n","class Attention(nn.Module):\n","    def __init__(self, dim, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0., proj_drop=0.):\n","        super().__init__()\n","        self.num_heads = num_heads\n","        head_dim = dim // num_heads\n","\n","        self.scale = qk_scale or head_dim ** -0.5\n","\n","        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n","        self.attn_drop = nn.Dropout(attn_drop)\n","        self.proj = nn.Linear(dim, dim)\n","        self.proj_drop = nn.Dropout(proj_drop)\n","\n","    def forward(self, x):\n","        B, N, C = x.shape\n","        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n","        q, k, v = qkv[0], qkv[1], qkv[2]\n","\n","        attn = (q @ k.transpose(-2, -1)) * self.scale\n","        attn = attn.softmax(dim=-1)\n","        attn = self.attn_drop(attn)\n","\n","        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n","        x = self.proj(x)\n","        x = self.proj_drop(x)\n","        return x\n","\n","class Block(nn.Module):\n","\n","    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop=0., attn_drop=0.,\n","                 drop_path=0., act_layer=nn.GELU, norm_layer=nn.LayerNorm):\n","        super().__init__()\n","        self.norm1 = norm_layer(dim)\n","        self.attn = Attention(\n","            dim, num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop)\n","        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n","        self.norm2 = norm_layer(dim)\n","        mlp_hidden_dim = int(dim * mlp_ratio)\n","        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n","\n","    def forward(self, x):\n","        x = x + self.drop_path(self.attn(self.norm1(x)))\n","        x = x + self.drop_path(self.mlp(self.norm2(x)))\n","        return x\n","\n","\n","def get_sinusoid_encoding(n_position, d_hid):\n","    ''' Sinusoid position encoding table '''\n","\n","    def get_position_angle_vec(position):\n","        return [position / np.power(10000, 2 * (hid_j // 2) / d_hid) for hid_j in range(d_hid)]\n","\n","    sinusoid_table = np.array([get_position_angle_vec(pos_i) for pos_i in range(n_position)])\n","    sinusoid_table[:, 0::2] = np.sin(sinusoid_table[:, 0::2])  # dim 2i\n","    sinusoid_table[:, 1::2] = np.cos(sinusoid_table[:, 1::2])  # dim 2i+1\n","\n","    return torch.FloatTensor(sinusoid_table).unsqueeze(0)"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-03-07T10:32:59.961676Z","iopub.status.busy":"2024-03-07T10:32:59.961312Z","iopub.status.idle":"2024-03-07T10:33:02.510603Z","shell.execute_reply":"2024-03-07T10:33:02.509636Z","shell.execute_reply.started":"2024-03-07T10:32:59.961648Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["adopt performer encoder for tokens-to-token\n"]},{"data":{"text/plain":["==========================================================================================\n","Layer (type:depth-idx)                   Output Shape              Param #\n","==========================================================================================\n","T2T_ViT                                  [32, 4]                   101,376\n","├─T2T_module: 1-1                        [32, 196, 512]            --\n","│    └─Unfold: 2-1                       [32, 147, 3136]           --\n","│    └─Token_performer: 2-2              [32, 3136, 64]            2,048\n","│    │    └─LayerNorm: 3-1               [32, 3136, 147]           294\n","│    │    └─Linear: 3-2                  [32, 3136, 192]           28,416\n","│    │    └─Linear: 3-3                  [32, 3136, 64]            4,160\n","│    │    └─Dropout: 3-4                 [32, 3136, 64]            --\n","│    │    └─LayerNorm: 3-5               [32, 3136, 64]            128\n","│    │    └─Sequential: 3-6              [32, 3136, 64]            8,320\n","│    └─Unfold: 2-3                       [32, 576, 784]            --\n","│    └─Token_performer: 2-4              [32, 784, 64]             2,048\n","│    │    └─LayerNorm: 3-7               [32, 784, 576]            1,152\n","│    │    └─Linear: 3-8                  [32, 784, 192]            110,784\n","│    │    └─Linear: 3-9                  [32, 784, 64]             4,160\n","│    │    └─Dropout: 3-10                [32, 784, 64]             --\n","│    │    └─LayerNorm: 3-11              [32, 784, 64]             128\n","│    │    └─Sequential: 3-12             [32, 784, 64]             8,320\n","│    └─Unfold: 2-5                       [32, 576, 196]            --\n","│    └─Linear: 2-6                       [32, 196, 512]            295,424\n","├─Dropout: 1-2                           [32, 197, 512]            --\n","├─ModuleList: 1-3                        --                        --\n","│    └─Block: 2-7                        [32, 197, 512]            --\n","│    │    └─LayerNorm: 3-13              [32, 197, 512]            1,024\n","│    │    └─Attention: 3-14              [32, 197, 512]            1,049,088\n","│    │    └─Identity: 3-15               [32, 197, 512]            --\n","│    │    └─LayerNorm: 3-16              [32, 197, 512]            1,024\n","│    │    └─Mlp: 3-17                    [32, 197, 512]            1,574,912\n","│    │    └─Identity: 3-18               [32, 197, 512]            --\n","│    └─Block: 2-8                        [32, 197, 512]            --\n","│    │    └─LayerNorm: 3-19              [32, 197, 512]            1,024\n","│    │    └─Attention: 3-20              [32, 197, 512]            1,049,088\n","│    │    └─Identity: 3-21               [32, 197, 512]            --\n","│    │    └─LayerNorm: 3-22              [32, 197, 512]            1,024\n","│    │    └─Mlp: 3-23                    [32, 197, 512]            1,574,912\n","│    │    └─Identity: 3-24               [32, 197, 512]            --\n","│    └─Block: 2-9                        [32, 197, 512]            --\n","│    │    └─LayerNorm: 3-25              [32, 197, 512]            1,024\n","│    │    └─Attention: 3-26              [32, 197, 512]            1,049,088\n","│    │    └─Identity: 3-27               [32, 197, 512]            --\n","│    │    └─LayerNorm: 3-28              [32, 197, 512]            1,024\n","│    │    └─Mlp: 3-29                    [32, 197, 512]            1,574,912\n","│    │    └─Identity: 3-30               [32, 197, 512]            --\n","│    └─Block: 2-10                       [32, 197, 512]            --\n","│    │    └─LayerNorm: 3-31              [32, 197, 512]            1,024\n","│    │    └─Attention: 3-32              [32, 197, 512]            1,049,088\n","│    │    └─Identity: 3-33               [32, 197, 512]            --\n","│    │    └─LayerNorm: 3-34              [32, 197, 512]            1,024\n","│    │    └─Mlp: 3-35                    [32, 197, 512]            1,574,912\n","│    │    └─Identity: 3-36               [32, 197, 512]            --\n","│    └─Block: 2-11                       [32, 197, 512]            --\n","│    │    └─LayerNorm: 3-37              [32, 197, 512]            1,024\n","│    │    └─Attention: 3-38              [32, 197, 512]            1,049,088\n","│    │    └─Identity: 3-39               [32, 197, 512]            --\n","│    │    └─LayerNorm: 3-40              [32, 197, 512]            1,024\n","│    │    └─Mlp: 3-41                    [32, 197, 512]            1,574,912\n","│    │    └─Identity: 3-42               [32, 197, 512]            --\n","│    └─Block: 2-12                       [32, 197, 512]            --\n","│    │    └─LayerNorm: 3-43              [32, 197, 512]            1,024\n","│    │    └─Attention: 3-44              [32, 197, 512]            1,049,088\n","│    │    └─Identity: 3-45               [32, 197, 512]            --\n","│    │    └─LayerNorm: 3-46              [32, 197, 512]            1,024\n","│    │    └─Mlp: 3-47                    [32, 197, 512]            1,574,912\n","│    │    └─Identity: 3-48               [32, 197, 512]            --\n","│    └─Block: 2-13                       [32, 197, 512]            --\n","│    │    └─LayerNorm: 3-49              [32, 197, 512]            1,024\n","│    │    └─Attention: 3-50              [32, 197, 512]            1,049,088\n","│    │    └─Identity: 3-51               [32, 197, 512]            --\n","│    │    └─LayerNorm: 3-52              [32, 197, 512]            1,024\n","│    │    └─Mlp: 3-53                    [32, 197, 512]            1,574,912\n","│    │    └─Identity: 3-54               [32, 197, 512]            --\n","│    └─Block: 2-14                       [32, 197, 512]            --\n","│    │    └─LayerNorm: 3-55              [32, 197, 512]            1,024\n","│    │    └─Attention: 3-56              [32, 197, 512]            1,049,088\n","│    │    └─Identity: 3-57               [32, 197, 512]            --\n","│    │    └─LayerNorm: 3-58              [32, 197, 512]            1,024\n","│    │    └─Mlp: 3-59                    [32, 197, 512]            1,574,912\n","│    │    └─Identity: 3-60               [32, 197, 512]            --\n","│    └─Block: 2-15                       [32, 197, 512]            --\n","│    │    └─LayerNorm: 3-61              [32, 197, 512]            1,024\n","│    │    └─Attention: 3-62              [32, 197, 512]            1,049,088\n","│    │    └─Identity: 3-63               [32, 197, 512]            --\n","│    │    └─LayerNorm: 3-64              [32, 197, 512]            1,024\n","│    │    └─Mlp: 3-65                    [32, 197, 512]            1,574,912\n","│    │    └─Identity: 3-66               [32, 197, 512]            --\n","│    └─Block: 2-16                       [32, 197, 512]            --\n","│    │    └─LayerNorm: 3-67              [32, 197, 512]            1,024\n","│    │    └─Attention: 3-68              [32, 197, 512]            1,049,088\n","│    │    └─Identity: 3-69               [32, 197, 512]            --\n","│    │    └─LayerNorm: 3-70              [32, 197, 512]            1,024\n","│    │    └─Mlp: 3-71                    [32, 197, 512]            1,574,912\n","│    │    └─Identity: 3-72               [32, 197, 512]            --\n","│    └─Block: 2-17                       [32, 197, 512]            --\n","│    │    └─LayerNorm: 3-73              [32, 197, 512]            1,024\n","│    │    └─Attention: 3-74              [32, 197, 512]            1,049,088\n","│    │    └─Identity: 3-75               [32, 197, 512]            --\n","│    │    └─LayerNorm: 3-76              [32, 197, 512]            1,024\n","│    │    └─Mlp: 3-77                    [32, 197, 512]            1,574,912\n","│    │    └─Identity: 3-78               [32, 197, 512]            --\n","│    └─Block: 2-18                       [32, 197, 512]            --\n","│    │    └─LayerNorm: 3-79              [32, 197, 512]            1,024\n","│    │    └─Attention: 3-80              [32, 197, 512]            1,049,088\n","│    │    └─Identity: 3-81               [32, 197, 512]            --\n","│    │    └─LayerNorm: 3-82              [32, 197, 512]            1,024\n","│    │    └─Mlp: 3-83                    [32, 197, 512]            1,574,912\n","│    │    └─Identity: 3-84               [32, 197, 512]            --\n","│    └─Block: 2-19                       [32, 197, 512]            --\n","│    │    └─LayerNorm: 3-85              [32, 197, 512]            1,024\n","│    │    └─Attention: 3-86              [32, 197, 512]            1,049,088\n","│    │    └─Identity: 3-87               [32, 197, 512]            --\n","│    │    └─LayerNorm: 3-88              [32, 197, 512]            1,024\n","│    │    └─Mlp: 3-89                    [32, 197, 512]            1,574,912\n","│    │    └─Identity: 3-90               [32, 197, 512]            --\n","│    └─Block: 2-20                       [32, 197, 512]            --\n","│    │    └─LayerNorm: 3-91              [32, 197, 512]            1,024\n","│    │    └─Attention: 3-92              [32, 197, 512]            1,049,088\n","│    │    └─Identity: 3-93               [32, 197, 512]            --\n","│    │    └─LayerNorm: 3-94              [32, 197, 512]            1,024\n","│    │    └─Mlp: 3-95                    [32, 197, 512]            1,574,912\n","│    │    └─Identity: 3-96               [32, 197, 512]            --\n","│    └─Block: 2-21                       [32, 197, 512]            --\n","│    │    └─LayerNorm: 3-97              [32, 197, 512]            1,024\n","│    │    └─Attention: 3-98              [32, 197, 512]            1,049,088\n","│    │    └─Identity: 3-99               [32, 197, 512]            --\n","│    │    └─LayerNorm: 3-100             [32, 197, 512]            1,024\n","│    │    └─Mlp: 3-101                   [32, 197, 512]            1,574,912\n","│    │    └─Identity: 3-102              [32, 197, 512]            --\n","│    └─Block: 2-22                       [32, 197, 512]            --\n","│    │    └─LayerNorm: 3-103             [32, 197, 512]            1,024\n","│    │    └─Attention: 3-104             [32, 197, 512]            1,049,088\n","│    │    └─Identity: 3-105              [32, 197, 512]            --\n","│    │    └─LayerNorm: 3-106             [32, 197, 512]            1,024\n","│    │    └─Mlp: 3-107                   [32, 197, 512]            1,574,912\n","│    │    └─Identity: 3-108              [32, 197, 512]            --\n","│    └─Block: 2-23                       [32, 197, 512]            --\n","│    │    └─LayerNorm: 3-109             [32, 197, 512]            1,024\n","│    │    └─Attention: 3-110             [32, 197, 512]            1,049,088\n","│    │    └─Identity: 3-111              [32, 197, 512]            --\n","│    │    └─LayerNorm: 3-112             [32, 197, 512]            1,024\n","│    │    └─Mlp: 3-113                   [32, 197, 512]            1,574,912\n","│    │    └─Identity: 3-114              [32, 197, 512]            --\n","│    └─Block: 2-24                       [32, 197, 512]            --\n","│    │    └─LayerNorm: 3-115             [32, 197, 512]            1,024\n","│    │    └─Attention: 3-116             [32, 197, 512]            1,049,088\n","│    │    └─Identity: 3-117              [32, 197, 512]            --\n","│    │    └─LayerNorm: 3-118             [32, 197, 512]            1,024\n","│    │    └─Mlp: 3-119                   [32, 197, 512]            1,574,912\n","│    │    └─Identity: 3-120              [32, 197, 512]            --\n","│    └─Block: 2-25                       [32, 197, 512]            --\n","│    │    └─LayerNorm: 3-121             [32, 197, 512]            1,024\n","│    │    └─Attention: 3-122             [32, 197, 512]            1,049,088\n","│    │    └─Identity: 3-123              [32, 197, 512]            --\n","│    │    └─LayerNorm: 3-124             [32, 197, 512]            1,024\n","│    │    └─Mlp: 3-125                   [32, 197, 512]            1,574,912\n","│    │    └─Identity: 3-126              [32, 197, 512]            --\n","│    └─Block: 2-26                       [32, 197, 512]            --\n","│    │    └─LayerNorm: 3-127             [32, 197, 512]            1,024\n","│    │    └─Attention: 3-128             [32, 197, 512]            1,049,088\n","│    │    └─Identity: 3-129              [32, 197, 512]            --\n","│    │    └─LayerNorm: 3-130             [32, 197, 512]            1,024\n","│    │    └─Mlp: 3-131                   [32, 197, 512]            1,574,912\n","│    │    └─Identity: 3-132              [32, 197, 512]            --\n","│    └─Block: 2-27                       [32, 197, 512]            --\n","│    │    └─LayerNorm: 3-133             [32, 197, 512]            1,024\n","│    │    └─Attention: 3-134             [32, 197, 512]            1,049,088\n","│    │    └─Identity: 3-135              [32, 197, 512]            --\n","│    │    └─LayerNorm: 3-136             [32, 197, 512]            1,024\n","│    │    └─Mlp: 3-137                   [32, 197, 512]            1,574,912\n","│    │    └─Identity: 3-138              [32, 197, 512]            --\n","│    └─Block: 2-28                       [32, 197, 512]            --\n","│    │    └─LayerNorm: 3-139             [32, 197, 512]            1,024\n","│    │    └─Attention: 3-140             [32, 197, 512]            1,049,088\n","│    │    └─Identity: 3-141              [32, 197, 512]            --\n","│    │    └─LayerNorm: 3-142             [32, 197, 512]            1,024\n","│    │    └─Mlp: 3-143                   [32, 197, 512]            1,574,912\n","│    │    └─Identity: 3-144              [32, 197, 512]            --\n","│    └─Block: 2-29                       [32, 197, 512]            --\n","│    │    └─LayerNorm: 3-145             [32, 197, 512]            1,024\n","│    │    └─Attention: 3-146             [32, 197, 512]            1,049,088\n","│    │    └─Identity: 3-147              [32, 197, 512]            --\n","│    │    └─LayerNorm: 3-148             [32, 197, 512]            1,024\n","│    │    └─Mlp: 3-149                   [32, 197, 512]            1,574,912\n","│    │    └─Identity: 3-150              [32, 197, 512]            --\n","│    └─Block: 2-30                       [32, 197, 512]            --\n","│    │    └─LayerNorm: 3-151             [32, 197, 512]            1,024\n","│    │    └─Attention: 3-152             [32, 197, 512]            1,049,088\n","│    │    └─Identity: 3-153              [32, 197, 512]            --\n","│    │    └─LayerNorm: 3-154             [32, 197, 512]            1,024\n","│    │    └─Mlp: 3-155                   [32, 197, 512]            1,574,912\n","│    │    └─Identity: 3-156              [32, 197, 512]            --\n","├─LayerNorm: 1-4                         [32, 197, 512]            1,024\n","├─Linear: 1-5                            [32, 4]                   2,052\n","==========================================================================================\n","Total params: 63,594,986\n","Trainable params: 63,490,026\n","Non-trainable params: 104,960\n","Total mult-adds (G): 2.03\n","==========================================================================================\n","Input size (MB): 19.27\n","Forward/backward pass size (MB): 6931.79\n","Params size (MB): 253.96\n","Estimated Total Size (MB): 7205.02\n","=========================================================================================="]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["# Copyright (c) [2012]-[2021] Shanghai Yitu Technology Co., Ltd.\n","#\n","# This source code is licensed under the Clear BSD License\n","# LICENSE file in the root directory of this file\n","# All rights reserved.\n","\"\"\"\n","T2T-ViT\n","\"\"\"\n","import torch\n","import torch.nn as nn\n","\n","from timm.models.helpers import load_pretrained\n","from timm.models.registry import register_model\n","from timm.models.layers import trunc_normal_\n","import numpy as np\n","\n","\n","def _cfg(url='', **kwargs):\n","    return {\n","        'url': url,\n","        'num_classes': 1000, 'input_size': (3, 224, 224), 'pool_size': None,\n","        'crop_pct': .9, 'interpolation': 'bicubic',\n","        'mean': (0.485, 0.456, 0.406), 'std': (0.229, 0.224, 0.225),\n","        'classifier': 'head',\n","        **kwargs\n","    }\n","\n","default_cfgs = {\n","    'T2t_vit_7': _cfg(),\n","    'T2t_vit_10': _cfg(),\n","    'T2t_vit_12': _cfg(),\n","    'T2t_vit_14': _cfg(),\n","    'T2t_vit_19': _cfg(),\n","    'T2t_vit_24': _cfg(),\n","    'T2t_vit_t_14': _cfg(),\n","    'T2t_vit_t_19': _cfg(),\n","    'T2t_vit_t_24': _cfg(),\n","    'T2t_vit_14_resnext': _cfg(),\n","    'T2t_vit_14_wide': _cfg(),\n","}\n","\n","class T2T_module(nn.Module):\n","    \"\"\"\n","    Tokens-to-Token encoding module\n","    \"\"\"\n","    def __init__(self, img_size=224, tokens_type='performer', in_chans=3, embed_dim=768, token_dim=64):\n","        super().__init__()\n","\n","        if tokens_type == 'transformer':\n","            print('adopt transformer encoder for tokens-to-token')\n","            self.soft_split0 = nn.Unfold(kernel_size=(7, 7), stride=(4, 4), padding=(2, 2))\n","            self.soft_split1 = nn.Unfold(kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n","            self.soft_split2 = nn.Unfold(kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n","\n","            self.attention1 = Token_transformer(dim=in_chans * 7 * 7, in_dim=token_dim, num_heads=1, mlp_ratio=1.0)\n","            self.attention2 = Token_transformer(dim=token_dim * 3 * 3, in_dim=token_dim, num_heads=1, mlp_ratio=1.0)\n","            self.project = nn.Linear(token_dim * 3 * 3, embed_dim)\n","\n","        elif tokens_type == 'performer':\n","            print('adopt performer encoder for tokens-to-token')\n","            self.soft_split0 = nn.Unfold(kernel_size=(7, 7), stride=(4, 4), padding=(2, 2))\n","            self.soft_split1 = nn.Unfold(kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n","            self.soft_split2 = nn.Unfold(kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n","\n","            #self.attention1 = Token_performer(dim=token_dim, in_dim=in_chans*7*7, kernel_ratio=0.5)\n","            #self.attention2 = Token_performer(dim=token_dim, in_dim=token_dim*3*3, kernel_ratio=0.5)\n","            self.attention1 = Token_performer(dim=in_chans*7*7, in_dim=token_dim, kernel_ratio=0.5)\n","            self.attention2 = Token_performer(dim=token_dim*3*3, in_dim=token_dim, kernel_ratio=0.5)\n","            self.project = nn.Linear(token_dim * 3 * 3, embed_dim)\n","\n","        elif tokens_type == 'convolution':  # just for comparison with conolution, not our model\n","            # for this tokens type, you need change forward as three convolution operation\n","            print('adopt convolution layers for tokens-to-token')\n","            self.soft_split0 = nn.Conv2d(3, token_dim, kernel_size=(7, 7), stride=(4, 4), padding=(2, 2))  # the 1st convolution\n","            self.soft_split1 = nn.Conv2d(token_dim, token_dim, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1)) # the 2nd convolution\n","            self.project = nn.Conv2d(token_dim, embed_dim, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1)) # the 3rd convolution\n","\n","        self.num_patches = (img_size // (4 * 2 * 2)) * (img_size // (4 * 2 * 2))  # there are 3 sfot split, stride are 4,2,2 seperately\n","\n","    def forward(self, x):\n","        # step0: soft split\n","        x = self.soft_split0(x).transpose(1, 2)\n","\n","        # iteration1: re-structurization/reconstruction\n","        x = self.attention1(x)\n","        B, new_HW, C = x.shape\n","        x = x.transpose(1,2).reshape(B, C, int(np.sqrt(new_HW)), int(np.sqrt(new_HW)))\n","        # iteration1: soft split\n","        x = self.soft_split1(x).transpose(1, 2)\n","\n","        # iteration2: re-structurization/reconstruction\n","        x = self.attention2(x)\n","        B, new_HW, C = x.shape\n","        x = x.transpose(1, 2).reshape(B, C, int(np.sqrt(new_HW)), int(np.sqrt(new_HW)))\n","        # iteration2: soft split\n","        x = self.soft_split2(x).transpose(1, 2)\n","\n","        # final tokens\n","        x = self.project(x)\n","\n","        return x\n","\n","class T2T_ViT(nn.Module):\n","    def __init__(self, img_size=224, tokens_type='performer', in_chans=3, num_classes=1000, embed_dim=768, depth=12,\n","                 num_heads=12, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop_rate=0., attn_drop_rate=0.,\n","                 drop_path_rate=0., norm_layer=nn.LayerNorm, token_dim=64):\n","        super().__init__()\n","        self.num_classes = num_classes\n","        self.num_features = self.embed_dim = embed_dim  # num_features for consistency with other models\n","\n","        self.tokens_to_token = T2T_module(\n","                img_size=img_size, tokens_type=tokens_type, in_chans=in_chans, embed_dim=embed_dim, token_dim=token_dim)\n","        num_patches = self.tokens_to_token.num_patches\n","\n","        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n","        self.pos_embed = nn.Parameter(data=get_sinusoid_encoding(n_position=num_patches + 1, d_hid=embed_dim), requires_grad=False)\n","        self.pos_drop = nn.Dropout(p=drop_rate)\n","\n","        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]  # stochastic depth decay rule\n","        self.blocks = nn.ModuleList([\n","            Block(\n","                dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale,\n","                drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[i], norm_layer=norm_layer)\n","            for i in range(depth)])\n","        self.norm = norm_layer(embed_dim)\n","\n","        # Classifier head\n","        self.head = nn.Linear(embed_dim, num_classes) if num_classes > 0 else nn.Identity()\n","\n","        trunc_normal_(self.cls_token, std=.02)\n","        self.apply(self._init_weights)\n","\n","    def _init_weights(self, m):\n","        if isinstance(m, nn.Linear):\n","            trunc_normal_(m.weight, std=.02)\n","            if isinstance(m, nn.Linear) and m.bias is not None:\n","                nn.init.constant_(m.bias, 0)\n","        elif isinstance(m, nn.LayerNorm):\n","            nn.init.constant_(m.bias, 0)\n","            nn.init.constant_(m.weight, 1.0)\n","\n","    @torch.jit.ignore\n","    def no_weight_decay(self):\n","        return {'cls_token'}\n","\n","    def get_classifier(self):\n","        return self.head\n","\n","    def reset_classifier(self, num_classes, global_pool=''):\n","        self.num_classes = num_classes\n","        self.head = nn.Linear(self.embed_dim, num_classes) if num_classes > 0 else nn.Identity()\n","\n","    def forward_features(self, x):\n","        B = x.shape[0]\n","        x = self.tokens_to_token(x)\n","\n","        cls_tokens = self.cls_token.expand(B, -1, -1)\n","        x = torch.cat((cls_tokens, x), dim=1)\n","        x = x + self.pos_embed\n","        x = self.pos_drop(x)\n","\n","        for blk in self.blocks:\n","            x = blk(x)\n","\n","        x = self.norm(x)\n","        return x[:, 0]\n","\n","    def forward(self, x):\n","        x = self.forward_features(x)\n","        x = self.head(x)\n","        return x\n","\n","@register_model\n","def t2t_vit_7(pretrained=False, **kwargs): # adopt performer for tokens to token\n","    if pretrained:\n","        kwargs.setdefault('qk_scale', 256 ** -0.5)\n","    model = T2T_ViT(tokens_type='performer', embed_dim=256, depth=7, num_heads=4, mlp_ratio=2., **kwargs)\n","    model.default_cfg = default_cfgs['T2t_vit_7']\n","    if pretrained:\n","        load_pretrained(\n","            model, num_classes=model.num_classes, in_chans=kwargs.get('in_chans', 3))\n","    return model\n","\n","@register_model\n","def t2t_vit_10(pretrained=False, **kwargs): # adopt performer for tokens to token\n","    if pretrained:\n","        kwargs.setdefault('qk_scale', 256 ** -0.5)\n","    model = T2T_ViT(tokens_type='performer', embed_dim=256, depth=10, num_heads=4, mlp_ratio=2., **kwargs)\n","    model.default_cfg = default_cfgs['T2t_vit_10']\n","    if pretrained:\n","        load_pretrained(\n","            model, num_classes=model.num_classes, in_chans=kwargs.get('in_chans', 3))\n","    return model\n","\n","@register_model\n","def t2t_vit_12(pretrained=False, **kwargs): # adopt performer for tokens to token\n","    if pretrained:\n","        kwargs.setdefault('qk_scale', 256 ** -0.5)\n","    model = T2T_ViT(tokens_type='performer', embed_dim=256, depth=12, num_heads=4, mlp_ratio=2., **kwargs)\n","    model.default_cfg = default_cfgs['T2t_vit_12']\n","    if pretrained:\n","        load_pretrained(\n","            model, num_classes=model.num_classes, in_chans=kwargs.get('in_chans', 3))\n","    return model\n","\n","\n","@register_model\n","def t2t_vit_14(pretrained=False, **kwargs):  # adopt performer for tokens to token\n","    if pretrained:\n","        kwargs.setdefault('qk_scale', 384 ** -0.5)\n","    model = T2T_ViT(tokens_type='performer', embed_dim=384, depth=14, num_heads=6, mlp_ratio=3., **kwargs)\n","    model.default_cfg = default_cfgs['T2t_vit_14']\n","    if pretrained:\n","        load_pretrained(\n","            model, num_classes=model.num_classes, in_chans=kwargs.get('in_chans', 3))\n","    return model\n","\n","@register_model\n","def t2t_vit_19(pretrained=False, **kwargs): # adopt performer for tokens to token\n","    if pretrained:\n","        kwargs.setdefault('qk_scale', 448 ** -0.5)\n","    model = T2T_ViT(tokens_type='performer', embed_dim=448, depth=19, num_heads=7, mlp_ratio=3., **kwargs)\n","    model.default_cfg = default_cfgs['T2t_vit_19']\n","    if pretrained:\n","        load_pretrained(\n","            model, num_classes=model.num_classes, in_chans=kwargs.get('in_chans', 3))\n","    return model\n","\n","@register_model\n","def t2t_vit_24(pretrained=False, **kwargs): # adopt performer for tokens to token\n","    if pretrained:\n","        kwargs.setdefault('qk_scale', 512 ** -0.5)\n","    model = T2T_ViT(tokens_type='performer', embed_dim=512, depth=24, num_heads=8, mlp_ratio=3., **kwargs)\n","    model.default_cfg = default_cfgs['T2t_vit_24']\n","    if pretrained:\n","        load_pretrained(\n","            model, num_classes=model.num_classes, in_chans=kwargs.get('in_chans', 3))\n","    return model\n","\n","@register_model\n","def t2t_vit_t_14(pretrained=False, **kwargs):  # adopt transformers for tokens to token\n","    if pretrained:\n","        kwargs.setdefault('qk_scale', 384 ** -0.5)\n","    model = T2T_ViT(tokens_type='transformer', embed_dim=384, depth=14, num_heads=6, mlp_ratio=3., **kwargs)\n","    model.default_cfg = default_cfgs['T2t_vit_t_14']\n","    if pretrained:\n","        load_pretrained(\n","            model, num_classes=model.num_classes, in_chans=kwargs.get('in_chans', 3))\n","    return model\n","\n","@register_model\n","def t2t_vit_t_19(pretrained=False, **kwargs):  # adopt transformers for tokens to token\n","    if pretrained:\n","        kwargs.setdefault('qk_scale', 448 ** -0.5)\n","    model = T2T_ViT(tokens_type='transformer', embed_dim=448, depth=19, num_heads=7, mlp_ratio=3., **kwargs)\n","    model.default_cfg = default_cfgs['T2t_vit_t_19']\n","    if pretrained:\n","        load_pretrained(\n","            model, num_classes=model.num_classes, in_chans=kwargs.get('in_chans', 3))\n","    return model\n","\n","@register_model\n","def t2t_vit_t_24(pretrained=False, **kwargs):  # adopt transformers for tokens to token\n","    if pretrained:\n","        kwargs.setdefault('qk_scale', 512 ** -0.5)\n","    model = T2T_ViT(tokens_type='transformer', embed_dim=512, depth=24, num_heads=8, mlp_ratio=3., **kwargs)\n","    model.default_cfg = default_cfgs['T2t_vit_t_24']\n","    if pretrained:\n","        load_pretrained(\n","            model, num_classes=model.num_classes, in_chans=kwargs.get('in_chans', 3))\n","    return model\n","\n","# rexnext and wide structure\n","@register_model\n","def t2t_vit_14_resnext(pretrained=False, **kwargs):\n","    if pretrained:\n","        kwargs.setdefault('qk_scale', 384 ** -0.5)\n","    model = T2T_ViT(tokens_type='performer', embed_dim=384, depth=14, num_heads=32, mlp_ratio=3., **kwargs)\n","    model.default_cfg = default_cfgs['T2t_vit_14_resnext']\n","    if pretrained:\n","        load_pretrained(\n","            model, num_classes=model.num_classes, in_chans=kwargs.get('in_chans', 3))\n","    return model\n","\n","@register_model\n","def t2t_vit_14_wide(pretrained=False, **kwargs):\n","    if pretrained:\n","        kwargs.setdefault('qk_scale', 512 ** -0.5)\n","    model = T2T_ViT(tokens_type='performer', embed_dim=768, depth=4, num_heads=12, mlp_ratio=3., **kwargs)\n","    model.default_cfg = default_cfgs['T2t_vit_14_wide']\n","    if pretrained:\n","        load_pretrained(\n","            model, num_classes=model.num_classes, in_chans=kwargs.get('in_chans', 3))\n","    return model\n","\n","\n","import torchinfo\n","model=t2t_vit_24(num_classes=4)\n","torchinfo.summary(model,(32,3,224,224))"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-03-07T06:57:51.509178Z","iopub.status.busy":"2024-03-07T06:57:51.508724Z","iopub.status.idle":"2024-03-07T06:57:51.516558Z","shell.execute_reply":"2024-03-07T06:57:51.515251Z","shell.execute_reply.started":"2024-03-07T06:57:51.509141Z"},"trusted":true},"outputs":[{"data":{"text/plain":["4"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["len(CLASSES)"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-03-07T10:33:18.170337Z","iopub.status.busy":"2024-03-07T10:33:18.169960Z","iopub.status.idle":"2024-03-07T10:33:18.175501Z","shell.execute_reply":"2024-03-07T10:33:18.174474Z","shell.execute_reply.started":"2024-03-07T10:33:18.170307Z"},"trusted":true},"outputs":[],"source":["\n","dataloaders = {\n","    \"train\": train_dataloader,\n","    \"val\": valid_dataloader,\n","    #\"test\": test_dataloader\n","}\n","\n","dataset_sizes = {\n","    \"train\": train_len,\n","    \"val\": valid_len,\n","    #\"test\": test_len\n","}\n"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-03-07T10:33:20.155306Z","iopub.status.busy":"2024-03-07T10:33:20.154919Z","iopub.status.idle":"2024-03-07T10:33:20.168764Z","shell.execute_reply":"2024-03-07T10:33:20.167904Z","shell.execute_reply.started":"2024-03-07T10:33:20.155277Z"},"trusted":true},"outputs":[],"source":["model.to(device)\n","\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(model.parameters(),lr=0.001,weight_decay=0.00001)\n","criterion = criterion.to(device)\n","exp_lr_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=15, gamma=0.97)"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2024-03-07T10:33:22.069359Z","iopub.status.busy":"2024-03-07T10:33:22.068998Z","iopub.status.idle":"2024-03-07T10:33:22.089852Z","shell.execute_reply":"2024-03-07T10:33:22.088885Z","shell.execute_reply.started":"2024-03-07T10:33:22.069330Z"},"trusted":true},"outputs":[],"source":["import time\n","import copy\n","import torch\n","import pandas as pd\n","import os\n","from tqdm import tqdm\n","\n","def train_model(model, criterion, optimizer, exp_lr_scheduler, num_epochs=75, load_checkpoint=False, checkpoint_path=None):\n","    since = time.time()\n","    best_model_wts = copy.deepcopy(model.state_dict())\n","    best_acc = 0.0\n","    df_train = pd.DataFrame(columns=['epoch', 'train_loss', 'train_acc'])\n","    df_val = pd.DataFrame(columns=['epoch', 'val_loss', 'val_acc'])\n","    if load_checkpoint:\n","        if checkpoint_path is None:\n","            raise ValueError(\"Checkpoint path is not specified.\")\n","\n","        checkpoint = torch.load(checkpoint_path)\n","        model.load_state_dict(checkpoint['model_state_dict'])\n","        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n","        original_learning_rate = optimizer.param_groups[0]['lr']\n","        print(f\"Original Learning Rate: {original_learning_rate}\")\n","        loss = checkpoint['loss']\n","\n","        start_epoch = 90\n","    else:\n","        start_epoch = 0\n","\n","    for epoch in range(start_epoch, num_epochs):\n","        print(f'Epoch {epoch}/{num_epochs - 1}')\n","        print(\"-\" * 10)\n","\n","        if epoch == 0:\n","            if not os.path.isdir(\"/kaggle/working/\"):\n","                os.mkdir(\"/kaggle/working/\")\n","\n","        for phase in ['train', 'val']:  # We do training and validation phase per epoch\n","            if phase == 'train':\n","                model.train()  # model to training mode\n","            else:\n","                model.eval()  # model to evaluate\n","\n","            running_loss = 0.0\n","            running_corrects = 0.0\n","            total_samples = 0\n","\n","            progress_bar = tqdm(dataloaders[phase], desc=f'{phase.capitalize()} Epoch {epoch}/{num_epochs - 1}', leave=False)\n","\n","            for inputs, labels in progress_bar:\n","                inputs = inputs.to(device)\n","                labels = labels.to(device)\n","\n","                optimizer.zero_grad()\n","\n","                with torch.set_grad_enabled(phase == 'train'):  # no autograd makes validation go faster\n","                    outputs = model(inputs)\n","                    _, preds = torch.max(outputs, 1)  # used for accuracy\n","                    loss = criterion(outputs, labels)\n","\n","                    if phase == 'train':\n","                        loss.backward()\n","                        optimizer.step()\n","\n","                running_loss += loss.item() * inputs.size(0)\n","                running_corrects += torch.sum(preds == labels.data)\n","                total_samples += labels.size(0)\n","\n","                # Update progress bar description with live accuracy\n","                accuracy = running_corrects.double() / total_samples\n","                progress_bar.set_postfix(loss=running_loss / total_samples, accuracy=accuracy)\n","\n","            if phase == 'train':\n","                exp_lr_scheduler.step()  # step at the end of the epoch\n","\n","            epoch_loss = running_loss / dataset_sizes[phase]\n","            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n","\n","            if phase == 'train':\n","                df_new_row = pd.DataFrame({'epoch': [epoch], 'train_loss': [epoch_loss], 'train_acc': [epoch_acc.cpu()]})\n","                df_train = pd.concat([df_train, df_new_row])\n","                df_train.to_csv('train_metrics.csv')\n","            elif phase == 'val':\n","                df_new_row = pd.DataFrame({'epoch': [epoch], 'val_loss': [epoch_loss], 'val_acc': [epoch_acc.cpu()]})\n","                df_val = pd.concat([df_val, df_new_row])\n","                df_val.to_csv('val_metrics.csv')\n","\n","            print(\"{} Loss: {:.4f} Acc: {:.4f}\".format(phase, epoch_loss, epoch_acc))\n","\n","            # Save torch model for checkpoints\n","            if epoch % 5 == 0:\n","                torch.save({\n","                    'epoch': epoch,\n","                    'model_state_dict': model.state_dict(),\n","                    'optimizer_state_dict': optimizer.state_dict(),\n","                    'loss': epoch_loss,\n","                }, f\"/kaggle/working/sav_model{epoch}.pt\")\n","\n","            if phase == 'val' and epoch_acc > best_acc:\n","                best_acc = epoch_acc\n","                best_model_wts = copy.deepcopy(model.state_dict())  # keep the best validation accuracy model\n","        print()\n","\n","    time_elapsed = time.time() - since\n","    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n","    print(\"Best Val Acc: {:.4f}\".format(best_acc))\n","\n","    model.load_state_dict(best_model_wts)\n","    return model, df_train, df_val\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-07T10:33:25.178577Z","iopub.status.busy":"2024-03-07T10:33:25.178204Z","iopub.status.idle":"2024-03-07T16:56:16.915047Z","shell.execute_reply":"2024-03-07T16:56:16.913849Z","shell.execute_reply.started":"2024-03-07T10:33:25.178545Z"},"trusted":true},"outputs":[],"source":["model, df_train, df_val = train_model(model, criterion, optimizer, exp_lr_scheduler, num_epochs=100, load_checkpoint=False, checkpoint_path=None)\n","# Save the best model weights at the end of training\n","torch.save(model.state_dict(), '/kaggle/working/best_model_weights.pth')\n","\n"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2024-03-07T17:09:59.651071Z","iopub.status.busy":"2024-03-07T17:09:59.650338Z","iopub.status.idle":"2024-03-07T17:10:00.166467Z","shell.execute_reply":"2024-03-07T17:10:00.165479Z","shell.execute_reply.started":"2024-03-07T17:09:59.651033Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["['both', 'infection', 'ischaemia', 'none']\n"]}],"source":["import torchvision.transforms as transforms\n","mean = [0.485, 0.456, 0.406]\n","std = [0.229, 0.224, 0.225]\n","test_transform = transforms.Compose([\n","                                transforms.Resize((224, 224)),\n","                                transforms.ToTensor(),\n","                  \n","                                    ])\n","\n","\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","kwargs = {'num_workers': 1, 'pin_memory': True} if device=='cuda' else {}\n","\n","\n","test_dataset_path = '/kaggle/input/dfuc-2021-split/A NEW DATASET SPLIT/test'\n","\n","\n","# Create a test dataset\n","test_dataset = datasets.ImageFolder(test_dataset_path, transform=test_transform)\n","\n","# Create a test dataloader\n","test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=False)  # Set shuffle to False\n","CLASSES = test_dataset.classes\n","\n","test_len = len(test_dataset)\n","print(CLASSES)"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2024-03-07T17:10:02.505122Z","iopub.status.busy":"2024-03-07T17:10:02.504131Z","iopub.status.idle":"2024-03-07T17:10:02.511602Z","shell.execute_reply":"2024-03-07T17:10:02.510671Z","shell.execute_reply.started":"2024-03-07T17:10:02.505079Z"},"trusted":true},"outputs":[{"data":{"text/plain":["4"]},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"source":["len(CLASSES)"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2024-03-07T17:10:09.893521Z","iopub.status.busy":"2024-03-07T17:10:09.893183Z","iopub.status.idle":"2024-03-07T17:10:09.898211Z","shell.execute_reply":"2024-03-07T17:10:09.897266Z","shell.execute_reply.started":"2024-03-07T17:10:09.893495Z"},"trusted":true},"outputs":[],"source":["dataloaders = {\n","    \"test\": test_dataloader\n","}\n","\n","dataset_sizes = {\n","    \"test\": test_len\n","}\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-07T17:10:19.675799Z","iopub.status.busy":"2024-03-07T17:10:19.675400Z","iopub.status.idle":"2024-03-07T17:10:34.289325Z","shell.execute_reply":"2024-03-07T17:10:34.288442Z","shell.execute_reply.started":"2024-03-07T17:10:19.675769Z"},"trusted":true},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import numpy as np\n","from sklearn.metrics import classification_report, accuracy_score\n","\n","# Assuming you have 'test_dataloader', 'test_dataset', 'device', 'model', and 'criterion' defined\n","\n","checkpoint = torch.load('/kaggle/working/best_model_weights.pth')\n","model.load_state_dict(checkpoint)\n","\n","model.eval()\n","criterion = nn.CrossEntropyLoss()\n","\n","model.to(device)\n","\n","running_corrects = 0\n","test_loss = 0\n","class_correct = [0] * len(test_dataset.classes)\n","class_total = [0] * len(test_dataset.classes)\n","\n","# Lists to store true and predicted labels\n","all_true_labels = []\n","all_pred_labels = []\n","\n","with torch.no_grad():\n","    for inputs, labels in test_dataloader:\n","        inputs = inputs.to(device)\n","        labels = labels.to(device)\n","\n","        outputs = model(inputs)\n","        _, preds = torch.max(outputs, 1)\n","\n","        running_corrects += torch.sum(preds == labels.data)\n","        loss = criterion(outputs, labels)\n","        test_loss += loss.item() * inputs.size(0)\n","\n","        for i in range(len(labels)):\n","            label = labels[i].item()\n","            class_correct[label] += int(preds[i] == label)\n","            class_total[label] += 1\n","\n","            # Collect true and predicted labels\n","            all_true_labels.append(label)\n","            all_pred_labels.append(preds[i].item())\n","\n","# Calculate metrics\n","test_acc = running_corrects.double() / len(test_dataset)\n","test_loss = test_loss / len(test_dataset)\n","\n","overall_accuracy = accuracy_score(all_true_labels, all_pred_labels)\n","classification_report_str = classification_report(all_true_labels, all_pred_labels, target_names=test_dataset.classes)\n","\n","print(f\"Test Accuracy: {test_acc:.4f}\")\n","print(f\"Test Loss: {test_loss:.4f}\")\n","print(f\"Overall Accuracy: {overall_accuracy:.4f}\")\n","print(\"Classification Report:\")\n","print(classification_report_str)\n","\n","# Calculate and print class-wise accuracy\n","for i, class_name in enumerate(test_dataset.classes):\n","    class_acc = class_correct[i] / class_total[i]\n","    print(f\"Class {class_name} Accuracy: {class_acc:.4f}\")\n"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2024-03-07T17:11:46.611913Z","iopub.status.busy":"2024-03-07T17:11:46.611528Z","iopub.status.idle":"2024-03-07T17:11:46.618176Z","shell.execute_reply":"2024-03-07T17:11:46.617138Z","shell.execute_reply.started":"2024-03-07T17:11:46.611884Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["/kaggle/working\n"]}],"source":["%cd /kaggle/working/"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2024-03-07T17:11:50.926355Z","iopub.status.busy":"2024-03-07T17:11:50.925998Z","iopub.status.idle":"2024-03-07T17:11:50.932593Z","shell.execute_reply":"2024-03-07T17:11:50.931671Z","shell.execute_reply.started":"2024-03-07T17:11:50.926326Z"},"trusted":true},"outputs":[{"data":{"text/html":["<a href='sav_model90.pt' target='_blank'>sav_model90.pt</a><br>"],"text/plain":["/kaggle/working/sav_model90.pt"]},"execution_count":17,"metadata":{},"output_type":"execute_result"}],"source":["from IPython.display import FileLink\n","FileLink(r'sav_model90.pt')"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":4548795,"sourceId":7774637,"sourceType":"datasetVersion"}],"dockerImageVersionId":30665,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
